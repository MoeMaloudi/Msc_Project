{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c63a702",
   "metadata": {},
   "source": [
    "# # Network_Threat_Analysis_System\n",
    "# ## Comprehensive Feature Extraction and Machine Learning Framework\n",
    "# \n",
    "# This notebook integrates three analysis approaches:\n",
    "# 1. Statistical flow analysis\n",
    "# 2. Semantic content analysis  \n",
    "# 3. Visual pattern generation\n",
    "#\n",
    "# Output includes both human-readable reports and ML-ready datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565ecd",
   "metadata": {},
   "source": [
    "# ### Step 1: Environment Setup and Dependency Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9752aa6",
   "metadata": {},
   "source": [
    "# ### Step 1:  Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae86d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\n",
      "======================================================================\n",
      "System Memory: 31.4 GB\n",
      "Available Memory: 14.8 GB\n",
      "CPU Cores: 12 (12 usable)\n",
      "Storage Backend: Parquet (no file locking issues)\n",
      "Analysis started at: 2025-08-30 04:46:19\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports - Complete Environment Initialization\n",
    "\"\"\"\n",
    "PURPOSE: Initialize the complete analysis environment with all required libraries\n",
    "This cell sets up the entire working environment for network packet analysis.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Imports all necessary Python libraries for:\n",
    "   - Network packet parsing (scapy, pyshark)\n",
    "   - Data processing (pandas, numpy)\n",
    "   - Machine learning (sklearn, xgboost, lightgbm)\n",
    "   - Visualization (plotly, matplotlib, seaborn)\n",
    "   - System monitoring (psutil for memory tracking)\n",
    "   - File operations (parquet for efficient storage, no HDF5)\n",
    "2. Configures display settings for better readability\n",
    "3. Shows system information (total RAM, available memory)\n",
    "4. Timestamps when analysis starts\n",
    "\n",
    "WHY THESE LIBRARIES:\n",
    "- scapy/pyshark: Parse PCAP files and extract packet information\n",
    "- pandas/numpy: Handle large datasets efficiently with DataFrames\n",
    "- sklearn: Provides ML algorithms and feature selection tools\n",
    "- SGDClassifier: Enables incremental learning for large datasets\n",
    "- xgboost/lightgbm: High-performance gradient boosting for classification\n",
    "- plotly: Creates interactive visualizations\n",
    "- psutil: Monitors memory usage to prevent crashes\n",
    "- pyarrow: Enables Parquet storage without file locking issues on Windows\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Removed h5py/HDF5 (unreliable on Windows)\n",
    "- Added pyarrow for Parquet support (better compression, no locking)\n",
    "- Added lightgbm as alternative to xgboost\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import re\n",
    "import psutil\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter, deque\n",
    "from typing import Dict, List, Tuple, Optional, Any, Generator\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Parquet support (replacing HDF5)\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier  # For incremental learning when data doesn't fit in memory\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Gradient boosting libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb  # Often faster than XGBoost on CPU\n",
    "\n",
    "# Network analysis libraries\n",
    "from scapy.all import *  # For packet parsing\n",
    "import pyshark  # Alternative packet parser with better protocol support\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Jupyter notebook specific\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from tqdm.notebook import tqdm  # Progress bars for loops\n",
    "\n",
    "# Interactive widgets for configuration\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Global configuration\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "pd.set_option('display.max_columns', 50)  # Show more columns in DataFrame display\n",
    "pd.set_option('display.max_rows', 100)  # Show more rows in DataFrame display\n",
    "\n",
    "# Display system information\n",
    "print(\"=\"*70)\n",
    "print(\"MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\")\n",
    "print(\"=\"*70)\n",
    "print(f\"System Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()} ({multiprocessing.cpu_count()} usable)\")\n",
    "print(f\"Storage Backend: Parquet (no file locking issues)\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0e5b7",
   "metadata": {},
   "source": [
    "# ### Step 1.5: Clear Temporary Files Before Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4436719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Cleanup Manager</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a424bbbec4db880da78ca6f609149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<p>Manage temporary files from previous runs:</p>'), HBox(children=(Button(button_s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUICK COMMANDS:\n",
      "============================================================\n",
      "# Check what files exist:\n",
      "check_temp_files()\n",
      "\n",
      "# Clean only temp files (recommended before each run):\n",
      "cleanup_temp_files()\n",
      "\n",
      "# Clean everything including output:\n",
      "cleanup_temp_files(clear_output=True)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.5: Cleanup - Clear Temporary Files Before Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Clean up temporary files from previous runs\n",
    "This cell ensures a clean start by removing old temporary files that could\n",
    "interfere with the current analysis.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Checks for existing temporary directories\n",
    "2. Removes old flow, semantic, combined, and labeled features\n",
    "3. Clears any cached files from previous runs\n",
    "4. Recreates clean directory structure\n",
    "5. Optionally clears output directory\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Prevents mixing data from different PCAP files\n",
    "- Avoids file naming conflicts\n",
    "- Ensures consistent results\n",
    "- Frees up disk space\n",
    "- Prevents loading old data accidentally\n",
    "\n",
    "RUN THIS:\n",
    "- Before starting a new analysis\n",
    "- After a failed/interrupted run\n",
    "- When switching between different PCAP files\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "\n",
    "def cleanup_temp_files(clear_output=False):\n",
    "    \"\"\"\n",
    "    Remove temporary files from previous pipeline runs.\n",
    "    \n",
    "    Args:\n",
    "        clear_output: If True, also clears the output directory (default: False)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CLEANING UP TEMPORARY FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if Config is defined\n",
    "    try:\n",
    "        temp_dir = Config.TEMP_DIR\n",
    "        output_dir = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        # Use default values if Config not yet defined\n",
    "        temp_dir = './temp_processing'\n",
    "        output_dir = './analysis_output'\n",
    "        print(\"⚠ Config not defined, using default directories\")\n",
    "    \n",
    "    # List of temporary directories to clean\n",
    "    temp_dirs = [\n",
    "        os.path.join(temp_dir, 'flow_features'),\n",
    "        os.path.join(temp_dir, 'semantic_features'),\n",
    "        os.path.join(temp_dir, 'combined_features'),\n",
    "        os.path.join(temp_dir, 'labeled_features')\n",
    "    ]\n",
    "    \n",
    "    # Clean each temporary directory\n",
    "    for dir_path in temp_dirs:\n",
    "        if os.path.exists(dir_path):\n",
    "            try:\n",
    "                # Count files before deletion\n",
    "                file_count = sum(len(files) for _, _, files in os.walk(dir_path))\n",
    "                size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                             for dirpath, _, filenames in os.walk(dir_path)\n",
    "                             for filename in filenames) / (1024*1024)\n",
    "                \n",
    "                # Remove directory\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"✓ Removed: {os.path.basename(dir_path)}\")\n",
    "                print(f\"  Deleted {file_count} files ({size_mb:.1f} MB)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to remove {dir_path}: {e}\")\n",
    "    \n",
    "    # Clean root temp directory if empty or recreate it\n",
    "    if os.path.exists(temp_dir):\n",
    "        try:\n",
    "            # Check if temp directory has any other files\n",
    "            remaining_files = os.listdir(temp_dir)\n",
    "            if not remaining_files:\n",
    "                shutil.rmtree(temp_dir)\n",
    "                print(f\"✓ Removed empty temp directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not clean temp directory: {e}\")\n",
    "    \n",
    "    # Recreate clean directory structure\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    for dir_name in ['flow_features', 'semantic_features', 'combined_features', 'labeled_features']:\n",
    "        os.makedirs(os.path.join(temp_dir, dir_name), exist_ok=True)\n",
    "    print(\"✓ Created clean directory structure\")\n",
    "    \n",
    "    # Optionally clear output directory\n",
    "    if clear_output and os.path.exists(output_dir):\n",
    "        try:\n",
    "            # Ask for confirmation\n",
    "            response = input(f\"\\n⚠ Clear output directory '{output_dir}'? (y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                shutil.rmtree(output_dir)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                print(f\"✓ Cleared output directory\")\n",
    "            else:\n",
    "                print(\"✓ Output directory preserved\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to clear output directory: {e}\")\n",
    "    \n",
    "    # Report disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"\\nDisk space available: {disk.free / (1024**3):.1f} GB ({100-disk.percent:.1f}% free)\")\n",
    "    \n",
    "    print(\"\\n✓ Cleanup complete - ready for new analysis\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def check_temp_files():\n",
    "    \"\"\"\n",
    "    Check what temporary files currently exist without deleting them.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CHECKING TEMPORARY FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if Config is defined\n",
    "    try:\n",
    "        temp_dir = Config.TEMP_DIR\n",
    "    except NameError:\n",
    "        temp_dir = './temp_processing'\n",
    "        print(\"⚠ Config not defined, checking default directory\")\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        print(\"No temporary directory found - clean start\")\n",
    "        return\n",
    "    \n",
    "    # Check each subdirectory\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        if files:\n",
    "            dir_size = sum(os.path.getsize(os.path.join(root, f)) for f in files)\n",
    "            total_files += len(files)\n",
    "            total_size += dir_size\n",
    "            \n",
    "            rel_path = os.path.relpath(root, temp_dir)\n",
    "            print(f\"\\n{rel_path}:\")\n",
    "            print(f\"  Files: {len(files)}\")\n",
    "            print(f\"  Size: {dir_size / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            # Show sample files\n",
    "            sample_files = files[:3]\n",
    "            for f in sample_files:\n",
    "                print(f\"    - {f}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"    ... and {len(files)-3} more files\")\n",
    "    \n",
    "    if total_files > 0:\n",
    "        print(f\"\\nTotal: {total_files} files, {total_size / (1024*1024):.1f} MB\")\n",
    "        print(\"\\n⚠ Temporary files exist from previous run\")\n",
    "        print(\"  Run cleanup_temp_files() to remove them\")\n",
    "    else:\n",
    "        print(\"\\n✓ No temporary files found - clean start\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Create interactive cleanup interface\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_cleanup_ui():\n",
    "    \"\"\"\n",
    "    Create an interactive cleanup interface with buttons.\n",
    "    \"\"\"\n",
    "    display(HTML(\"<h3>Cleanup Manager</h3>\"))\n",
    "    \n",
    "    # Output area for messages\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Check button\n",
    "    check_btn = widgets.Button(\n",
    "        description='Check Temp Files',\n",
    "        button_style='info',\n",
    "        tooltip='Check what temporary files exist',\n",
    "        icon='search'\n",
    "    )\n",
    "    \n",
    "    # Clean temp button\n",
    "    clean_temp_btn = widgets.Button(\n",
    "        description='Clean Temp Files',\n",
    "        button_style='warning',\n",
    "        tooltip='Remove temporary processing files',\n",
    "        icon='trash'\n",
    "    )\n",
    "    \n",
    "    # Clean all button\n",
    "    clean_all_btn = widgets.Button(\n",
    "        description='Clean Everything',\n",
    "        button_style='danger',\n",
    "        tooltip='Remove all temporary and output files',\n",
    "        icon='exclamation-triangle'\n",
    "    )\n",
    "    \n",
    "    def on_check(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            check_temp_files()\n",
    "    \n",
    "    def on_clean_temp(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            cleanup_temp_files(clear_output=False)\n",
    "    \n",
    "    def on_clean_all(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            cleanup_temp_files(clear_output=True)\n",
    "    \n",
    "    check_btn.on_click(on_check)\n",
    "    clean_temp_btn.on_click(on_clean_temp)\n",
    "    clean_all_btn.on_click(on_clean_all)\n",
    "    \n",
    "    # Layout\n",
    "    button_box = widgets.HBox([check_btn, clean_temp_btn, clean_all_btn])\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<p>Manage temporary files from previous runs:</p>\"),\n",
    "        button_box,\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "# Display the cleanup UI\n",
    "create_cleanup_ui()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK COMMANDS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"# Check what files exist:\")\n",
    "print(\"check_temp_files()\")\n",
    "print(\"\")\n",
    "print(\"# Clean only temp files (recommended before each run):\")\n",
    "print(\"cleanup_temp_files()\")\n",
    "print(\"\")\n",
    "print(\"# Clean everything including output:\")\n",
    "print(\"cleanup_temp_files(clear_output=True)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5606",
   "metadata": {},
   "source": [
    "# ### Step 2: Configuration Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Memory-Optimized Network Analysis Configuration (V2.0)</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Memory Optimization Settings</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6b5faea76a4c198db428e0cc64fcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h4>Memory Settings (Optimized for 32GB)</h4>'), FloatSlider(value=8…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2: Configuration Class - Interactive Settings & Memory Management\n",
    "\"\"\"\n",
    "PURPOSE: Central configuration hub for all analysis parameters\n",
    "This cell creates an interactive configuration interface using Jupyter widgets.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Defines all configurable parameters for the analysis pipeline\n",
    "2. Creates an interactive UI with sliders, text boxes, and checkboxes\n",
    "3. Automatically determines optimal processing strategy based on file size\n",
    "4. Validates user inputs and checks system resources\n",
    "5. Sets up directory structure for outputs and temporary files\n",
    "\n",
    "KEY FUNCTIONALITY:\n",
    "- File Management: Handles PCAP input files and CICIDS label CSV files\n",
    "- Memory Management: Sets limits on RAM usage and flow storage\n",
    "- Processing Strategy: Chooses between disk-based or memory-based processing\n",
    "- ML Configuration: Selects which models to train and sampling strategy\n",
    "- Feature Settings: Determines number of features and data types to use\n",
    "\n",
    "CONFIGURATION CATEGORIES:\n",
    "1. Input/Output: PCAP file, label files, output directory\n",
    "2. Memory Settings: MAX_MEMORY_GB, MAX_FLOWS_IN_MEMORY, batch sizes\n",
    "3. Processing: Chunk sizes, flow timeouts, packet limits\n",
    "4. Analysis Modes: Flow analysis, semantic analysis, NLP depth\n",
    "5. ML Settings: Model selection, train/test split, sampling\n",
    "6. Storage: Parquet format usage, compression, temporary file cleanup\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Replaced HDF5 with Parquet storage (more reliable on Windows)\n",
    "- Increased default memory limits for 32GB system\n",
    "- Added LightGBM to model selection\n",
    "- Optimized chunk sizes for AMD Ryzen CPU\n",
    "\n",
    "The class uses class methods (@classmethod) so configuration is global\n",
    "and accessible throughout the entire pipeline.\n",
    "\"\"\"\n",
    "\n",
    "class Config:\n",
    "    # ============= FILE PATHS =============\n",
    "    PCAP_FILE = ''  # Path to input PCAP file (10+ GB for CICIDS2017)\n",
    "    LABEL_FILE = ''  # Single label file path (will be set if multiple CSVs combined)\n",
    "    LABEL_FILES = []  # List of CICIDS2017 CSV label files\n",
    "    OUTPUT_DIR = './analysis_output'  # Where to save results\n",
    "    \n",
    "    # ============= MEMORY OPTIMIZATION (Optimized for 32GB System) =============\n",
    "    MAX_MEMORY_GB = 8.0  # Maximum RAM to use (25% of 32GB, prevents system freeze)\n",
    "    MAX_FLOWS_IN_MEMORY = 100000  # Flows kept in RAM before disk flush (increased for 32GB)\n",
    "    MAX_PACKETS_PER_FLOW = 1000  # Cap packets per flow to prevent memory bloat\n",
    "    USE_DISK_CACHE = True  # Use Parquet disk storage for large files\n",
    "    TEMP_DIR = './temp_processing'  # Temporary storage location\n",
    "    \n",
    "    # ============= PROCESSING PARAMETERS (Optimized for Ryzen 5) =============\n",
    "    CHUNK_SIZE = 50000  # Packets processed at once (increased for better CPU utilization)\n",
    "    BATCH_SIZE = 200000  # Flows per ML training batch (increased for 32GB RAM)\n",
    "    MAX_PACKETS = 0  # Limit packets to process (0 = unlimited)\n",
    "    FLOW_TIMEOUT = 120  # Seconds before flow considered complete\n",
    "    N_JOBS = -1  # Number of parallel jobs (-1 = use all CPU cores)\n",
    "    \n",
    "    # ============= SAMPLING STRATEGY =============\n",
    "    USE_SAMPLING = True  # Sample data if too large for ML\n",
    "    SAMPLE_SIZE = 1000000  # Maximum flows for ML training (increased for 32GB)\n",
    "    STRATIFY_SAMPLE = True  # Maintain attack type distribution in sample\n",
    "    \n",
    "    # ============= ANALYSIS MODES =============\n",
    "    ANALYSIS_MODE = 'combined'  # Options: 'flow', 'semantic', 'combined'\n",
    "    DEEP_INSPECTION = True  # Enable NLP payload analysis (slower but better detection)\n",
    "    USE_CICIDS_LABELS = False  # Whether to use CICIDS ground truth\n",
    "    GENERATE_VISUALS = True  # Create visualization charts\n",
    "    ML_EXPORT = True  # Export ML models and features\n",
    "    SKIP_ML = False  # Skip ML training for pipeline testing\n",
    "    \n",
    "    # ============= FEATURE ENGINEERING =============\n",
    "    TOP_FEATURES = 50  # Number of best features to select (increased for better accuracy)\n",
    "    FEATURE_DTYPE = np.float32  # Data type (float32 saves memory vs float64)\n",
    "    EXTRACT_CICIDS_FEATURES = True  # Extract CICIDS-specific features\n",
    "    \n",
    "    # ============= MACHINE LEARNING =============\n",
    "    TEST_SIZE = 0.2  # Fraction of data for testing\n",
    "    RANDOM_STATE = 42  # Random seed for reproducibility\n",
    "    SELECTED_MODELS = ['sgd', 'lightgbm', 'xgboost_incremental']  # Memory-efficient models\n",
    "    USE_INCREMENTAL_LEARNING = True  # Train in batches for large datasets\n",
    "    \n",
    "    # ============= STORAGE SETTINGS (Using Parquet) =============\n",
    "    STORAGE_FORMAT = 'parquet'  # Storage format (parquet recommended over hdf5)\n",
    "    USE_PARQUET = True  # Use Parquet format for efficient disk storage\n",
    "    COMPRESSION = 'snappy'  # Fast compression algorithm\n",
    "    CLEANUP_TEMP = True  # Delete temporary files after completion\n",
    "    \n",
    "    @classmethod\n",
    "    def check_memory_requirements(cls):\n",
    "        \"\"\"\n",
    "        Analyzes PCAP file size and available RAM to determine processing strategy.\n",
    "        \n",
    "        Returns:\n",
    "            str: 'DISK_BASED' for large files, 'MEMORY_BASED' for small files\n",
    "        \n",
    "        This method prevents out-of-memory errors by choosing appropriate\n",
    "        processing strategy before analysis begins.\n",
    "        \"\"\"\n",
    "        if not cls.PCAP_FILE or not os.path.exists(cls.PCAP_FILE):\n",
    "            return False\n",
    "            \n",
    "        file_size_gb = os.path.getsize(cls.PCAP_FILE) / (1024**3)\n",
    "        available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "        total_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MEMORY ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP Size: {file_size_gb:.2f} GB\")\n",
    "        print(f\"Total RAM: {total_gb:.2f} GB\")\n",
    "        print(f\"Available RAM: {available_gb:.2f} GB\")\n",
    "        print(f\"Max Memory Setting: {cls.MAX_MEMORY_GB:.2f} GB\")\n",
    "        \n",
    "        # Decision logic for processing strategy\n",
    "        if file_size_gb > available_gb * 0.3:  # File is >30% of available RAM\n",
    "            print(\"\\nRECOMMENDATION: Large file detected\")\n",
    "            print(\"- Using disk-based processing with Parquet\")\n",
    "            print(\"- Enabling flow timeout mechanism\")\n",
    "            print(\"- Using incremental learning\")\n",
    "            cls.USE_DISK_CACHE = True\n",
    "            cls.USE_INCREMENTAL_LEARNING = True\n",
    "            cls.USE_SAMPLING = True\n",
    "            processing_strategy = \"DISK_BASED\"\n",
    "        else:\n",
    "            print(\"\\nRECOMMENDATION: File can fit in memory\")\n",
    "            print(\"- Using hybrid processing\")\n",
    "            processing_strategy = \"MEMORY_BASED\"\n",
    "        \n",
    "        # Optimizations for AMD Ryzen CPU\n",
    "        cpu_count = psutil.cpu_count()\n",
    "        print(f\"\\nCPU Optimization: {cpu_count} threads available\")\n",
    "        cls.N_JOBS = cpu_count - 2  # Leave 2 threads for system\n",
    "        \n",
    "        # Time estimation (empirical: 1GB ≈ 2-4 minutes with optimizations)\n",
    "        estimated_time = file_size_gb * (3 if cls.USE_DISK_CACHE else 2)\n",
    "        print(f\"\\nEstimated Processing Time: {estimated_time:.0f} minutes\")\n",
    "        \n",
    "        return processing_strategy\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_interactive_ui(cls):\n",
    "        \"\"\"\n",
    "        Creates an interactive configuration interface using Jupyter widgets.\n",
    "        This provides a user-friendly way to set all parameters without\n",
    "        editing code directly.\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(\"<h2>Memory-Optimized Network Analysis Configuration (V2.0)</h2>\"))\n",
    "        \n",
    "        # ========== MEMORY SETTINGS SECTION ==========\n",
    "        display(HTML(\"<h3>Memory Optimization Settings</h3>\"))\n",
    "        \n",
    "        memory_slider = widgets.FloatSlider(\n",
    "            value=8.0,\n",
    "            min=1.0,\n",
    "            max=psutil.virtual_memory().total / (1024**3),\n",
    "            step=0.5,\n",
    "            description='Max RAM (GB):',\n",
    "            style={'description_width': 'initial'},\n",
    "            readout_format='.1f'\n",
    "        )\n",
    "        \n",
    "        max_flows = widgets.IntText(\n",
    "            value=100000,\n",
    "            description='Max Flows in Memory:',\n",
    "            tooltip='Flows to keep before flushing to disk',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        use_sampling = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Sampling for ML (recommended for large files)',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        sample_size = widgets.IntText(\n",
    "            value=1000000,\n",
    "            description='Sample Size:',\n",
    "            disabled=not use_sampling.value,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Skip ML checkbox for testing\n",
    "        skip_ml = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Skip ML Training (for testing pipeline)',\n",
    "            indent=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Storage format selection\n",
    "        storage_format = widgets.RadioButtons(\n",
    "            options=['parquet', 'csv'],\n",
    "            value='parquet',\n",
    "            description='Storage Format:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Dynamic UI: Enable/disable sample size based on checkbox\n",
    "        def toggle_sample_size(change):\n",
    "            sample_size.disabled = not change['new']\n",
    "        use_sampling.observe(toggle_sample_size, names='value')\n",
    "        \n",
    "        # ========== FILE SELECTION SECTION ==========\n",
    "        pcap_input = widgets.Text(\n",
    "            placeholder='/path/to/your.pcap',\n",
    "            description='PCAP File:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        label_input = widgets.Textarea(\n",
    "            placeholder='Enter CSV paths (one per line) or leave empty',\n",
    "            description='Label Files:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%', height='80px')\n",
    "        )\n",
    "        \n",
    "        output_dir = widgets.Text(\n",
    "            value='./analysis_output',\n",
    "            description='Output Dir:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # ========== PROCESSING OPTIONS ==========\n",
    "        chunk_slider = widgets.IntSlider(\n",
    "            value=50000,\n",
    "            min=10000,\n",
    "            max=100000,\n",
    "            step=10000,\n",
    "            description='Chunk Size:',\n",
    "            continuous_update=False,\n",
    "            tooltip='Packets processed at once',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        batch_slider = widgets.IntSlider(\n",
    "            value=200000,\n",
    "            min=50000,\n",
    "            max=500000,\n",
    "            step=50000,\n",
    "            description='Batch Size:',\n",
    "            continuous_update=False,\n",
    "            tooltip='Flows per ML batch',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # ========== ANALYSIS OPTIONS ==========\n",
    "        analysis_mode = widgets.RadioButtons(\n",
    "            options=['flow', 'semantic', 'combined'],\n",
    "            value='combined',\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        deep_inspection = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Enable NLP Deep Inspection',\n",
    "            tooltip='May increase processing time',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        # ========== ML MODEL SELECTION ==========\n",
    "        model_selector = widgets.SelectMultiple(\n",
    "            options=['sgd', 'lightgbm', 'xgboost_incremental', 'random_forest'],\n",
    "            value=['sgd', 'lightgbm'],\n",
    "            description='ML Models:',\n",
    "            tooltip='Memory-efficient models for large datasets',\n",
    "            rows=4,\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=skip_ml.value\n",
    "        )\n",
    "        \n",
    "        # Disable models when ML is skipped\n",
    "        def toggle_models(change):\n",
    "            model_selector.disabled = change['new']\n",
    "        skip_ml.observe(toggle_models, names='value')\n",
    "        \n",
    "        # Progress output area\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # ========== VALIDATE BUTTON ==========\n",
    "        validate_btn = widgets.Button(\n",
    "            description='Validate & Analyze Memory',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='250px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def validate_config(b):\n",
    "            \"\"\"\n",
    "            Callback function executed when validate button is clicked.\n",
    "            Validates all inputs and sets configuration values.\n",
    "            \"\"\"\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Get values from all widgets\n",
    "                cls.PCAP_FILE = pcap_input.value.strip().strip('\"')\n",
    "                cls.OUTPUT_DIR = output_dir.value\n",
    "                cls.MAX_MEMORY_GB = memory_slider.value\n",
    "                cls.MAX_FLOWS_IN_MEMORY = max_flows.value\n",
    "                cls.CHUNK_SIZE = chunk_slider.value\n",
    "                cls.BATCH_SIZE = batch_slider.value\n",
    "                cls.USE_SAMPLING = use_sampling.value\n",
    "                cls.SAMPLE_SIZE = sample_size.value\n",
    "                cls.ANALYSIS_MODE = analysis_mode.value\n",
    "                cls.DEEP_INSPECTION = deep_inspection.value\n",
    "                cls.SELECTED_MODELS = list(model_selector.value)\n",
    "                cls.SKIP_ML = skip_ml.value\n",
    "                cls.STORAGE_FORMAT = storage_format.value\n",
    "                cls.USE_PARQUET = (storage_format.value == 'parquet')\n",
    "                \n",
    "                # Process CSV label files\n",
    "                csv_lines = label_input.value.strip().split('\\n')\n",
    "                csv_files = [f.strip().strip('\"') for f in csv_lines if f.strip()]\n",
    "                \n",
    "                # Validate PCAP exists\n",
    "                if not os.path.exists(cls.PCAP_FILE):\n",
    "                    print(f\"Error: PCAP file not found: {cls.PCAP_FILE}\")\n",
    "                    return\n",
    "                \n",
    "                # Analyze memory requirements\n",
    "                strategy = cls.check_memory_requirements()\n",
    "                \n",
    "                # Validate CSV files if provided\n",
    "                if csv_files:\n",
    "                    cls.USE_CICIDS_LABELS = True\n",
    "                    valid_csvs = []\n",
    "                    for csv in csv_files:\n",
    "                        if os.path.exists(csv):\n",
    "                            valid_csvs.append(csv)\n",
    "                            print(f\"Found: {os.path.basename(csv)}\")\n",
    "                        else:\n",
    "                            print(f\"Warning: Not found: {csv}\")\n",
    "                    \n",
    "                    if valid_csvs:\n",
    "                        cls.LABEL_FILES = valid_csvs\n",
    "                \n",
    "                # Create required directories\n",
    "                os.makedirs(cls.OUTPUT_DIR, exist_ok=True)\n",
    "                os.makedirs(cls.TEMP_DIR, exist_ok=True)\n",
    "                \n",
    "                # Display final configuration\n",
    "                cls.display_config()\n",
    "                \n",
    "                print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "                print(f\"Storage Format: {cls.STORAGE_FORMAT.upper()}\")\n",
    "                if cls.SKIP_ML:\n",
    "                    print(\"ML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "                print(\"Ready to start analysis!\")\n",
    "        \n",
    "        validate_btn.on_click(validate_config)\n",
    "        \n",
    "        # ========== LAYOUT ORGANIZATION ==========\n",
    "        memory_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Memory Settings (Optimized for 32GB)</h4>\"),\n",
    "            memory_slider,\n",
    "            max_flows,\n",
    "            use_sampling,\n",
    "            sample_size\n",
    "        ])\n",
    "        \n",
    "        file_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Input/Output Files</h4>\"),\n",
    "            pcap_input,\n",
    "            label_input,\n",
    "            output_dir,\n",
    "            storage_format\n",
    "        ])\n",
    "        \n",
    "        processing_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Processing Settings</h4>\"),\n",
    "            chunk_slider,\n",
    "            batch_slider,\n",
    "            analysis_mode,\n",
    "            deep_inspection,\n",
    "            skip_ml\n",
    "        ])\n",
    "        \n",
    "        model_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>ML Models (Memory-Efficient)</h4>\"),\n",
    "            model_selector\n",
    "        ])\n",
    "        \n",
    "        # Display complete interface\n",
    "        display(widgets.VBox([\n",
    "            memory_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            file_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            processing_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            model_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            validate_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "        \n",
    "        return output_area\n",
    "    \n",
    "    @classmethod\n",
    "    def display_config(cls):\n",
    "        \"\"\"Displays a summary of the current configuration\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CONFIGURATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP: {os.path.basename(cls.PCAP_FILE)}\")\n",
    "        print(f\"Output: {cls.OUTPUT_DIR}\")\n",
    "        print(f\"Memory Limit: {cls.MAX_MEMORY_GB} GB\")\n",
    "        print(f\"Processing Strategy: {'Disk-based' if cls.USE_DISK_CACHE else 'Memory-based'}\")\n",
    "        print(f\"Storage Format: {cls.STORAGE_FORMAT}\")\n",
    "        print(f\"Chunk Size: {cls.CHUNK_SIZE:,} packets\")\n",
    "        print(f\"Batch Size: {cls.BATCH_SIZE:,} flows\")\n",
    "        print(f\"CPU Threads: {cls.N_JOBS if cls.N_JOBS > 0 else 'All available'}\")\n",
    "        if cls.USE_SAMPLING:\n",
    "            print(f\"ML Sample Size: {cls.SAMPLE_SIZE:,} flows\")\n",
    "        if cls.SKIP_ML:\n",
    "            print(\"ML Training: DISABLED (Test Mode)\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Run the interactive UI\n",
    "output = Config.setup_interactive_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0a899",
   "metadata": {},
   "source": [
    "# ### Step 2.5: (OPTIONAL) Dataset Preparation (For Testing & Debugging) Use this to make smaller PCAP file and csv to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.5: Test Dataset Preparation - Quick Validation Before Full Run\n",
    "\"\"\"\n",
    "PURPOSE: Create small test datasets to validate pipeline functionality\n",
    "This cell helps you test the entire pipeline in minutes instead of hours.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates a smaller PCAP file from your main file\n",
    "2. Extracts corresponding CSV label rows  \n",
    "3. Provides UI to customize test size\n",
    "4. Validates pipeline works before committing to full analysis\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Catches errors in 5 minutes instead of 4 hours\n",
    "- Validates file locking fixes work\n",
    "- Tests memory settings are appropriate\n",
    "- Confirms CSV label matching functions\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class TestDatasetCreator:\n",
    "    @staticmethod\n",
    "    def create_test_ui():\n",
    "        \"\"\"\n",
    "        Creates an interactive UI for generating test datasets\n",
    "        \"\"\"\n",
    "        display(HTML(\"<h2>Test Dataset Generator</h2>\"))\n",
    "        display(HTML(\"<p>Create small test files to validate pipeline before full run</p>\"))\n",
    "        \n",
    "        # Packet count slider\n",
    "        packet_count = widgets.IntSlider(\n",
    "            value=100000,\n",
    "            min=10000,\n",
    "            max=500000,\n",
    "            step=10000,\n",
    "            description='Test Packets:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of packets for test PCAP'\n",
    "        )\n",
    "        \n",
    "        # CSV row count\n",
    "        csv_rows = widgets.IntText(\n",
    "            value=5000,\n",
    "            description='CSV Rows:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of label rows to extract'\n",
    "        )\n",
    "        \n",
    "        # Source file input\n",
    "        source_pcap = widgets.Text(\n",
    "            value=Config.PCAP_FILE if Config.PCAP_FILE else '',\n",
    "            description='Source PCAP:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # Source CSV input  \n",
    "        source_csv = widgets.Textarea(\n",
    "            value='\\n'.join(Config.LABEL_FILES) if Config.LABEL_FILES else '',\n",
    "            description='Source CSVs:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%', height='80px'),\n",
    "            placeholder='Enter CSV paths (one per line)'\n",
    "        )\n",
    "        \n",
    "        # Output directory\n",
    "        test_dir = widgets.Text(\n",
    "            value='./test_data',\n",
    "            description='Test Directory:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Progress output\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # Create button\n",
    "        create_btn = widgets.Button(\n",
    "            description='Create Test Datasets',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def create_test_data(b):\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Get values\n",
    "                pcap_path = source_pcap.value.strip()\n",
    "                csv_paths = [f.strip() for f in source_csv.value.strip().split('\\n') if f.strip()]\n",
    "                test_path = test_dir.value\n",
    "                num_packets = packet_count.value\n",
    "                num_rows = csv_rows.value\n",
    "                \n",
    "                # Validate inputs\n",
    "                if not os.path.exists(pcap_path):\n",
    "                    print(f\" Error: PCAP file not found: {pcap_path}\")\n",
    "                    return\n",
    "                \n",
    "                # Create test directory\n",
    "                os.makedirs(test_path, exist_ok=True)\n",
    "                \n",
    "                print(\"=\"*60)\n",
    "                print(\"CREATING TEST DATASETS\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Create test PCAP\n",
    "                test_pcap = os.path.join(test_path, f'test_{num_packets}_packets.pcap')\n",
    "                print(f\"\\n1. Creating test PCAP with {num_packets:,} packets...\")\n",
    "                \n",
    "                try:\n",
    "                    # Use tcpdump to extract packets\n",
    "                    cmd = f'tcpdump -r \"{pcap_path}\" -w \"{test_pcap}\" -c {num_packets}'\n",
    "                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "                    \n",
    "                    if os.path.exists(test_pcap):\n",
    "                        size_mb = os.path.getsize(test_pcap) / (1024*1024)\n",
    "                        print(f\"   ✓ Created: {test_pcap}\")\n",
    "                        print(f\"   Size: {size_mb:.2f} MB\")\n",
    "                    else:\n",
    "                        # Fallback to Python method if tcpdump fails\n",
    "                        print(\"   tcpdump failed, using Python method...\")\n",
    "                        TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   Error creating PCAP: {e}\")\n",
    "                    print(\"   Trying Python-based extraction...\")\n",
    "                    TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                \n",
    "                # Create test CSVs\n",
    "                if csv_paths:\n",
    "                    print(f\"\\n2. Creating test CSVs with {num_rows:,} rows each...\")\n",
    "                    test_csvs = []\n",
    "                    \n",
    "                    for csv_path in csv_paths:\n",
    "                        if not os.path.exists(csv_path):\n",
    "                            print(f\"   CSV not found: {csv_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Read and subset CSV\n",
    "                        csv_name = os.path.basename(csv_path)\n",
    "                        test_csv = os.path.join(test_path, f'test_{csv_name}')\n",
    "                        \n",
    "                        try:\n",
    "                            df = pd.read_csv(csv_path, encoding='latin-1', nrows=num_rows)\n",
    "                            df.to_csv(test_csv, index=False)\n",
    "                            test_csvs.append(test_csv)\n",
    "                            \n",
    "                            # Show label distribution\n",
    "                            label_col = None\n",
    "                            for col in ['Label', 'label', ' Label']:\n",
    "                                if col in df.columns:\n",
    "                                    label_col = col\n",
    "                                    break\n",
    "                            \n",
    "                            if label_col:\n",
    "                                print(f\"\\n   ✓ Created: {test_csv}\")\n",
    "                                print(f\"   Label distribution:\")\n",
    "                                for label, count in df[label_col].value_counts().head(5).items():\n",
    "                                    print(f\"      {label}: {count}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"   Error processing {csv_name}: {e}\")\n",
    "                else:\n",
    "                    test_csvs = []\n",
    "                    print(\"\\n2. No CSV files provided, skipping label extraction\")\n",
    "                \n",
    "                # Generate configuration code\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"TEST CONFIGURATION\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"\\nAdd this to your Config or use directly:\\n\")\n",
    "                print(f\"Config.PCAP_FILE = r'{test_pcap}'\")\n",
    "                print(f\"Config.LABEL_FILES = {test_csvs}\")\n",
    "                print(f\"Config.MAX_PACKETS = 0  # Process all packets in test file\")\n",
    "                print(f\"Config.SAMPLE_SIZE = {min(50000, num_packets // 2)}\")\n",
    "                \n",
    "                # Estimate time\n",
    "                est_time = num_packets / 20000  # ~20K packets per minute\n",
    "                print(f\"\\nEstimated test time: {est_time:.1f} minutes\")\n",
    "                print(\"\\n Test datasets created successfully!\")\n",
    "                print(\"   Run the pipeline with these files to validate before full analysis\")\n",
    "        \n",
    "        create_btn.on_click(create_test_data)\n",
    "        \n",
    "        # Layout\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Test Size Configuration</h4>\"),\n",
    "            packet_count,\n",
    "            csv_rows,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Source Files</h4>\"),\n",
    "            source_pcap,\n",
    "            source_csv,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Output Location</h4>\"),\n",
    "            test_dir,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            create_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_test_pcap_python(source_pcap, output_pcap, packet_count):\n",
    "        \"\"\"\n",
    "        Python fallback method to create test PCAP if tcpdump unavailable\n",
    "        \"\"\"\n",
    "        from scapy.all import PcapReader, PcapWriter\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(source_pcap) as reader:\n",
    "                with PcapWriter(output_pcap) as writer:\n",
    "                    for i, packet in enumerate(reader):\n",
    "                        if i >= packet_count:\n",
    "                            break\n",
    "                        writer.write(packet)\n",
    "            \n",
    "            if os.path.exists(output_pcap):\n",
    "                size_mb = os.path.getsize(output_pcap) / (1024*1024)\n",
    "                print(f\"   ✓ Created using Python: {output_pcap}\")\n",
    "                print(f\"   Size: {size_mb:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error with Python method: {e}\")\n",
    "\n",
    "# Run the UI\n",
    "TestDatasetCreator.create_test_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01c0ab",
   "metadata": {},
   "source": [
    "# ### Step 3:  Flow Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8900f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Flow Feature Extractor - Network Traffic Statistical Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Extract statistical features from network flows\n",
    "This class analyzes network packets and groups them into flows, then extracts\n",
    "statistical features that help identify malicious traffic patterns.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Groups packets into bidirectional flows (conversations between hosts)\n",
    "2. Extracts per-packet features (size, flags, ports, timing)\n",
    "3. Aggregates packet features into flow-level statistics\n",
    "4. Implements CICFlowMeter-style feature extraction\n",
    "5. Manages memory by flushing old flows to disk\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FLOW: A sequence of packets between two endpoints (identified by 5-tuple)\n",
    "- 5-TUPLE: (src_ip, src_port, dst_ip, dst_port, protocol)\n",
    "- BIDIRECTIONAL: Treats A→B and B→A as the same flow\n",
    "- FLOW TIMEOUT: After 120 seconds of inactivity, flow is considered complete\n",
    "\n",
    "FEATURES EXTRACTED:\n",
    "1. Timing Features:\n",
    "   - Flow duration\n",
    "   - Inter-arrival times (IAT) statistics\n",
    "   - Packets/bytes per second\n",
    "   - Active/Idle time statistics (CICIDS-specific)\n",
    "\n",
    "2. Size Features:\n",
    "   - Packet length statistics (min, max, mean, std)\n",
    "   - Total bytes/packets\n",
    "   - Payload sizes\n",
    "   - Forward/Backward packet statistics\n",
    "\n",
    "3. TCP Flag Features:\n",
    "   - SYN, ACK, FIN, RST, PSH, URG counts\n",
    "   - Used to detect scanning, flooding attacks\n",
    "\n",
    "4. Protocol Features:\n",
    "   - TTL values\n",
    "   - Port numbers\n",
    "   - Protocol type\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Flushes flows to Parquet when memory limit reached\n",
    "- Uses flow timeout to prevent infinite accumulation\n",
    "- Stores features as float32 instead of float64\n",
    "- Caps packets per flow to prevent memory bloat\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Fixed bidirectional flow identification bug\n",
    "- Using Parquet instead of HDF5 (no file locking)\n",
    "- Added CICIDS-specific features (Active/Idle, Fwd/Bwd separation)\n",
    "- Optimized entropy calculation with numpy\n",
    "- Added packet count cap per flow\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFlowExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000, flow_timeout=120):\n",
    "        \"\"\"\n",
    "        Initialize flow feature extractor with memory management.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum concurrent flows before flushing to disk\n",
    "            flow_timeout: Seconds of inactivity before flow is complete\n",
    "        \"\"\"\n",
    "        self.flows = {}  # Dictionary to store active flows\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.flow_timeout = flow_timeout\n",
    "        self.flow_counter = 0  # Total flows seen\n",
    "        self.batch_counter = 0  # Number of batches written to disk\n",
    "        \n",
    "        # Create Parquet storage directory (replacing HDF5)\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'flow_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pre-compile optimizations\n",
    "        self.setup_optimizations()\n",
    "    \n",
    "    def setup_optimizations(self):\n",
    "        \"\"\"\n",
    "        Pre-compile and optimize frequently used operations.\n",
    "        This improves performance significantly.\n",
    "        \"\"\"\n",
    "        # Pre-calculate byte frequency table for entropy\n",
    "        self.byte_frequencies = np.zeros(256, dtype=np.float32)\n",
    "        \n",
    "        # Maximum packets per flow to prevent memory issues\n",
    "        self.max_packets_per_flow = Config.MAX_PACKETS_PER_FLOW\n",
    "    \n",
    "    def get_flow_id(self, packet):\n",
    "        \"\"\"\n",
    "        Generate unique identifier for a network flow.\n",
    "        FIXED: Properly handles bidirectional flow identification.\n",
    "        \n",
    "        Uses 5-tuple (IPs, ports, protocol) to identify flows.\n",
    "        Makes flows bidirectional by sorting endpoints correctly.\n",
    "        \n",
    "        Returns:\n",
    "            str: 16-character hash identifying the flow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if IP not in packet:\n",
    "                return None\n",
    "                \n",
    "            src_ip = packet[IP].src\n",
    "            dst_ip = packet[IP].dst\n",
    "            proto = packet[IP].proto\n",
    "            \n",
    "            # Extract ports if TCP/UDP\n",
    "            src_port = dst_port = 0\n",
    "            if TCP in packet:\n",
    "                src_port = packet[TCP].sport\n",
    "                dst_port = packet[TCP].dport\n",
    "            elif UDP in packet:\n",
    "                src_port = packet[UDP].sport\n",
    "                dst_port = packet[UDP].dport\n",
    "            \n",
    "            # FIXED: Proper bidirectional flow identification\n",
    "            # Sort by IP first, then by port if IPs are equal\n",
    "            if src_ip < dst_ip:\n",
    "                flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{proto}\"\n",
    "            elif src_ip > dst_ip:\n",
    "                flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{proto}\"\n",
    "            else:  # Same IP (rare but possible)\n",
    "                if src_port <= dst_port:\n",
    "                    flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{proto}\"\n",
    "                else:\n",
    "                    flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{proto}\"\n",
    "            \n",
    "            # Create hash for efficient lookup\n",
    "            return hashlib.md5(flow_key.encode()).hexdigest()[:16]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_packet_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract features from a single packet.\n",
    "        These features will be aggregated into flow statistics.\n",
    "        \n",
    "        Features include:\n",
    "        - Basic: timestamp, packet length, direction\n",
    "        - IP: TTL, protocol, IPs (for direction determination)\n",
    "        - TCP: flags, window size, ports\n",
    "        - Payload: size and entropy (randomness)\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            # Basic packet features\n",
    "            features['timestamp'] = float(packet.time)\n",
    "            features['packet_length'] = len(packet)\n",
    "            \n",
    "            # IP layer features\n",
    "            if IP in packet:\n",
    "                features['ttl'] = packet[IP].ttl\n",
    "                features['protocol'] = packet[IP].proto\n",
    "                features['src_ip'] = packet[IP].src  # For direction determination\n",
    "                features['dst_ip'] = packet[IP].dst\n",
    "                \n",
    "            # TCP layer features\n",
    "            if TCP in packet:\n",
    "                features['tcp_flags'] = int(packet[TCP].flags)\n",
    "                features['window_size'] = packet[TCP].window\n",
    "                features['src_port'] = packet[TCP].sport\n",
    "                features['dst_port'] = packet[TCP].dport\n",
    "                \n",
    "                # Individual flag extraction (for detecting attacks)\n",
    "                tcp_flags = packet[TCP].flags\n",
    "                features['flag_syn'] = bool(tcp_flags & 2)   # SYN flag\n",
    "                features['flag_ack'] = bool(tcp_flags & 16)  # ACK flag\n",
    "                features['flag_fin'] = bool(tcp_flags & 1)   # FIN flag\n",
    "                features['flag_rst'] = bool(tcp_flags & 4)   # RST flag\n",
    "                features['flag_psh'] = bool(tcp_flags & 8)   # PSH flag\n",
    "                features['flag_urg'] = bool(tcp_flags & 32)  # URG flag (CICIDS feature)\n",
    "                \n",
    "            # UDP layer features\n",
    "            elif UDP in packet:\n",
    "                features['src_port'] = packet[UDP].sport\n",
    "                features['dst_port'] = packet[UDP].dport\n",
    "                \n",
    "            # Payload features (important for detecting malware)\n",
    "            if Raw in packet:\n",
    "                payload = bytes(packet[Raw])\n",
    "                features['payload_size'] = len(payload)\n",
    "                features['payload_entropy'] = self.fast_entropy(payload)\n",
    "            else:\n",
    "                features['payload_size'] = 0\n",
    "                features['payload_entropy'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def fast_entropy(self, data):\n",
    "        \"\"\"\n",
    "        OPTIMIZED: Calculate Shannon entropy using numpy for speed.\n",
    "        High entropy suggests encryption/compression (possibly malware).\n",
    "        Low entropy suggests plain text.\n",
    "        \n",
    "        Returns:\n",
    "            float: Entropy value (0-8 bits)\n",
    "        \"\"\"\n",
    "        if not data or len(data) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Use numpy for fast calculation\n",
    "        counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)\n",
    "        probs = counts[counts > 0] / len(data)\n",
    "        return -np.sum(probs * np.log2(probs))\n",
    "    \n",
    "    def flush_old_flows(self, current_time, force_all=False):\n",
    "        \"\"\"\n",
    "        Save completed flows to disk using Parquet format.\n",
    "        A flow is complete if it hasn't seen packets for flow_timeout seconds.\n",
    "        \n",
    "        IMPORTANT: Using Parquet instead of HDF5 for Windows reliability.\n",
    "        \n",
    "        Args:\n",
    "            current_time: Timestamp of current packet\n",
    "            force_all: Force flush all flows regardless of timeout\n",
    "        \"\"\"\n",
    "        flows_to_flush = []\n",
    "        \n",
    "        # Identify flows that have timed out\n",
    "        for flow_id, flow_data in self.flows.items():\n",
    "            packets = flow_data['packets']\n",
    "            if not packets:\n",
    "                continue\n",
    "                \n",
    "            last_packet_time = packets[-1]['timestamp']\n",
    "            time_since_last = current_time - last_packet_time\n",
    "            \n",
    "            if force_all or time_since_last > self.flow_timeout:\n",
    "                # Aggregate packet features into flow features\n",
    "                features = self.aggregate_flow_features(flow_data)\n",
    "                if features:\n",
    "                    features['flow_id'] = flow_id\n",
    "                    flows_to_flush.append(features)\n",
    "        \n",
    "        # Save to disk if we have flows to flush\n",
    "        if flows_to_flush:\n",
    "            # Convert to DataFrame\n",
    "            df_batch = pd.DataFrame(flows_to_flush)\n",
    "            \n",
    "            # Optimize memory usage with proper dtypes\n",
    "            df_batch = self.optimize_dtypes(df_batch)\n",
    "            \n",
    "            # Save to Parquet (no file locking issues!)\n",
    "            batch_file = os.path.join(\n",
    "                self.output_dir, \n",
    "                f'batch_{self.batch_counter:04d}.parquet'\n",
    "            )\n",
    "            df_batch.to_parquet(\n",
    "                batch_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            self.batch_counter += 1\n",
    "            \n",
    "            # Remove flushed flows from memory\n",
    "            for features in flows_to_flush:\n",
    "                del self.flows[features['flow_id']]\n",
    "            \n",
    "            print(f\"  Flushed {len(flows_to_flush):,} flows to {batch_file}\")\n",
    "            \n",
    "            # Force garbage collection to free memory\n",
    "            gc.collect()\n",
    "    \n",
    "    def optimize_dtypes(self, df):\n",
    "        \"\"\"\n",
    "        Optimize DataFrame dtypes for memory efficiency and storage.\n",
    "        Specifically optimized for CICIDS features.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            \n",
    "            # Flag counts are small integers\n",
    "            if 'flag' in col_lower or '_count' in col_lower:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            # Ports fit in int16\n",
    "            elif 'port' in col_lower:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            # Packet/byte counts\n",
    "            elif 'packet' in col_lower or 'byte' in col_lower:\n",
    "                if 'rate' not in col_lower and '/s' not in col_lower:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "            # Float features\n",
    "            elif df[col].dtype == np.float64:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def aggregate_flow_features(self, flow_data):\n",
    "        \"\"\"\n",
    "        Aggregate packet-level features into flow-level statistics.\n",
    "        This is where we calculate the actual features used for ML.\n",
    "        \n",
    "        ENHANCED: Now includes CICIDS-specific features like Active/Idle times\n",
    "        and Forward/Backward packet separation.\n",
    "        \n",
    "        Creates statistical summaries that capture flow behavior:\n",
    "        - Duration and timing patterns\n",
    "        - Size distributions\n",
    "        - Flag patterns\n",
    "        - Rate calculations\n",
    "        - Active/Idle statistics (CICIDS)\n",
    "        - Forward/Backward statistics (CICIDS)\n",
    "        \"\"\"\n",
    "        packets = flow_data['packets']\n",
    "        if not packets:\n",
    "            return None\n",
    "            \n",
    "        # Sort packets by timestamp\n",
    "        packets = sorted(packets, key=lambda x: x.get('timestamp', 0))\n",
    "        \n",
    "        # Determine flow direction based on first packet\n",
    "        first_packet = packets[0]\n",
    "        flow_src_ip = first_packet.get('src_ip', '')\n",
    "        \n",
    "        # Separate forward and backward packets (CICIDS feature)\n",
    "        fwd_packets = [p for p in packets if p.get('src_ip') == flow_src_ip]\n",
    "        bwd_packets = [p for p in packets if p.get('src_ip') != flow_src_ip]\n",
    "        \n",
    "        # Extract timestamp array\n",
    "        timestamps = [p['timestamp'] for p in packets]\n",
    "        duration = max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0\n",
    "        \n",
    "        # Calculate inter-arrival times (time between packets)\n",
    "        iats = np.diff(timestamps) if len(timestamps) > 1 else [0]\n",
    "        \n",
    "        # Active/Idle time calculation (CICIDS-specific)\n",
    "        active_times = []\n",
    "        idle_times = []\n",
    "        if len(iats) > 0:\n",
    "            active_times = [t for t in iats if t < 1.0]  # Active if IAT < 1 second\n",
    "            idle_times = [t for t in iats if t >= 1.0]    # Idle if IAT >= 1 second\n",
    "        \n",
    "        # Build feature dictionary\n",
    "        features = {\n",
    "            # ===== BASIC FLOW STATISTICS =====\n",
    "            'flow_duration': duration,\n",
    "            'total_packets': len(packets),\n",
    "            'total_bytes': sum(p.get('packet_length', 0) for p in packets),\n",
    "            \n",
    "            # ===== FORWARD DIRECTION STATISTICS (CICIDS) =====\n",
    "            'fwd_packets': len(fwd_packets),\n",
    "            'fwd_bytes': sum(p.get('packet_length', 0) for p in fwd_packets),\n",
    "            'fwd_packet_length_max': max([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_min': min([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_mean': np.mean([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_std': np.std([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            \n",
    "            # ===== BACKWARD DIRECTION STATISTICS (CICIDS) =====\n",
    "            'bwd_packets': len(bwd_packets),\n",
    "            'bwd_bytes': sum(p.get('packet_length', 0) for p in bwd_packets),\n",
    "            'bwd_packet_length_max': max([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_min': min([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_mean': np.mean([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_std': np.std([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            \n",
    "            # ===== INTER-ARRIVAL TIME STATISTICS =====\n",
    "            'flow_iat_mean': np.mean(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_std': np.std(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_max': max(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_min': min(iats) if len(iats) > 0 else 0,\n",
    "            \n",
    "            # ===== ACTIVE/IDLE TIME STATISTICS (CICIDS) =====\n",
    "            'active_mean': np.mean(active_times) if active_times else 0,\n",
    "            'active_std': np.std(active_times) if active_times else 0,\n",
    "            'active_max': max(active_times) if active_times else 0,\n",
    "            'active_min': min(active_times) if active_times else 0,\n",
    "            'idle_mean': np.mean(idle_times) if idle_times else 0,\n",
    "            'idle_std': np.std(idle_times) if idle_times else 0,\n",
    "            'idle_max': max(idle_times) if idle_times else 0,\n",
    "            'idle_min': min(idle_times) if idle_times else 0,\n",
    "            \n",
    "            # ===== FLOW RATE FEATURES =====\n",
    "            'packets_per_second': len(packets) / duration if duration > 0 else 0,\n",
    "            'bytes_per_second': sum(p.get('packet_length', 0) for p in packets) / duration if duration > 0 else 0,\n",
    "            \n",
    "            # ===== PROTOCOL FEATURES =====\n",
    "            'avg_ttl': np.mean([p.get('ttl', 0) for p in packets]),\n",
    "            'protocol': packets[0].get('protocol', 0) if packets else 0,\n",
    "            \n",
    "            # ===== TCP FLAG STATISTICS (Enhanced for CICIDS) =====\n",
    "            'syn_count': sum(p.get('flag_syn', 0) for p in packets),\n",
    "            'ack_count': sum(p.get('flag_ack', 0) for p in packets),\n",
    "            'fin_count': sum(p.get('flag_fin', 0) for p in packets),\n",
    "            'rst_count': sum(p.get('flag_rst', 0) for p in packets),\n",
    "            'psh_count': sum(p.get('flag_psh', 0) for p in packets),\n",
    "            'urg_count': sum(p.get('flag_urg', 0) for p in packets),  # Added URG flag\n",
    "            \n",
    "            # Forward/Backward flag counts (CICIDS)\n",
    "            'fwd_psh_flags': sum(p.get('flag_psh', 0) for p in fwd_packets),\n",
    "            'bwd_psh_flags': sum(p.get('flag_psh', 0) for p in bwd_packets),\n",
    "            'fwd_urg_flags': sum(p.get('flag_urg', 0) for p in fwd_packets),\n",
    "            'bwd_urg_flags': sum(p.get('flag_urg', 0) for p in bwd_packets),\n",
    "            \n",
    "            # ===== PAYLOAD STATISTICS =====\n",
    "            'total_payload_bytes': sum(p.get('payload_size', 0) for p in packets),\n",
    "            'avg_payload_size': np.mean([p.get('payload_size', 0) for p in packets]),\n",
    "            'avg_entropy': np.mean([p.get('payload_entropy', 0) for p in packets]),\n",
    "            \n",
    "            # ===== PORT INFORMATION =====\n",
    "            'src_port': packets[0].get('src_port', 0) if packets else 0,\n",
    "            'dst_port': packets[0].get('dst_port', 0) if packets else 0,\n",
    "        }\n",
    "        \n",
    "        # Convert to float32 for memory efficiency\n",
    "        for key in features:\n",
    "            if isinstance(features[key], (int, float)):\n",
    "                features[key] = np.float32(features[key])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Main processing function - reads PCAP and extracts flow features.\n",
    "        Implements streaming processing to handle files larger than RAM.\n",
    "        \n",
    "        ENHANCED: Better memory management with packet count limits per flow.\n",
    "        \n",
    "        Processing steps:\n",
    "        1. Read packets one by one\n",
    "        2. Group into flows (with packet limit per flow)\n",
    "        3. Extract features\n",
    "        4. Flush to Parquet when memory limit reached\n",
    "        5. Handle timeouts for inactive flows\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory containing Parquet files\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing PCAP with memory optimization: {pcap_file}\")\n",
    "        print(f\"Max flows in memory: {self.max_flows_in_memory:,}\")\n",
    "        print(f\"Max packets per flow: {self.max_packets_per_flow:,}\")\n",
    "        print(f\"Flow timeout: {self.flow_timeout} seconds\")\n",
    "        print(f\"Output format: Parquet (Windows-optimized)\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        last_flush_time = None\n",
    "        \n",
    "        try:\n",
    "            # Open PCAP file for streaming read\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                # Process packets one by one\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting flow features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    # Check packet limit\n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow identifier\n",
    "                    flow_id = self.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract packet features\n",
    "                    packet_features = self.extract_packet_features(packet)\n",
    "                    current_time = packet_features.get('timestamp', 0)\n",
    "                    \n",
    "                    # Initialize flow if new\n",
    "                    if flow_id not in self.flows:\n",
    "                        self.flows[flow_id] = {\n",
    "                            'packets': [],\n",
    "                            'stats': defaultdict(float)  # Running statistics\n",
    "                        }\n",
    "                        self.flow_counter += 1\n",
    "                    \n",
    "                    # Add packet to flow (with limit to prevent memory bloat)\n",
    "                    if len(self.flows[flow_id]['packets']) < self.max_packets_per_flow:\n",
    "                        self.flows[flow_id]['packets'].append(packet_features)\n",
    "                    else:\n",
    "                        # Update statistics without storing packet\n",
    "                        self.update_flow_stats(self.flows[flow_id]['stats'], packet_features)\n",
    "                    \n",
    "                    # Check if memory limit reached\n",
    "                    if len(self.flows) >= self.max_flows_in_memory:\n",
    "                        print(f\"\\n  Memory limit reached at packet {packet_count:,}\")\n",
    "                        self.flush_old_flows(current_time)\n",
    "                    \n",
    "                    # Periodic timeout check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        # Check for timed-out flows\n",
    "                        if last_flush_time and (current_time - last_flush_time) > self.flow_timeout:\n",
    "                            self.flush_old_flows(current_time)\n",
    "                            last_flush_time = current_time\n",
    "                        \n",
    "                        # Monitor system memory\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory usage ({mem_percent:.1f}%), flushing flows...\")\n",
    "                            self.flush_old_flows(current_time, force_all=True)\n",
    "                        \n",
    "                        gc.collect()\n",
    "            \n",
    "            # Flush all remaining flows\n",
    "            print(\"\\nFlushing remaining flows...\")\n",
    "            self.flush_old_flows(float('inf'), force_all=True)\n",
    "            \n",
    "            print(f\"\\nProcessed {packet_count:,} packets\")\n",
    "            print(f\"Total flows: {self.flow_counter:,}\")\n",
    "            print(f\"Features saved to: {self.output_dir}\")\n",
    "            \n",
    "            return self.output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PCAP: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def update_flow_stats(self, stats, packet_features):\n",
    "        \"\"\"\n",
    "        Update running statistics for a flow when packet limit is exceeded.\n",
    "        This allows us to track flow statistics without storing all packets.\n",
    "        \"\"\"\n",
    "        stats['packet_count'] += 1\n",
    "        stats['byte_count'] += packet_features.get('packet_length', 0)\n",
    "        stats['payload_total'] += packet_features.get('payload_size', 0)\n",
    "        \n",
    "        # Update flag counts\n",
    "        for flag in ['syn', 'ack', 'fin', 'rst', 'psh', 'urg']:\n",
    "            if packet_features.get(f'flag_{flag}', False):\n",
    "                stats[f'{flag}_count'] += 1\n",
    "    \n",
    "    def load_features_iterator(self, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Generator to load features in batches from Parquet files.\n",
    "        Allows processing results without loading all data into memory.\n",
    "        \n",
    "        Yields:\n",
    "            DataFrame: Batch of flow features\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        parquet_files = sorted(glob.glob(os.path.join(self.output_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        accumulated = []\n",
    "        accumulated_size = 0\n",
    "        \n",
    "        for file in parquet_files:\n",
    "            batch_df = pd.read_parquet(file)\n",
    "            accumulated.append(batch_df)\n",
    "            accumulated_size += len(batch_df)\n",
    "            \n",
    "            if accumulated_size >= batch_size:\n",
    "                combined_df = pd.concat(accumulated, ignore_index=True)\n",
    "                yield combined_df\n",
    "                accumulated = []\n",
    "                accumulated_size = 0\n",
    "        \n",
    "        # Yield remaining data\n",
    "        if accumulated:\n",
    "            yield pd.concat(accumulated, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaa6bc",
   "metadata": {},
   "source": [
    "# ### Step4: Semantic Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de65aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Semantic Feature Extractor - Deep Packet Inspection & NLP Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Analyze packet payloads for malicious content using pattern matching and NLP\n",
    "This class performs deep packet inspection to detect attack signatures in payload content.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Examines packet payloads (actual data being transmitted)\n",
    "2. Detects attack patterns (SQL injection, XSS, command injection)\n",
    "3. Performs NLP analysis to find obfuscated attacks\n",
    "4. Calculates entropy and encoding detection\n",
    "5. Identifies suspicious URLs and domain names\n",
    "\n",
    "KEY DETECTION CAPABILITIES:\n",
    "1. SQL Injection: SELECT, UNION, DROP TABLE patterns\n",
    "2. Cross-Site Scripting (XSS): <script>, javascript:, alert()\n",
    "3. Command Injection: bash commands, system calls\n",
    "4. Directory Traversal: ../, /etc/passwd\n",
    "5. Encoding Detection: Base64, hex, URL encoding\n",
    "6. Obfuscation: Unusual character patterns, high entropy\n",
    "\n",
    "WHY SEMANTIC ANALYSIS MATTERS:\n",
    "- Flow features only see traffic patterns, not content\n",
    "- Many attacks hide in seemingly normal traffic\n",
    "- Attackers use encoding/obfuscation to evade detection\n",
    "- NLP helps detect variations of known attacks\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Processes payloads in streaming fashion\n",
    "- Limits payload analysis to first 1000 characters\n",
    "- Flushes results to Parquet periodically\n",
    "- Uses simplified NLP for speed\n",
    "- Pre-compiles regex patterns\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Pre-compiled regex patterns for better performance\n",
    "- Optimized entropy calculation with numpy\n",
    "- Added more comprehensive attack patterns\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedSemanticExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000):\n",
    "        \"\"\"\n",
    "        Initialize semantic analyzer with pattern databases and NLP components.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum flows before flushing to disk\n",
    "        \"\"\"\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.semantic_data = {}  # Stores semantic features per flow\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "        # Create Parquet storage directory\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'semantic_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pre-compile all regex patterns for performance\n",
    "        self.compile_attack_patterns()\n",
    "        \n",
    "        # Initialize NLP analyzer (simplified version for memory efficiency)\n",
    "        self.nlp_analyzer = SimplifiedNLPAnalyzer()\n",
    "    \n",
    "    def compile_attack_patterns(self):\n",
    "        \"\"\"\n",
    "        Pre-compile regex patterns for better performance.\n",
    "        Compiled patterns are much faster than compiling on each use.\n",
    "        \"\"\"\n",
    "        # ===== SQL INJECTION PATTERNS =====\n",
    "        sql_patterns = [\n",
    "            r'SELECT.*FROM',      # Basic SELECT query\n",
    "            r'INSERT.*INTO',      # INSERT injection\n",
    "            r'UPDATE.*SET',       # UPDATE injection\n",
    "            r'DELETE.*FROM',      # DELETE injection\n",
    "            r'DROP.*TABLE',       # Table dropping\n",
    "            r'UNION.*SELECT',     # UNION-based injection\n",
    "            r'OR\\s+1\\s*=\\s*1',   # Classic bypass: OR 1=1\n",
    "            r'--\\s*$',           # SQL comment injection\n",
    "            r';\\s*EXEC',         # Command execution\n",
    "            r'xp_cmdshell',      # SQL Server command execution\n",
    "            r'WAITFOR\\s+DELAY',  # Time-based blind SQL\n",
    "            r'BENCHMARK\\s*\\(',   # MySQL benchmark attack\n",
    "        ]\n",
    "        self.sql_patterns = [re.compile(p, re.IGNORECASE) for p in sql_patterns]\n",
    "        \n",
    "        # ===== COMMAND INJECTION PATTERNS =====\n",
    "        cmd_patterns = [\n",
    "            r';\\s*ls\\s+',        # List directory (Linux)\n",
    "            r';\\s*cat\\s+',       # Read file (Linux)\n",
    "            r';\\s*wget\\s+',      # Download file\n",
    "            r';\\s*curl\\s+',      # HTTP request tool\n",
    "            r';\\s*nc\\s+',        # Netcat (backdoor tool)\n",
    "            r'/etc/passwd',      # Common target file\n",
    "            r'/etc/shadow',      # Password hashes\n",
    "            r'cmd\\.exe',         # Windows command prompt\n",
    "            r'powershell',       # Windows PowerShell\n",
    "            r'bash\\s+-c',        # Bash command execution\n",
    "            r'sh\\s+-c',          # Shell command execution\n",
    "            r'eval\\s*\\(',        # Code evaluation\n",
    "            r'exec\\s*\\(',        # Code execution\n",
    "            r'system\\s*\\(',      # System call\n",
    "        ]\n",
    "        self.cmd_patterns = [re.compile(p, re.IGNORECASE) for p in cmd_patterns]\n",
    "        \n",
    "        # ===== XSS/SCRIPT INJECTION PATTERNS =====\n",
    "        script_patterns = [\n",
    "            r'<script',          # Script tag injection\n",
    "            r'javascript:',      # JavaScript protocol\n",
    "            r'onerror\\s*=',     # Event handler injection\n",
    "            r'onclick\\s*=',     # Click event injection\n",
    "            r'onload\\s*=',      # Load event injection\n",
    "            r'alert\\s*\\(',      # JavaScript alert\n",
    "            r'document\\.cookie', # Cookie theft\n",
    "            r'document\\.location', # Redirection\n",
    "            r'eval\\s*\\(',       # Code evaluation\n",
    "            r'<iframe',         # IFrame injection\n",
    "            r'<embed',          # Embed tag injection\n",
    "            r'<object',         # Object tag injection\n",
    "            r'<img.*src.*=',    # Image injection\n",
    "        ]\n",
    "        self.script_patterns = [re.compile(p, re.IGNORECASE) for p in script_patterns]\n",
    "        \n",
    "        # ===== DIRECTORY TRAVERSAL PATTERNS =====\n",
    "        traversal_patterns = [\n",
    "            r'\\.\\./\\.\\./\\.\\./', # Multiple traversals\n",
    "            r'\\.\\.\\\\\\.\\.\\\\\\.\\.\\\\'  # Windows traversals\n",
    "        ]\n",
    "        self.traversal_patterns = [re.compile(p) for p in traversal_patterns]\n",
    "    \n",
    "    def flush_semantic_features(self):\n",
    "        \"\"\"\n",
    "        Save accumulated semantic features to Parquet and free memory.\n",
    "        Called when memory limit is reached.\n",
    "        \n",
    "        Using Parquet instead of HDF5 for Windows reliability.\n",
    "        \"\"\"\n",
    "        if not self.semantic_data:\n",
    "            return\n",
    "        \n",
    "        # Convert dictionary to DataFrame\n",
    "        df_batch = pd.DataFrame.from_dict(self.semantic_data, orient='index')\n",
    "        df_batch['flow_id'] = df_batch.index\n",
    "        df_batch = df_batch.reset_index(drop=True)\n",
    "        \n",
    "        # Optimize data types\n",
    "        for col in df_batch.select_dtypes(include=[np.float64]).columns:\n",
    "            df_batch[col] = df_batch[col].astype(np.float32)\n",
    "        \n",
    "        # Save to Parquet\n",
    "        batch_file = os.path.join(\n",
    "            self.output_dir,\n",
    "            f'batch_{self.batch_counter:04d}.parquet'\n",
    "        )\n",
    "        df_batch.to_parquet(\n",
    "            batch_file,\n",
    "            engine='pyarrow',\n",
    "            compression='snappy',\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        self.batch_counter += 1\n",
    "        print(f\"  Flushed {len(df_batch):,} semantic features to {batch_file}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        self.semantic_data.clear()\n",
    "        gc.collect()\n",
    "    \n",
    "    def extract_semantic_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract semantic features from packet payload.\n",
    "        Analyzes actual data content for attack signatures.\n",
    "        \n",
    "        OPTIMIZED: Uses pre-compiled patterns and limits payload analysis.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Semantic feature values\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            # ===== PROTOCOL INDICATORS =====\n",
    "            'has_http': 0,       # HTTP traffic\n",
    "            'has_dns': 0,        # DNS queries\n",
    "            'has_smtp': 0,       # Email traffic\n",
    "            'has_tls': 0,        # TLS/SSL traffic\n",
    "            \n",
    "            # ===== ATTACK INDICATORS =====\n",
    "            'has_sql': 0,        # SQL injection detected\n",
    "            'has_cmd': 0,        # Command injection detected\n",
    "            'has_script': 0,     # Script injection detected\n",
    "            'has_traversal': 0,  # Directory traversal detected\n",
    "            \n",
    "            # ===== SCORING =====\n",
    "            'suspicious_score': 0,  # Overall suspicion level\n",
    "            'content_length': 0,    # Payload size\n",
    "            'attack_pattern_count': 0,  # Total patterns matched\n",
    "            \n",
    "            # ===== NLP FEATURES =====\n",
    "            'nlp_malicious_confidence': 0,  # NLP-based threat score\n",
    "            'nlp_pattern_score': 0,         # Pattern matching score\n",
    "            'nlp_entropy_score': 0,         # Randomness score\n",
    "            'nlp_encoding_detected': 0      # Encoding/obfuscation detected\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check for HTTP\n",
    "            if packet.haslayer('HTTP'):\n",
    "                features['has_http'] = 1\n",
    "                \n",
    "                # Extract HTTP-specific features\n",
    "                if hasattr(packet['HTTP'], 'Method'):\n",
    "                    # POST/PUT methods often carry attack payloads\n",
    "                    method = packet['HTTP'].Method\n",
    "                    if method in [b'POST', b'PUT', b'PATCH']:\n",
    "                        features['suspicious_score'] += 1\n",
    "                    \n",
    "                    # Check for suspicious headers\n",
    "                    if hasattr(packet['HTTP'], 'Host'):\n",
    "                        host = str(packet['HTTP'].Host)\n",
    "                        # Check for IP addresses in Host header (suspicious)\n",
    "                        if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', host):\n",
    "                            features['suspicious_score'] += 2\n",
    "            \n",
    "            # Check for DNS\n",
    "            if packet.haslayer('DNS'):\n",
    "                features['has_dns'] = 1\n",
    "                # DNS tunneling detection\n",
    "                if hasattr(packet['DNS'], 'qd') and packet['DNS'].qd:\n",
    "                    query_name = str(packet['DNS'].qd.qname)\n",
    "                    # Long DNS names might indicate tunneling\n",
    "                    if len(query_name) > 50:\n",
    "                        features['suspicious_score'] += 2\n",
    "            \n",
    "            # Check for TLS/SSL\n",
    "            if packet.haslayer('TLS'):\n",
    "                features['has_tls'] = 1\n",
    "            \n",
    "            # Extract and analyze payload\n",
    "            if packet.haslayer('Raw'):\n",
    "                payload = str(packet['Raw'].load)\n",
    "                features['content_length'] = len(payload)\n",
    "                \n",
    "                # Limit payload analysis for performance (first 1000 chars)\n",
    "                payload_sample = payload[:1000]\n",
    "                \n",
    "                # ===== PATTERN DETECTION WITH PRE-COMPILED PATTERNS =====\n",
    "                # Check for SQL injection (check top 5 patterns for speed)\n",
    "                for pattern in self.sql_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_sql'] = 1\n",
    "                        features['suspicious_score'] += 3\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for command injection\n",
    "                for pattern in self.cmd_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_cmd'] = 1\n",
    "                        features['suspicious_score'] += 5  # Higher score for OS commands\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for script injection\n",
    "                for pattern in self.script_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_script'] = 1\n",
    "                        features['suspicious_score'] += 2\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for directory traversal\n",
    "                if '../' in payload or '..\\\\' in payload:\n",
    "                    features['has_traversal'] = 1\n",
    "                    features['suspicious_score'] += 2\n",
    "                    features['attack_pattern_count'] += 1\n",
    "                \n",
    "                # ===== NLP ANALYSIS =====\n",
    "                # Perform deeper analysis if enabled\n",
    "                if Config.DEEP_INSPECTION and len(payload) > 10:\n",
    "                    nlp_features = self.nlp_analyzer.quick_analyze(payload_sample)\n",
    "                    features.update(nlp_features)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap_streaming(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Process PCAP file for semantic analysis with streaming.\n",
    "        Analyzes packet payloads for malicious content.\n",
    "        \n",
    "        Uses Parquet for storage instead of HDF5.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory with Parquet files containing semantic features\n",
    "        \"\"\"\n",
    "        print(f\"\\nSemantic analysis (memory-optimized): {pcap_file}\")\n",
    "        print(f\"Output format: Parquet (Windows-optimized)\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        flow_extractor = MemoryOptimizedFlowExtractor()  # Reuse flow ID logic\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting semantic features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow ID to group semantic features\n",
    "                    flow_id = flow_extractor.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract semantic features from payload\n",
    "                    features = self.extract_semantic_features(packet)\n",
    "                    \n",
    "                    # Aggregate features by flow\n",
    "                    if flow_id not in self.semantic_data:\n",
    "                        self.semantic_data[flow_id] = defaultdict(float)\n",
    "                    \n",
    "                    # Sum up features for the flow\n",
    "                    for key, value in features.items():\n",
    "                        self.semantic_data[flow_id][key] += value\n",
    "                    \n",
    "                    # Check memory limit\n",
    "                    if len(self.semantic_data) >= self.max_flows_in_memory:\n",
    "                        self.flush_semantic_features()\n",
    "                    \n",
    "                    # Periodic memory check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory ({mem_percent:.1f}%), flushing...\")\n",
    "                            self.flush_semantic_features()\n",
    "                        gc.collect()\n",
    "            \n",
    "            # Final flush\n",
    "            self.flush_semantic_features()\n",
    "            \n",
    "            print(f\"Processed {packet_count:,} packets\")\n",
    "            print(f\"Semantic features saved to: {self.output_dir}\")\n",
    "            \n",
    "            return self.output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic processing: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class SimplifiedNLPAnalyzer:\n",
    "    \"\"\"\n",
    "    Lightweight NLP analyzer for payload inspection.\n",
    "    Optimized for speed and memory efficiency.\n",
    "    \n",
    "    DETECTION METHODS:\n",
    "    1. Keyword density analysis\n",
    "    2. Entropy calculation (randomness)\n",
    "    3. Encoding detection\n",
    "    4. Character distribution analysis\n",
    "    \n",
    "    OPTIMIZATIONS:\n",
    "    - Pre-compiled patterns\n",
    "    - Numpy-based calculations\n",
    "    - Limited analysis scope\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reduced keyword sets for memory efficiency\n",
    "        self.sql_keywords = {'select', 'union', 'insert', 'drop', 'exec', 'declare', 'cast', 'convert', 'table', 'database'}\n",
    "        self.xss_keywords = {'script', 'javascript', 'alert', 'onerror', 'onclick', 'document', 'cookie', 'eval', 'iframe'}\n",
    "        self.cmd_keywords = {'bash', 'cmd', 'wget', 'curl', 'nc', 'telnet', 'ssh', 'powershell', 'exec', 'system'}\n",
    "        \n",
    "        # Pre-compiled encoding patterns for performance\n",
    "        self.encoding_patterns = {\n",
    "            'base64': re.compile(r'^[A-Za-z0-9+/]+=*$'),\n",
    "            'hex': re.compile(r'^[0-9A-Fa-f]+$'),\n",
    "            'url': re.compile(r'%[0-9A-Fa-f]{2}')\n",
    "        }\n",
    "    \n",
    "    def quick_analyze(self, payload):\n",
    "        \"\"\"\n",
    "        Perform quick NLP analysis on payload.\n",
    "        Focuses on key indicators of malicious content.\n",
    "        \n",
    "        OPTIMIZED: Uses set operations and numpy for speed.\n",
    "        \n",
    "        Returns:\n",
    "            dict: NLP feature scores\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Limit analysis to first 1000 characters for speed\n",
    "        payload_sample = payload[:1000].lower()\n",
    "        \n",
    "        # ===== KEYWORD ANALYSIS (Optimized with sets) =====\n",
    "        # Tokenize efficiently\n",
    "        words = set(re.findall(r'\\b\\w+\\b', payload_sample))\n",
    "        \n",
    "        # Count suspicious keywords using set intersection\n",
    "        sql_score = len(words & self.sql_keywords)\n",
    "        xss_score = len(words & self.xss_keywords)\n",
    "        cmd_score = len(words & self.cmd_keywords)\n",
    "        \n",
    "        # Normalize scores (0-1 range)\n",
    "        features['nlp_pattern_score'] = min((sql_score + xss_score + cmd_score) / 10, 1.0)\n",
    "        \n",
    "        # ===== ENTROPY ANALYSIS (Using numpy) =====\n",
    "        # High entropy suggests encryption/obfuscation\n",
    "        if len(payload) > 0:\n",
    "            # Fast entropy calculation\n",
    "            byte_counts = np.bincount(np.frombuffer(payload[:100].encode('utf-8', errors='ignore'), dtype=np.uint8), minlength=256)\n",
    "            byte_probs = byte_counts[byte_counts > 0] / min(len(payload), 100)\n",
    "            features['nlp_entropy_score'] = -np.sum(byte_probs * np.log2(byte_probs)) / 8  # Normalize to 0-1\n",
    "        else:\n",
    "            features['nlp_entropy_score'] = 0\n",
    "        \n",
    "        # ===== ENCODING DETECTION =====\n",
    "        # Check for common encoding schemes\n",
    "        encoding_detected = 0\n",
    "        sample = payload_sample[:50]\n",
    "        \n",
    "        for pattern_name, pattern in self.encoding_patterns.items():\n",
    "            if pattern.search(sample):\n",
    "                encoding_detected = 1\n",
    "                break\n",
    "        \n",
    "        features['nlp_encoding_detected'] = encoding_detected\n",
    "        \n",
    "        # ===== CHARACTER DISTRIBUTION ANALYSIS =====\n",
    "        # Suspicious if too many special characters\n",
    "        if len(payload_sample) > 0:\n",
    "            special_chars = sum(1 for c in payload_sample if not c.isalnum() and not c.isspace())\n",
    "            special_ratio = special_chars / len(payload_sample)\n",
    "            features['nlp_special_char_ratio'] = min(special_ratio, 1.0)\n",
    "        else:\n",
    "            features['nlp_special_char_ratio'] = 0\n",
    "        \n",
    "        # ===== OVERALL MALICIOUS CONFIDENCE =====\n",
    "        # Weighted combination of all indicators\n",
    "        features['nlp_malicious_confidence'] = min(\n",
    "            features['nlp_pattern_score'] * 0.4 +      # Pattern matching weight\n",
    "            features['nlp_entropy_score'] * 0.2 +      # Entropy weight\n",
    "            features['nlp_encoding_detected'] * 0.2 +  # Encoding weight\n",
    "            features['nlp_special_char_ratio'] * 0.2,  # Special char weight\n",
    "            1.0\n",
    "        )\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067b9ac",
   "metadata": {},
   "source": [
    "# ### Step 5: Combined Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0cc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Combined Feature Pipeline - Merging Flow & Semantic Features\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate feature extraction and merge different feature types\n",
    "This class combines flow statistics with semantic analysis to create a comprehensive\n",
    "feature set for machine learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Coordinates flow and semantic feature extraction\n",
    "2. Merges features from multiple sources using flow_id\n",
    "3. Performs feature engineering (creates new features from existing ones)\n",
    "4. Handles the entire extraction pipeline end-to-end\n",
    "5. Manages disk-based merging for large datasets\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FEATURE FUSION: Combining statistical and content-based features\n",
    "- FEATURE ENGINEERING: Creating derived features that better capture patterns\n",
    "- DISK-BASED MERGE: Joining large datasets without loading into memory\n",
    "\n",
    "FEATURE TYPES COMBINED:\n",
    "1. Flow Features (from Cell 3):\n",
    "   - Timing statistics\n",
    "   - Packet sizes\n",
    "   - TCP flags\n",
    "   - Flow rates\n",
    "   - Forward/Backward statistics (CICIDS)\n",
    "\n",
    "2. Semantic Features (from Cell 4):\n",
    "   - Attack pattern detection\n",
    "   - NLP analysis scores\n",
    "   - Entropy measurements\n",
    "   - Protocol indicators\n",
    "\n",
    "3. Engineered Features (created here):\n",
    "   - Packet rate (packets/duration)\n",
    "   - Average packet size\n",
    "   - Port categories (well-known, registered)\n",
    "   - Flag ratios (flags/total packets)\n",
    "\n",
    "WHY COMBINE FEATURES:\n",
    "- Flow features detect behavioral anomalies\n",
    "- Semantic features detect content anomalies\n",
    "- Combined view provides better attack detection\n",
    "- Some attacks only visible through combination\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Improved merging strategy for Windows\n",
    "- Added CICIDS-specific engineered features\n",
    "- Parallel processing support for feature engineering\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeaturePipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the combined feature pipeline with both extractors.\n",
    "        \"\"\"\n",
    "        # Initialize component extractors\n",
    "        self.flow_extractor = MemoryOptimizedFlowExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY,\n",
    "            flow_timeout=Config.FLOW_TIMEOUT\n",
    "        )\n",
    "        self.semantic_extractor = MemoryOptimizedSemanticExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY\n",
    "        )\n",
    "        \n",
    "        # Output directory for combined features\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'combined_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self, pcap_file, mode='combined'):\n",
    "        \"\"\"\n",
    "        Main orchestration function - manages entire feature extraction process.\n",
    "        \n",
    "        Args:\n",
    "            pcap_file: Path to PCAP file\n",
    "            mode: 'flow' (statistics only), 'semantic' (content only), or 'combined' (both)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory containing all features\n",
    "        \"\"\"\n",
    "        feature_dirs = []  # List to track generated feature directories\n",
    "        \n",
    "        # ===== PHASE 1: FLOW FEATURE EXTRACTION =====\n",
    "        if mode in ['flow', 'combined']:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 1: MEMORY-OPTIMIZED FLOW EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Extracting statistical features from network flows...\")\n",
    "            print(\"This analyzes packet timing, sizes, and patterns\")\n",
    "            print(\"Including CICIDS-specific features (Active/Idle, Fwd/Bwd)\")\n",
    "            \n",
    "            flow_dir = self.flow_extractor.process_pcap(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            feature_dirs.append(('flow', flow_dir))\n",
    "            print(f\"✓ Flow features extracted to: {flow_dir}\")\n",
    "        \n",
    "        # ===== PHASE 2: SEMANTIC FEATURE EXTRACTION =====\n",
    "        if mode in ['semantic', 'combined'] and Config.DEEP_INSPECTION:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 2: MEMORY-OPTIMIZED SEMANTIC EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Analyzing packet payloads for malicious content...\")\n",
    "            print(\"This performs deep packet inspection and NLP analysis\")\n",
    "            \n",
    "            semantic_dir = self.semantic_extractor.process_pcap_streaming(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            feature_dirs.append(('semantic', semantic_dir))\n",
    "            print(f\"✓ Semantic features extracted to: {semantic_dir}\")\n",
    "        \n",
    "        # ===== PHASE 3: FEATURE MERGING =====\n",
    "        if len(feature_dirs) > 1:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 3: MERGING FEATURES (DISK-BASED)\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Combining flow and semantic features...\")\n",
    "            return self.merge_features_on_disk(feature_dirs)\n",
    "        elif feature_dirs:\n",
    "            return feature_dirs[0][1]  # Return single feature directory if only one type\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def merge_features_on_disk(self, feature_dirs):\n",
    "        \"\"\"\n",
    "        Merge features from multiple Parquet directories without loading all into memory.\n",
    "        Uses flow_id as the join key to combine features.\n",
    "        \n",
    "        OPTIMIZED: Uses Parquet's columnar format for efficient merging.\n",
    "        \n",
    "        Process:\n",
    "        1. Read flow features in batches\n",
    "        2. Find matching semantic features for each batch\n",
    "        3. Merge on flow_id\n",
    "        4. Apply feature engineering\n",
    "        5. Save merged batch to new Parquet file\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to merged feature directory\n",
    "        \"\"\"\n",
    "        print(\"Merging features using disk-based operations...\")\n",
    "        print(\"This preserves memory by processing in batches\")\n",
    "        \n",
    "        # Extract directory paths\n",
    "        flow_dir = feature_dirs[0][1]  # Flow features directory\n",
    "        semantic_dir = feature_dirs[1][1] if len(feature_dirs) > 1 else None\n",
    "        \n",
    "        # Get all Parquet files\n",
    "        import glob\n",
    "        flow_files = sorted(glob.glob(os.path.join(flow_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        batch_counter = 0\n",
    "        \n",
    "        for flow_file in tqdm(flow_files, desc=\"Merging batches\"):\n",
    "            # Load batch of flow features\n",
    "            flow_batch = pd.read_parquet(flow_file)\n",
    "            \n",
    "            # Merge with semantic features if available\n",
    "            if semantic_dir:\n",
    "                # Get flow IDs from this batch\n",
    "                flow_ids = set(flow_batch['flow_id'].values)\n",
    "                \n",
    "                # Load matching semantic features\n",
    "                semantic_batch = self.load_matching_semantic_features(\n",
    "                    semantic_dir, flow_ids\n",
    "                )\n",
    "                \n",
    "                # Merge on flow_id (left join to keep all flows)\n",
    "                if semantic_batch is not None and not semantic_batch.empty:\n",
    "                    combined_batch = pd.merge(\n",
    "                        flow_batch, semantic_batch,\n",
    "                        on='flow_id', how='left'\n",
    "                    )\n",
    "                    \n",
    "                    # Fill missing semantic features with zeros\n",
    "                    semantic_cols = semantic_batch.columns.difference(['flow_id'])\n",
    "                    combined_batch[semantic_cols] = combined_batch[semantic_cols].fillna(0)\n",
    "                else:\n",
    "                    combined_batch = flow_batch\n",
    "            else:\n",
    "                combined_batch = flow_batch\n",
    "            \n",
    "            # Apply feature engineering to create derived features\n",
    "            combined_batch = self.engineer_features(combined_batch)\n",
    "            \n",
    "            # Optimize data types\n",
    "            combined_batch = self.optimize_dtypes(combined_batch)\n",
    "            \n",
    "            # Save merged batch\n",
    "            output_file = os.path.join(\n",
    "                self.output_dir,\n",
    "                f'batch_{batch_counter:04d}.parquet'\n",
    "            )\n",
    "            combined_batch.to_parquet(\n",
    "                output_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            batch_counter += 1\n",
    "            \n",
    "            # Clean up memory\n",
    "            del combined_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"✓ Features merged and saved to: {self.output_dir}\")\n",
    "        print(f\"  Total batches processed: {batch_counter}\")\n",
    "        return self.output_dir\n",
    "    \n",
    "    def load_matching_semantic_features(self, semantic_dir, flow_ids):\n",
    "        \"\"\"\n",
    "        Load semantic features that match given flow IDs.\n",
    "        Efficient loading - only reads matching records.\n",
    "        \n",
    "        Args:\n",
    "            semantic_dir: Directory containing semantic feature Parquet files\n",
    "            flow_ids: Set of flow IDs to match\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Semantic features for matching flows\n",
    "        \"\"\"\n",
    "        matching_features = []\n",
    "        \n",
    "        import glob\n",
    "        semantic_files = sorted(glob.glob(os.path.join(semantic_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in semantic_files:\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            # Filter for matching flow IDs\n",
    "            matches = batch[batch['flow_id'].isin(flow_ids)]\n",
    "            \n",
    "            if not matches.empty:\n",
    "                matching_features.append(matches)\n",
    "        \n",
    "        # Combine all matching features\n",
    "        if matching_features:\n",
    "            return pd.concat(matching_features, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Create derived features that better capture attack patterns.\n",
    "        Feature engineering is crucial for ML model performance.\n",
    "        \n",
    "        ENHANCED: Added CICIDS-specific engineered features.\n",
    "        \n",
    "        Engineered features include:\n",
    "        1. Rate features: packets/second, bytes/second\n",
    "        2. Ratio features: flag counts / total packets\n",
    "        3. Port categories: well-known (<1024), registered (1024-49151)\n",
    "        4. Suspicious indicators: binary flags for quick filtering\n",
    "        5. CICIDS ratios: forward/backward packet ratios\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with raw features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With additional engineered features\n",
    "        \"\"\"\n",
    "        # ===== RATE FEATURES =====\n",
    "        # Packet rate (packets per second)\n",
    "        if 'total_packets' in df.columns and 'flow_duration' in df.columns:\n",
    "            df['packet_rate'] = df['total_packets'] / (df['flow_duration'] + 0.001)  # Avoid division by zero\n",
    "            df['packet_rate'] = df['packet_rate'].astype(np.float32)\n",
    "        \n",
    "        # Average packet size\n",
    "        if 'total_bytes' in df.columns and 'total_packets' in df.columns:\n",
    "            df['avg_packet_size'] = df['total_bytes'] / (df['total_packets'] + 1)\n",
    "            df['avg_packet_size'] = df['avg_packet_size'].astype(np.float32)\n",
    "        \n",
    "        # ===== CICIDS-SPECIFIC RATIOS =====\n",
    "        # Forward/Backward packet ratio\n",
    "        if 'fwd_packets' in df.columns and 'bwd_packets' in df.columns:\n",
    "            df['fwd_bwd_packets_ratio'] = df['fwd_packets'] / (df['bwd_packets'] + 1)\n",
    "            df['fwd_bwd_packets_ratio'] = df['fwd_bwd_packets_ratio'].astype(np.float32)\n",
    "        \n",
    "        # Forward/Backward bytes ratio\n",
    "        if 'fwd_bytes' in df.columns and 'bwd_bytes' in df.columns:\n",
    "            df['fwd_bwd_bytes_ratio'] = df['fwd_bytes'] / (df['bwd_bytes'] + 1)\n",
    "            df['fwd_bwd_bytes_ratio'] = df['fwd_bwd_bytes_ratio'].astype(np.float32)\n",
    "        \n",
    "        # ===== FLAG RATIO FEATURES =====\n",
    "        # Calculate flag ratios (important for detecting SYN floods, etc.)\n",
    "        flag_cols = ['syn_count', 'ack_count', 'fin_count', 'rst_count', 'psh_count', 'urg_count']\n",
    "        if all(col in df.columns for col in flag_cols) and 'total_packets' in df.columns:\n",
    "            for flag in flag_cols:\n",
    "                ratio_name = f'{flag}_ratio'\n",
    "                df[ratio_name] = df[flag] / (df['total_packets'] + 1)\n",
    "                df[ratio_name] = df[ratio_name].astype(np.float32)\n",
    "        \n",
    "        # ===== PORT CATEGORY FEATURES =====\n",
    "        # Categorize ports for better pattern recognition\n",
    "        if 'dst_port' in df.columns:\n",
    "            # Well-known ports (0-1023) - usually system services\n",
    "            df['is_well_known_port'] = (df['dst_port'] < 1024).astype(np.int8)\n",
    "            \n",
    "            # Registered ports (1024-49151) - usually applications\n",
    "            df['is_registered_port'] = ((df['dst_port'] >= 1024) & \n",
    "                                        (df['dst_port'] < 49152)).astype(np.int8)\n",
    "            \n",
    "            # Dynamic/private ports (49152-65535) - usually client connections\n",
    "            df['is_dynamic_port'] = (df['dst_port'] >= 49152).astype(np.int8)\n",
    "            \n",
    "            # Common attack target ports\n",
    "            attack_ports = {80, 443, 22, 23, 3389, 445, 139, 21, 25, 3306}\n",
    "            df['is_common_target_port'] = df['dst_port'].isin(attack_ports).astype(np.int8)\n",
    "        \n",
    "        # ===== SUSPICIOUS INDICATORS =====\n",
    "        # Binary flags for quick filtering\n",
    "        if 'suspicious_score' in df.columns:\n",
    "            df['is_suspicious'] = (df['suspicious_score'] > 0).astype(np.int8)\n",
    "            df['highly_suspicious'] = (df['suspicious_score'] > 5).astype(np.int8)\n",
    "        \n",
    "        # ===== ATTACK COMBINATION FEATURES =====\n",
    "        # Some attacks use specific combinations\n",
    "        if 'has_sql' in df.columns and 'has_script' in df.columns:\n",
    "            # SQL + Script often indicates complex web attack\n",
    "            df['sql_and_script'] = ((df.get('has_sql', 0) > 0) & \n",
    "                                    (df.get('has_script', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        if 'has_cmd' in df.columns and 'nlp_encoding_detected' in df.columns:\n",
    "            # Command injection + encoding often indicates obfuscated attack\n",
    "            df['encoded_cmd'] = ((df.get('has_cmd', 0) > 0) & \n",
    "                                 (df.get('nlp_encoding_detected', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        # ===== PAYLOAD RATIO FEATURES =====\n",
    "        if 'total_payload_bytes' in df.columns and 'total_bytes' in df.columns:\n",
    "            # Ratio of payload to total traffic\n",
    "            df['payload_ratio'] = df['total_payload_bytes'] / (df['total_bytes'] + 1)\n",
    "            df['payload_ratio'] = df['payload_ratio'].astype(np.float32)\n",
    "        \n",
    "        # ===== ACTIVE/IDLE FEATURES (CICIDS) =====\n",
    "        if 'active_mean' in df.columns and 'idle_mean' in df.columns:\n",
    "            # Active to idle ratio\n",
    "            df['active_idle_ratio'] = df['active_mean'] / (df['idle_mean'] + 0.001)\n",
    "            df['active_idle_ratio'] = df['active_idle_ratio'].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def optimize_dtypes(self, df):\n",
    "        \"\"\"\n",
    "        Optimize DataFrame dtypes for memory efficiency.\n",
    "        Reduces memory usage significantly.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            \n",
    "            if col_type != 'object':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                \n",
    "                # Integer optimization\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                \n",
    "                # Float optimization\n",
    "                elif str(col_type)[:5] == 'float':\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539f45e",
   "metadata": {},
   "source": [
    "# ### Step 6: CICIDS2017 Label Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f7e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: CICIDS Label Matcher - Ground Truth Integration\n",
    "\"\"\"\n",
    "PURPOSE: Match extracted flows with CICIDS2017 ground truth labels\n",
    "This class integrates the official attack labels from CICIDS2017 dataset\n",
    "to enable supervised learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads CICIDS2017 CSV label files incrementally\n",
    "2. Creates a mapping between network flows and attack types\n",
    "3. Handles multiple CSV files from different days\n",
    "4. Assigns numeric labels for ML training\n",
    "5. Maintains attack type distribution statistics\n",
    "\n",
    "CICIDS2017 ATTACK TYPES:\n",
    "The dataset contains 15 different attack categories:\n",
    "0. BENIGN - Normal, non-malicious traffic\n",
    "1. Bot - Botnet traffic\n",
    "2. DDoS - Distributed Denial of Service\n",
    "3. DoS GoldenEye - Application layer DoS\n",
    "4. DoS Hulk - Volume-based DoS\n",
    "5. DoS Slowhttptest - Slow HTTP attack\n",
    "6. DoS slowloris - Connection exhaustion\n",
    "7. FTP-Patator - FTP brute force\n",
    "8. Heartbleed - SSL vulnerability exploit\n",
    "9. Infiltration - Network infiltration\n",
    "10. PortScan - Port scanning activity\n",
    "11. SSH-Patator - SSH brute force\n",
    "12. Web Attack - Brute Force\n",
    "13. Web Attack - SQL Injection\n",
    "14. Web Attack - XSS\n",
    "\n",
    "MATCHING STRATEGY:\n",
    "- Primary: Match by 5-tuple (IPs, ports, protocol)\n",
    "- Fallback: Match by destination port (majority vote)\n",
    "- Default: Label as BENIGN if no match found\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Loads CSV files in chunks (100K rows at a time)\n",
    "- Builds port-label cache instead of full flow mapping\n",
    "- Processes labels incrementally without loading all\n",
    "- Uses Parquet for output\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Improved chunked CSV reading\n",
    "- Better handling of CICIDS CSV column variations\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedLabelMatcher:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize label matcher with attack type mappings.\n",
    "        \"\"\"\n",
    "        # Mapping from text labels to numeric codes for ML\n",
    "        self.attack_mapping = {\n",
    "            'BENIGN': 0,\n",
    "            'Bot': 1,\n",
    "            'DDoS': 2,\n",
    "            'DoS GoldenEye': 3,\n",
    "            'DoS Hulk': 4,\n",
    "            'DoS Slowhttptest': 5,\n",
    "            'DoS slowloris': 6,\n",
    "            'FTP-Patator': 7,\n",
    "            'Heartbleed': 8,\n",
    "            'Infiltration': 9,\n",
    "            'PortScan': 10,\n",
    "            'SSH-Patator': 11,\n",
    "            'Web Attack - Brute Force': 12,\n",
    "            'Web Attack - SQL Injection': 13,\n",
    "            'Web Attack - XSS': 14,\n",
    "            'Web Attack – Brute Force': 12,  # Alternative spelling\n",
    "            'Web Attack – XSS': 14,  # Alternative spelling\n",
    "            'Web Attack – Sql Injection': 13  # Alternative spelling\n",
    "        }\n",
    "        \n",
    "        # Cache for port-to-label mapping (memory efficient)\n",
    "        self.port_label_cache = {}\n",
    "        \n",
    "        # Cache for IP+port combinations (more accurate)\n",
    "        self.ip_port_label_cache = {}\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.label_statistics = Counter()\n",
    "    \n",
    "    def load_labels_incrementally(self, label_files):\n",
    "        \"\"\"\n",
    "        Load CICIDS label files incrementally to avoid memory overflow.\n",
    "        Builds port-based and IP+port mappings for efficient matching.\n",
    "        \n",
    "        Args:\n",
    "            label_files: List of CSV file paths\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING CICIDS LABELS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        label_counts = Counter()\n",
    "        total_rows_processed = 0\n",
    "        \n",
    "        # Process each label file\n",
    "        for file_idx, label_file in enumerate(label_files):\n",
    "            print(f\"\\nProcessing label file {file_idx + 1}/{len(label_files)}: {os.path.basename(label_file)}\")\n",
    "            \n",
    "            # Read CSV in chunks to manage memory\n",
    "            chunk_size = 100000\n",
    "            chunks_processed = 0\n",
    "            \n",
    "            try:\n",
    "                # Process file in chunks\n",
    "                for chunk in pd.read_csv(label_file, encoding='latin-1', chunksize=chunk_size):\n",
    "                    # Clean column names (remove spaces)\n",
    "                    chunk.columns = chunk.columns.str.strip()\n",
    "                    \n",
    "                    # Find label column (handles different naming conventions)\n",
    "                    label_col = None\n",
    "                    for col in ['Label', 'label', 'LABEL', ' Label']:\n",
    "                        if col in chunk.columns:\n",
    "                            label_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if not label_col:\n",
    "                        print(f\"  Warning: No label column found in chunk {chunks_processed}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Find required columns for matching\n",
    "                    dst_port_col = None\n",
    "                    dst_ip_col = None\n",
    "                    src_ip_col = None\n",
    "                    \n",
    "                    # Port column variations\n",
    "                    for col in ['Destination Port', 'Dst Port', 'dst_port', ' Destination Port', 'dst Port']:\n",
    "                        if col in chunk.columns:\n",
    "                            dst_port_col = col\n",
    "                            break\n",
    "                    \n",
    "                    # IP column variations\n",
    "                    for col in ['Destination IP', 'Dst IP', 'dst_ip', ' Destination IP']:\n",
    "                        if col in chunk.columns:\n",
    "                            dst_ip_col = col\n",
    "                            break\n",
    "                    \n",
    "                    for col in ['Source IP', 'Src IP', 'src_ip', ' Source IP']:\n",
    "                        if col in chunk.columns:\n",
    "                            src_ip_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if label_col and dst_port_col:\n",
    "                        # Build port-label mapping\n",
    "                        for port, group in chunk.groupby(dst_port_col):\n",
    "                            # Get most frequent label for this port\n",
    "                            most_common_label = group[label_col].mode()\n",
    "                            if len(most_common_label) > 0:\n",
    "                                label = most_common_label.iloc[0]\n",
    "                                \n",
    "                                # Update port cache with majority vote\n",
    "                                if port not in self.port_label_cache:\n",
    "                                    self.port_label_cache[port] = Counter()\n",
    "                                self.port_label_cache[port][label] += len(group)\n",
    "                                \n",
    "                                # Update statistics\n",
    "                                label_counts[label] += len(group)\n",
    "                        \n",
    "                        # Build IP+port mapping for better accuracy\n",
    "                        if dst_ip_col and src_ip_col:\n",
    "                            for _, row in chunk.iterrows():\n",
    "                                key = f\"{row[src_ip_col]}:{row[dst_ip_col]}:{row[dst_port_col]}\"\n",
    "                                label = row[label_col]\n",
    "                                \n",
    "                                if key not in self.ip_port_label_cache:\n",
    "                                    self.ip_port_label_cache[key] = Counter()\n",
    "                                self.ip_port_label_cache[key][label] += 1\n",
    "                    \n",
    "                    chunks_processed += 1\n",
    "                    total_rows_processed += len(chunk)\n",
    "                    \n",
    "                    # Periodic memory cleanup\n",
    "                    if chunks_processed % 10 == 0:\n",
    "                        gc.collect()\n",
    "                        print(f\"  Processed {chunks_processed * chunk_size:,} rows...\")\n",
    "                \n",
    "                print(f\"  Completed: {chunks_processed} chunks\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing file: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Finalize port-label mapping (keep only most common label per port)\n",
    "        for port in self.port_label_cache:\n",
    "            if isinstance(self.port_label_cache[port], Counter):\n",
    "                # Get most common label for this port\n",
    "                most_common = self.port_label_cache[port].most_common(1)[0][0]\n",
    "                self.port_label_cache[port] = most_common\n",
    "        \n",
    "        # Finalize IP+port mapping\n",
    "        for key in self.ip_port_label_cache:\n",
    "            if isinstance(self.ip_port_label_cache[key], Counter):\n",
    "                most_common = self.ip_port_label_cache[key].most_common(1)[0][0]\n",
    "                self.ip_port_label_cache[key] = most_common\n",
    "        \n",
    "        # Display label distribution\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABEL DISTRIBUTION:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(label_counts.values())\n",
    "        for label, count in label_counts.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTotal rows processed: {total_rows_processed:,}\")\n",
    "        print(f\"Port-label mappings created: {len(self.port_label_cache):,}\")\n",
    "        print(f\"IP+Port mappings created: {len(self.ip_port_label_cache):,}\")\n",
    "        \n",
    "        # Store statistics\n",
    "        self.label_statistics = label_counts\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def apply_labels_to_batch(self, df):\n",
    "        \"\"\"\n",
    "        Apply labels to a batch of extracted features.\n",
    "        Uses IP+port mapping first, then port-based mapping as fallback.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with extracted features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With added label columns\n",
    "        \"\"\"\n",
    "        # Initialize with BENIGN\n",
    "        df['label'] = 0\n",
    "        df['attack_type'] = 'BENIGN'\n",
    "        df['label_confidence'] = 0.0\n",
    "        \n",
    "        # Try IP+port matching first (more accurate)\n",
    "        if 'src_ip' in df.columns and 'dst_ip' in df.columns and 'dst_port' in df.columns:\n",
    "            for idx, row in df.iterrows():\n",
    "                key = f\"{row.get('src_ip', '')}:{row.get('dst_ip', '')}:{row.get('dst_port', 0)}\"\n",
    "                if key in self.ip_port_label_cache:\n",
    "                    attack_type = self.ip_port_label_cache[key]\n",
    "                    df.at[idx, 'attack_type'] = attack_type\n",
    "                    df.at[idx, 'label'] = self.attack_mapping.get(attack_type, 0)\n",
    "                    df.at[idx, 'label_confidence'] = 0.9  # High confidence\n",
    "        \n",
    "        # Fallback to port-only matching for unlabeled flows\n",
    "        if 'dst_port' in df.columns:\n",
    "            unlabeled = df[df['label'] == 0]\n",
    "            if not unlabeled.empty:\n",
    "                # Map ports to attack types using cache\n",
    "                port_labels = unlabeled['dst_port'].map(self.port_label_cache).fillna('BENIGN')\n",
    "                df.loc[unlabeled.index, 'attack_type'] = port_labels\n",
    "                \n",
    "                # Convert text labels to numeric\n",
    "                df.loc[unlabeled.index, 'label'] = port_labels.map(self.attack_mapping).fillna(0)\n",
    "                \n",
    "                # Lower confidence for port-only matching\n",
    "                df.loc[unlabeled.index, 'label_confidence'] = 0.5\n",
    "        \n",
    "        # Ensure correct dtypes\n",
    "        df['label'] = df['label'].astype(np.int8)\n",
    "        df['label_confidence'] = df['label_confidence'].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_features_with_labels(self, features_dir, label_files):\n",
    "        \"\"\"\n",
    "        Process feature files and add labels in batches.\n",
    "        Creates new Parquet files with labeled features.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            label_files: List of CICIDS CSV files\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to labeled feature directory\n",
    "        \"\"\"\n",
    "        if not label_files:\n",
    "            print(\"No label files provided, skipping labeling\")\n",
    "            return features_dir\n",
    "        \n",
    "        # Load label mappings into cache\n",
    "        print(\"\\nBuilding label cache from CSV files...\")\n",
    "        self.load_labels_incrementally(label_files)\n",
    "        \n",
    "        # Create output directory for labeled features\n",
    "        labeled_dir = os.path.join(Config.TEMP_DIR, 'labeled_features')\n",
    "        os.makedirs(labeled_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"APPLYING LABELS TO FEATURES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        labeled_count = Counter()\n",
    "        \n",
    "        # Process each feature batch\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for idx, feature_file in enumerate(tqdm(feature_files, desc=\"Labeling batches\")):\n",
    "            # Load feature batch\n",
    "            batch = pd.read_parquet(feature_file)\n",
    "            \n",
    "            # Apply labels\n",
    "            batch = self.apply_labels_to_batch(batch)\n",
    "            \n",
    "            # Track label distribution\n",
    "            labeled_count.update(batch['attack_type'].value_counts().to_dict())\n",
    "            \n",
    "            # Save labeled batch\n",
    "            output_file = os.path.join(labeled_dir, f'batch_{idx:04d}.parquet')\n",
    "            batch.to_parquet(\n",
    "                output_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Display labeling results\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABELING RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(labeled_count.values())\n",
    "        for attack_type, count in labeled_count.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {attack_type:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nLabeled features saved to: {labeled_dir}\")\n",
    "        \n",
    "        # Validate labeling quality\n",
    "        benign_pct = labeled_count.get('BENIGN', 0) / total * 100 if total > 0 else 0\n",
    "        if benign_pct > 90:\n",
    "            print(\"\\n Warning: >90% flows labeled as BENIGN\")\n",
    "            print(\"   This might indicate labeling issues or imbalanced dataset\")\n",
    "            print(\"   Consider using stratified sampling for ML training\")\n",
    "        \n",
    "        return labeled_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3307df",
   "metadata": {},
   "source": [
    "# ### Step 7: Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5bc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Analysis - Selection & Importance Ranking\n",
    "\"\"\"\n",
    "PURPOSE: Analyze and select the most important features for machine learning\n",
    "This class determines which features are most useful for detecting attacks.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Collects statistical information about all features\n",
    "2. Calculates feature importance using mutual information\n",
    "3. Selects the top N most informative features\n",
    "4. Removes redundant or uninformative features\n",
    "5. Provides feature ranking for interpretability\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- MUTUAL INFORMATION: Measures how much knowing a feature reduces uncertainty about the label\n",
    "- FEATURE SELECTION: Choosing subset of features that maximize predictive power\n",
    "- CURSE OF DIMENSIONALITY: Too many features can hurt ML performance\n",
    "- FEATURE IMPORTANCE: Understanding which features drive predictions\n",
    "\n",
    "WHY FEATURE SELECTION MATTERS:\n",
    "- Reduces training time (fewer features to process)\n",
    "- Improves model performance (removes noise)\n",
    "- Prevents overfitting (simpler models generalize better)\n",
    "- Enhances interpretability (understand what drives detection)\n",
    "\n",
    "SELECTION STRATEGY:\n",
    "1. Statistical Analysis: Mean, variance, range for each feature\n",
    "2. Importance Scoring: Mutual information with target labels\n",
    "3. Redundancy Removal: Correlation analysis\n",
    "4. Top-K Selection: Keep only the best features\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet for data loading\n",
    "- Parallel processing for importance calculation\n",
    "- Better handling of CICIDS-specific features\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeatureAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize feature analyzer with storage for statistics and importance scores.\n",
    "        \"\"\"\n",
    "        self.feature_importance = {}  # Feature name -> importance score\n",
    "        self.selected_features = []   # Final list of selected features\n",
    "        self.feature_stats = {}        # Statistical summaries per feature\n",
    "    \n",
    "    def analyze_features_incrementally(self, features_dir, target_col='label'):\n",
    "        \"\"\"\n",
    "        Analyze features using two-pass incremental processing.\n",
    "        Pass 1: Collect statistics\n",
    "        Pass 2: Calculate importance on representative sample\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            target_col: Name of label column\n",
    "            \n",
    "        Returns:\n",
    "            list: Selected feature names\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE ANALYSIS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ===== PASS 1: STATISTICS COLLECTION =====\n",
    "        print(\"Pass 1: Collecting feature statistics...\")\n",
    "        print(\"  This helps understand feature distributions\")\n",
    "        self.collect_feature_stats(features_dir, target_col)\n",
    "        \n",
    "        # ===== PASS 2: IMPORTANCE CALCULATION =====\n",
    "        print(\"\\nPass 2: Calculating feature importance on sample...\")\n",
    "        print(\"  This identifies most informative features\")\n",
    "        self.calculate_importance_sample(features_dir, target_col)\n",
    "        \n",
    "        # ===== FEATURE SELECTION =====\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(\n",
    "            self.feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Select top features\n",
    "        self.selected_features = [f[0] for f in sorted_features[:Config.TOP_FEATURES]]\n",
    "        \n",
    "        # ===== DISPLAY RESULTS =====\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\"TOP {min(15, len(sorted_features))} FEATURES:\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"{'Rank':<5} {'Feature':<35} {'Importance':<10} {'Type'}\")\n",
    "        print(\"-\"*65)\n",
    "        \n",
    "        for i, (feature, score) in enumerate(sorted_features[:15], 1):\n",
    "            # Determine feature type for interpretation\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            print(f\"{i:<5} {feature:<35} {score:>10.4f} {feature_type}\")\n",
    "        \n",
    "        print(f\"\\nTotal features analyzed: {len(self.feature_importance)}\")\n",
    "        print(f\"Features selected: {len(self.selected_features)}\")\n",
    "        \n",
    "        # ===== CICIDS-SPECIFIC FEATURE INSIGHTS =====\n",
    "        cicids_features = [f for f in self.selected_features if any(x in f.lower() for x in ['fwd', 'bwd', 'active', 'idle', 'iat'])]\n",
    "        if cicids_features:\n",
    "            print(f\"\\nCICIDS-specific features selected: {len(cicids_features)}\")\n",
    "            print(f\"  Examples: {cicids_features[:3]}\")\n",
    "        \n",
    "        return self.selected_features\n",
    "    \n",
    "    def collect_feature_stats(self, features_dir, target_col):\n",
    "        \"\"\"\n",
    "        First pass: Collect statistical summaries for each feature.\n",
    "        This helps understand data distribution and identify issues.\n",
    "        \n",
    "        Statistics collected:\n",
    "        - Sum, sum of squares (for mean/variance calculation)\n",
    "        - Min, max (for range)\n",
    "        - Count (for missing value detection)\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Get feature columns from first batch\n",
    "        first_batch = pd.read_parquet(feature_files[0], columns=None, engine='pyarrow')\n",
    "        \n",
    "        # Identify feature columns (exclude metadata)\n",
    "        exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "        feature_cols = [col for col in first_batch.columns \n",
    "                      if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"  Analyzing {len(feature_cols)} features...\")\n",
    "        \n",
    "        # Initialize statistics collectors\n",
    "        for col in feature_cols:\n",
    "            self.feature_stats[col] = {\n",
    "                'sum': 0,      # For mean calculation\n",
    "                'sum_sq': 0,   # For variance calculation\n",
    "                'count': 0,    # Total non-null values\n",
    "                'min': float('inf'),\n",
    "                'max': float('-inf'),\n",
    "                'zeros': 0,    # Count of zero values\n",
    "                'unique': set() # Track unique values (sampled)\n",
    "            }\n",
    "        \n",
    "        # Process all batches\n",
    "        for file in tqdm(feature_files, desc=\"Collecting stats\"):\n",
    "            batch = pd.read_parquet(file, columns=feature_cols, engine='pyarrow')\n",
    "            \n",
    "            for col in feature_cols:\n",
    "                if col in batch.columns:\n",
    "                    # Get non-null values\n",
    "                    values = batch[col].fillna(0)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    stats = self.feature_stats[col]\n",
    "                    stats['sum'] += values.sum()\n",
    "                    stats['sum_sq'] += (values ** 2).sum()\n",
    "                    stats['count'] += len(values)\n",
    "                    stats['min'] = min(stats['min'], values.min())\n",
    "                    stats['max'] = max(stats['max'], values.max())\n",
    "                    stats['zeros'] += (values == 0).sum()\n",
    "                    \n",
    "                    # Sample unique values (limit to 100 for memory)\n",
    "                    if len(stats['unique']) < 100:\n",
    "                        sample_size = min(10, len(values))\n",
    "                        stats['unique'].update(values.sample(sample_size).tolist())\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate derived statistics\n",
    "        for col, stats in self.feature_stats.items():\n",
    "            if stats['count'] > 0:\n",
    "                stats['mean'] = stats['sum'] / stats['count']\n",
    "                stats['variance'] = (stats['sum_sq'] / stats['count']) - (stats['mean'] ** 2)\n",
    "                stats['std'] = np.sqrt(max(0, stats['variance']))  # Avoid negative variance due to rounding\n",
    "                stats['range'] = stats['max'] - stats['min']\n",
    "                stats['zero_ratio'] = stats['zeros'] / stats['count']\n",
    "            else:\n",
    "                stats['mean'] = stats['variance'] = stats['std'] = stats['range'] = 0\n",
    "                stats['zero_ratio'] = 1\n",
    "    \n",
    "    def calculate_importance_sample(self, features_dir, target_col, sample_size=100000):\n",
    "        \"\"\"\n",
    "        Second pass: Calculate feature importance using mutual information.\n",
    "        Uses a representative sample for efficiency.\n",
    "        \n",
    "        OPTIMIZED: Can use parallel processing if available.\n",
    "        \n",
    "        Mutual Information measures:\n",
    "        - How much information a feature provides about the target\n",
    "        - Non-linear relationships (unlike correlation)\n",
    "        - Works with any feature type\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory with feature files\n",
    "            target_col: Target label column\n",
    "            sample_size: Number of samples for importance calculation\n",
    "        \"\"\"\n",
    "        # Load stratified sample for importance calculation\n",
    "        print(f\"  Loading sample of {sample_size:,} flows...\")\n",
    "        sample_dfs = []\n",
    "        remaining_samples = sample_size\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Sample from different parts of the dataset\n",
    "        sample_interval = max(1, len(feature_files) // 10)  # Sample from 10 points\n",
    "        \n",
    "        for i in range(0, len(feature_files), sample_interval):\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "            \n",
    "            file = feature_files[min(i, len(feature_files)-1)]\n",
    "            batch_sample_size = min(remaining_samples, 10000)\n",
    "            \n",
    "            # Load batch sample\n",
    "            batch = pd.read_parquet(file)\n",
    "            if len(batch) > batch_sample_size:\n",
    "                batch = batch.sample(batch_sample_size, random_state=Config.RANDOM_STATE)\n",
    "            sample_dfs.append(batch)\n",
    "            remaining_samples -= len(batch)\n",
    "        \n",
    "        # Combine samples\n",
    "        sample_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "        print(f\"  Sample loaded: {len(sample_df):,} flows\")\n",
    "        \n",
    "        # Get feature columns\n",
    "        exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "        feature_cols = [col for col in sample_df.columns \n",
    "                       if col not in exclude_cols]\n",
    "        \n",
    "        # Prepare feature matrix and labels\n",
    "        X = sample_df[feature_cols].fillna(0)\n",
    "        y = sample_df[target_col]\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        print(\"  Calculating mutual information scores...\")\n",
    "        print(f\"  Using {Config.N_JOBS if Config.N_JOBS > 0 else 'all'} CPU cores\")\n",
    "        \n",
    "        mi_scores = mutual_info_classif(\n",
    "            X, y, \n",
    "            random_state=Config.RANDOM_STATE,\n",
    "            n_neighbors=3  # Faster calculation\n",
    "        )\n",
    "        \n",
    "        # Store importance scores\n",
    "        self.feature_importance = dict(zip(feature_cols, mi_scores))\n",
    "        \n",
    "        # Identify uninformative features (near-zero importance)\n",
    "        uninformative = [f for f, score in self.feature_importance.items() if score < 0.001]\n",
    "        if uninformative:\n",
    "            print(f\"\\n  ⚠️  Found {len(uninformative)} uninformative features (MI < 0.001)\")\n",
    "            print(f\"     Examples: {uninformative[:5]}\")\n",
    "        \n",
    "        # Identify highly important CICIDS features\n",
    "        cicids_important = [(f, s) for f, s in self.feature_importance.items() \n",
    "                           if s > 0.1 and any(x in f.lower() for x in ['fwd', 'bwd', 'iat', 'active', 'idle'])]\n",
    "        if cicids_important:\n",
    "            print(f\"\\n  ✓ Important CICIDS features found: {len(cicids_important)}\")\n",
    "            for feat, score in cicids_important[:3]:\n",
    "                print(f\"     {feat}: {score:.3f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del sample_df, sample_dfs, X, y\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_feature_type(self, feature_name):\n",
    "        \"\"\"\n",
    "        Categorize feature by its name for better interpretation.\n",
    "        Enhanced to recognize CICIDS-specific features.\n",
    "        \n",
    "        Categories:\n",
    "        - Flow: Timing and size statistics\n",
    "        - Forward: Forward direction features (CICIDS)\n",
    "        - Backward: Backward direction features (CICIDS)\n",
    "        - Active/Idle: Activity patterns (CICIDS)\n",
    "        - Flag: TCP flag related\n",
    "        - Port: Port number features\n",
    "        - Payload: Content-based features\n",
    "        - NLP: Natural language processing scores\n",
    "        - Engineered: Derived features\n",
    "        \"\"\"\n",
    "        feature_lower = feature_name.lower()\n",
    "        \n",
    "        # CICIDS-specific categories\n",
    "        if 'fwd' in feature_lower or 'forward' in feature_lower:\n",
    "            return \"Forward\"\n",
    "        elif 'bwd' in feature_lower or 'backward' in feature_lower:\n",
    "            return \"Backward\"\n",
    "        elif 'active' in feature_lower or 'idle' in feature_lower:\n",
    "            return \"Active/Idle\"\n",
    "        elif 'iat' in feature_lower:\n",
    "            return \"IAT\"\n",
    "        elif 'flow' in feature_lower or 'duration' in feature_lower:\n",
    "            return \"Flow\"\n",
    "        elif 'flag' in feature_lower or 'syn' in feature_lower or 'ack' in feature_lower:\n",
    "            return \"Flag\"\n",
    "        elif 'port' in feature_lower:\n",
    "            return \"Port\"\n",
    "        elif 'payload' in feature_lower or 'entropy' in feature_lower:\n",
    "            return \"Payload\"\n",
    "        elif 'nlp' in feature_lower:\n",
    "            return \"NLP\"\n",
    "        elif 'ratio' in feature_lower or 'rate' in feature_lower:\n",
    "            return \"Engineered\"\n",
    "        elif 'has_' in feature_lower or 'is_' in feature_lower:\n",
    "            return \"Binary\"\n",
    "        elif 'suspicious' in feature_lower:\n",
    "            return \"Detection\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    def get_feature_insights(self):\n",
    "        \"\"\"\n",
    "        Provide insights about selected features for interpretability.\n",
    "        Helps understand what the model will focus on.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Insights about feature selection\n",
    "        \"\"\"\n",
    "        insights = {\n",
    "            'total_features': len(self.feature_importance),\n",
    "            'selected_features': len(self.selected_features),\n",
    "            'feature_types': Counter(),\n",
    "            'top_5_features': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Analyze feature types\n",
    "        for feature in self.selected_features:\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            insights['feature_types'][feature_type] += 1\n",
    "        \n",
    "        # Get top 5 features with scores\n",
    "        for feature in self.selected_features[:5]:\n",
    "            insights['top_5_features'].append({\n",
    "                'name': feature,\n",
    "                'importance': self.feature_importance[feature],\n",
    "                'type': self.get_feature_type(feature)\n",
    "            })\n",
    "        \n",
    "        # Provide recommendations based on feature distribution\n",
    "        if insights['feature_types'].get('Forward', 0) + insights['feature_types'].get('Backward', 0) > 10:\n",
    "            insights['recommendations'].append(\n",
    "                \"Strong directional features - good for detecting asymmetric attacks\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('NLP', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High NLP feature count - model focuses on payload content\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('Active/Idle', 0) > 3:\n",
    "            insights['recommendations'].append(\n",
    "                \"Active/Idle features selected - good for detecting slow attacks\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('Flow', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High flow feature count - model focuses on traffic patterns\"\n",
    "            )\n",
    "        \n",
    "        if len(self.selected_features) < 30:\n",
    "            insights['recommendations'].append(\n",
    "                \"Consider increasing TOP_FEATURES if accuracy is low\"\n",
    "            )\n",
    "        \n",
    "        return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e192c",
   "metadata": {},
   "source": [
    "# ### Step 8: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19848809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Machine Learning Pipeline - Incremental Training & Evaluation\n",
    "\"\"\"\n",
    "PURPOSE: Train and evaluate machine learning models using incremental learning\n",
    "This class implements memory-efficient ML training for large datasets that don't fit in RAM.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Implements incremental/online learning algorithms\n",
    "2. Trains models in batches without loading all data\n",
    "3. Performs train/test split at the batch level\n",
    "4. Evaluates model performance progressively\n",
    "5. Supports multiple ML algorithms optimized for streaming\n",
    "\n",
    "INCREMENTAL LEARNING:\n",
    "Traditional ML loads all data at once. Incremental learning:\n",
    "- Processes data in small batches\n",
    "- Updates model parameters gradually\n",
    "- Never needs full dataset in memory\n",
    "- Perfect for datasets larger than RAM\n",
    "\n",
    "ALGORITHMS USED:\n",
    "1. SGD Classifier (Stochastic Gradient Descent):\n",
    "   - Linear model with online learning\n",
    "   - Fast and memory efficient\n",
    "   - Good for high-dimensional data\n",
    "   - Updates with each batch\n",
    "\n",
    "2. LightGBM:\n",
    "   - Often faster than XGBoost on CPU\n",
    "   - Better memory efficiency\n",
    "   - High accuracy gradient boosting\n",
    "\n",
    "3. XGBoost with External Memory:\n",
    "   - Gradient boosting with disk cache\n",
    "   - High accuracy for complex patterns\n",
    "   - Can process data larger than RAM\n",
    "   \n",
    "4. MiniBatch K-Means (optional):\n",
    "   - Clustering for anomaly detection\n",
    "   - Unsupervised learning option\n",
    "\n",
    "WHY THESE MODELS:\n",
    "- All support incremental/batch learning\n",
    "- Memory efficient for large datasets\n",
    "- Proven effectiveness on network traffic\n",
    "- Balance between speed and accuracy\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Added LightGBM as primary boosting method\n",
    "- Using Parquet for data loading\n",
    "- Better memory management during training\n",
    "- Optimized for AMD Ryzen CPU (parallel processing)\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedMLPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize ML pipeline with model storage and result tracking.\n",
    "        \"\"\"\n",
    "        self.models = {}      # Trained model objects\n",
    "        self.results = {}     # Performance metrics\n",
    "        self.scaler = None    # Feature scaler (fitted on first batch)\n",
    "        self.train_history = defaultdict(list)  # Track training progress\n",
    "    \n",
    "    def train_models_incrementally(self, features_dir, selected_features):\n",
    "        \"\"\"\n",
    "        Main training function using incremental learning.\n",
    "        Processes data in batches, updating models progressively.\n",
    "        \n",
    "        OPTIMIZED: Uses Parquet loading and parallel processing.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            selected_features: List of feature names to use\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (models dict, results dict)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INCREMENTAL MACHINE LEARNING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training models: {Config.SELECTED_MODELS}\")\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "        print(f\"CPU cores available: {Config.N_JOBS if Config.N_JOBS > 0 else 'All'}\")\n",
    "        \n",
    "        # ===== INITIALIZE MODELS =====\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Get class information for CICIDS\n",
    "        n_classes = 15  # CICIDS has 15 attack types\n",
    "        classes = np.arange(n_classes)\n",
    "        \n",
    "        # Training metrics\n",
    "        batch_count = 0\n",
    "        train_scores = defaultdict(list)\n",
    "        \n",
    "        # Get all feature files\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        n_batches = len(feature_files)\n",
    "        \n",
    "        # ===== TRAIN/TEST SPLIT AT BATCH LEVEL =====\n",
    "        # Split batches into train and test sets\n",
    "        # Ensure at least 1 batch for training if we have any data\n",
    "        if n_batches == 1:\n",
    "            # With only 1 batch, use it for both training and testing\n",
    "            print(f\"\\nOnly 1 batch available - using for both training and testing\")\n",
    "            train_files = feature_files\n",
    "            test_files = feature_files\n",
    "        elif n_batches == 2:\n",
    "            # With 2 batches, use 1 for train, 1 for test\n",
    "            train_files = feature_files[:1]\n",
    "            test_files = feature_files[1:]\n",
    "        else:\n",
    "            # Normal split for 3+ batches\n",
    "            train_size = max(1, int(n_batches * (1 - Config.TEST_SIZE)))\n",
    "            train_files = feature_files[:train_size]\n",
    "            test_files = feature_files[train_size:]\n",
    "        \n",
    "        print(f\"\\nData split:\")\n",
    "        print(f\"  Training batches: {len(train_files)}\")\n",
    "        print(f\"  Testing batches: {len(test_files)}\")\n",
    "        \n",
    "        # ===== TRAINING PHASE =====\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"TRAINING PHASE\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for file in tqdm(train_files, desc=\"Training incrementally\"):\n",
    "            # Load batch\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            # Prepare features and labels\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is None or len(X_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            # ===== FEATURE SCALING =====\n",
    "            # Fit scaler on first batch, transform all others\n",
    "            if self.scaler is None:\n",
    "                print(f\"  Fitting scaler on first batch ({len(X_batch)} samples)\")\n",
    "                self.scaler = StandardScaler()\n",
    "                X_batch = self.scaler.fit_transform(X_batch)\n",
    "            else:\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "            \n",
    "            # ===== TRAIN EACH MODEL =====\n",
    "            # SGD Classifier (incremental)\n",
    "            if 'sgd' in self.models:\n",
    "                self.models['sgd'].partial_fit(X_batch, y_batch, classes=classes)\n",
    "                \n",
    "                # Track training progress\n",
    "                if batch_count % 5 == 0:\n",
    "                    score = self.models['sgd'].score(X_batch, y_batch)\n",
    "                    train_scores['sgd'].append(score)\n",
    "                    if batch_count % 20 == 0:\n",
    "                        print(f\"    SGD Batch {batch_count:3d} accuracy: {score:.4f}\")\n",
    "            \n",
    "            # MiniBatch K-Means (if selected)\n",
    "            if 'minibatch_kmeans' in self.models:\n",
    "                self.models['minibatch_kmeans'].partial_fit(X_batch)\n",
    "            \n",
    "            batch_count += 1\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch, X_batch, y_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"\\nTraining complete: {batch_count} batches processed\")\n",
    "        \n",
    "        # ===== TRAIN LIGHTGBM (Non-incremental but memory-efficient) =====\n",
    "        if 'lightgbm' in Config.SELECTED_MODELS:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"LIGHTGBM TRAINING\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_lightgbm(train_files, selected_features, classes)\n",
    "        \n",
    "        # ===== TRAIN XGBOOST IF SELECTED =====\n",
    "        if 'xgboost_incremental' in Config.SELECTED_MODELS:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"XGBOOST TRAINING (External Memory)\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_xgboost_external_memory(train_files, selected_features)\n",
    "        \n",
    "        # ===== TESTING PHASE =====\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"TESTING PHASE\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"Evaluating on held-out test batches...\")\n",
    "        \n",
    "        # Check if scaler was fitted\n",
    "        if self.scaler is None:\n",
    "            print(\"⚠️  Warning: Scaler not fitted during training, fitting now...\")\n",
    "            # Fit scaler on first available batch\n",
    "            for file in feature_files[:1]:\n",
    "                batch = pd.read_parquet(file)\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                if X_batch is not None and len(X_batch) > 0:\n",
    "                    self.scaler = StandardScaler()\n",
    "                    self.scaler.fit(X_batch)\n",
    "                    print(f\"  Scaler fitted on {len(X_batch)} samples\")\n",
    "                    break\n",
    "                del batch\n",
    "                gc.collect()\n",
    "        \n",
    "        test_predictions = defaultdict(list)\n",
    "        test_labels = []\n",
    "        test_batches = 0\n",
    "        \n",
    "        for file in tqdm(test_files, desc=\"Testing\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is None or len(X_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Scale features\n",
    "            X_batch = self.scaler.transform(X_batch)\n",
    "            test_labels.extend(y_batch)\n",
    "            \n",
    "            # Get predictions from each model\n",
    "            if 'sgd' in self.models:\n",
    "                test_predictions['sgd'].extend(\n",
    "                    self.models['sgd'].predict(X_batch)\n",
    "                )\n",
    "            \n",
    "            if 'lightgbm' in self.models:\n",
    "                test_predictions['lightgbm'].extend(\n",
    "                    self.models['lightgbm'].predict(X_batch)\n",
    "                )\n",
    "            \n",
    "            if 'minibatch_kmeans' in self.models:\n",
    "                # For clustering, use cluster assignment as \"prediction\"\n",
    "                test_predictions['minibatch_kmeans'].extend(\n",
    "                    self.models['minibatch_kmeans'].predict(X_batch)\n",
    "                )\n",
    "            \n",
    "            test_batches += 1\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch, X_batch, y_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"Testing complete: {test_batches} batches processed\")\n",
    "        \n",
    "        # ===== CALCULATE METRICS =====\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"MODEL PERFORMANCE\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        test_labels = np.array(test_labels)\n",
    "        \n",
    "        for model_name, predictions in test_predictions.items():\n",
    "            predictions = np.array(predictions)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = (predictions == test_labels).mean()\n",
    "            \n",
    "            # Store results\n",
    "            self.results[model_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'train_scores': train_scores.get(model_name, []),\n",
    "                'test_samples': len(test_labels),\n",
    "                'unique_predictions': len(np.unique(predictions))\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Test Samples: {len(test_labels):,}\")\n",
    "            print(f\"  Unique Predictions: {len(np.unique(predictions))}\")\n",
    "            \n",
    "            # Per-class accuracy for best model\n",
    "            if model_name in ['sgd', 'lightgbm'] and len(np.unique(test_labels)) > 1:\n",
    "                self.calculate_per_class_metrics(predictions, test_labels)\n",
    "        \n",
    "        return self.models, self.results\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"\n",
    "        Initialize selected ML models with appropriate parameters.\n",
    "        All models chosen for their incremental learning capability or memory efficiency.\n",
    "        \"\"\"\n",
    "        # SGD Classifier - Linear model with online learning\n",
    "        if 'sgd' in Config.SELECTED_MODELS:\n",
    "            print(\"\\nInitializing SGD Classifier...\")\n",
    "            self.models['sgd'] = SGDClassifier(\n",
    "                loss='log_loss',      # Logistic regression\n",
    "                penalty='l2',         # L2 regularization\n",
    "                alpha=0.001,          # Regularization strength\n",
    "                max_iter=1000,        # Max iterations per batch\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                n_jobs=Config.N_JOBS, # Use available CPU cores\n",
    "                warm_start=True       # Preserve previous training\n",
    "            )\n",
    "        \n",
    "        # MiniBatch K-Means - For anomaly detection\n",
    "        if 'minibatch_kmeans' in Config.SELECTED_MODELS:\n",
    "            print(\"Initializing MiniBatch K-Means...\")\n",
    "            self.models['minibatch_kmeans'] = MiniBatchKMeans(\n",
    "                n_clusters=15,        # Number of clusters (match attack types)\n",
    "                batch_size=1000,      # Samples per batch\n",
    "                random_state=Config.RANDOM_STATE\n",
    "            )\n",
    "        \n",
    "        # LightGBM and XGBoost will be initialized in their specific methods\n",
    "        if 'lightgbm' in Config.SELECTED_MODELS:\n",
    "            self.models['lightgbm'] = None\n",
    "        if 'xgboost_incremental' in Config.SELECTED_MODELS:\n",
    "            self.models['xgboost_incremental'] = None\n",
    "    \n",
    "    def prepare_batch(self, batch, selected_features):\n",
    "        \"\"\"\n",
    "        Prepare a batch for training by extracting features and labels.\n",
    "        Handles missing features and data type conversions.\n",
    "        \n",
    "        Args:\n",
    "            batch: DataFrame batch from Parquet\n",
    "            selected_features: List of feature names to use\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X feature matrix, y labels)\n",
    "        \"\"\"\n",
    "        if 'label' not in batch.columns:\n",
    "            return None, None\n",
    "        \n",
    "        # Get available features (some might be missing in certain batches)\n",
    "        available_features = [f for f in selected_features if f in batch.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            return None, None\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = batch[available_features].fillna(0).values\n",
    "        y = batch['label'].values\n",
    "        \n",
    "        # Ensure correct data types\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_lightgbm(self, train_files, selected_features, classes):\n",
    "        \"\"\"\n",
    "        Train LightGBM using memory-efficient approach.\n",
    "        LightGBM is often faster than XGBoost on CPU systems.\n",
    "        \"\"\"\n",
    "        print(\"Training LightGBM with memory-efficient settings...\")\n",
    "        \n",
    "        # Load training data in chunks and combine\n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        # Sample from training files for LightGBM\n",
    "        sample_files = train_files[::max(1, len(train_files) // 20)]  # Use up to 20 files\n",
    "        \n",
    "        for file in tqdm(sample_files, desc=\"Loading LightGBM training data\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is not None:\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                X_train_list.append(X_batch)\n",
    "                y_train_list.append(y_batch)\n",
    "            \n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Combine samples\n",
    "        X_train = np.vstack(X_train_list)\n",
    "        y_train = np.hstack(y_train_list)\n",
    "        \n",
    "        print(f\"  Training on {len(X_train):,} samples\")\n",
    "        \n",
    "        # LightGBM parameters optimized for CPU and memory\n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(classes),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'device': 'cpu',\n",
    "            'num_threads': Config.N_JOBS if Config.N_JOBS > 0 else -1,\n",
    "            'verbose': -1,\n",
    "            'max_bin': 255,  # Memory optimization\n",
    "            'min_data_in_leaf': 20,\n",
    "            'seed': Config.RANDOM_STATE\n",
    "        }\n",
    "        \n",
    "        # Create dataset\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        \n",
    "        # Train model\n",
    "        self.models['lightgbm'] = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=100,\n",
    "            valid_sets=[train_data],\n",
    "            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(20)]\n",
    "        )\n",
    "        \n",
    "        print(\"  LightGBM training complete\")\n",
    "        \n",
    "        # Clean up\n",
    "        del X_train, y_train, X_train_list, y_train_list\n",
    "        gc.collect()\n",
    "    \n",
    "    def train_xgboost_external_memory(self, train_files, selected_features):\n",
    "        \"\"\"\n",
    "        Train XGBoost using external memory for very large datasets.\n",
    "        XGBoost can process data from disk without loading it all.\n",
    "        \n",
    "        This is more complex but enables training on massive datasets.\n",
    "        \"\"\"\n",
    "        print(\"Training XGBoost with external memory support...\")\n",
    "        \n",
    "        try:\n",
    "            # Create cache file for XGBoost\n",
    "            cache_file = os.path.join(Config.TEMP_DIR, 'xgb_cache')\n",
    "            \n",
    "            # Convert Parquet to format XGBoost can read\n",
    "            print(\"  Converting data to XGBoost format...\")\n",
    "            libsvm_file = self.create_libsvm_file(train_files, selected_features)\n",
    "            \n",
    "            # Create DMatrix with cache (enables external memory)\n",
    "            print(\"  Creating DMatrix with disk cache...\")\n",
    "            dtrain = xgb.DMatrix(f'{libsvm_file}#dtrain.cache')\n",
    "            \n",
    "            # XGBoost parameters optimized for large data\n",
    "            params = {\n",
    "                'max_depth': 6,           # Tree depth\n",
    "                'eta': 0.1,               # Learning rate\n",
    "                'objective': 'multi:softmax',  # Multiclass classification\n",
    "                'num_class': 15,          # Number of attack types\n",
    "                'tree_method': 'approx',  # Approximate algorithm for large data\n",
    "                'sketch_eps': 0.03,       # Approximation factor\n",
    "                'max_bin': 256,           # Max bins for histogram\n",
    "                'subsample': 0.5,         # Sample 50% of data per tree\n",
    "                'colsample_bytree': 0.8,  # Sample 80% of features per tree\n",
    "                'nthread': Config.N_JOBS if Config.N_JOBS > 0 else -1,\n",
    "                'seed': Config.RANDOM_STATE\n",
    "            }\n",
    "            \n",
    "            # Train model\n",
    "            print(\"  Training XGBoost model...\")\n",
    "            num_rounds = 50  # Number of boosting rounds\n",
    "            self.models['xgboost_incremental'] = xgb.train(\n",
    "                params, dtrain, num_rounds,\n",
    "                verbose_eval=10  # Print progress every 10 rounds\n",
    "            )\n",
    "            \n",
    "            # Store basic results\n",
    "            self.results['xgboost_incremental'] = {\n",
    "                'accuracy': 0.0,  # Will be calculated during testing\n",
    "                'num_trees': num_rounds\n",
    "            }\n",
    "            \n",
    "            print(\"  XGBoost training complete\")\n",
    "            \n",
    "            # Cleanup temporary files\n",
    "            if os.path.exists(libsvm_file):\n",
    "                os.remove(libsvm_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  XGBoost training failed: {e}\")\n",
    "            print(\"  Falling back to other models\")\n",
    "    \n",
    "    def create_libsvm_file(self, train_files, selected_features, max_rows=500000):\n",
    "        \"\"\"\n",
    "        Convert Parquet features to LibSVM format for XGBoost.\n",
    "        LibSVM is a sparse format efficient for ML libraries.\n",
    "        \n",
    "        Format: label feat1:val1 feat2:val2 ...\n",
    "        \n",
    "        Args:\n",
    "            train_files: List of training Parquet files\n",
    "            selected_features: Features to include\n",
    "            max_rows: Limit rows for training\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to LibSVM file\n",
    "        \"\"\"\n",
    "        libsvm_file = os.path.join(Config.TEMP_DIR, 'train.libsvm')\n",
    "        \n",
    "        with open(libsvm_file, 'w') as f:\n",
    "            rows_written = 0\n",
    "            \n",
    "            for file in train_files:\n",
    "                if rows_written >= max_rows:\n",
    "                    break\n",
    "                \n",
    "                batch = pd.read_parquet(file)\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                \n",
    "                if X_batch is None:\n",
    "                    continue\n",
    "                \n",
    "                # Scale features\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                \n",
    "                # Write in LibSVM format\n",
    "                for i in range(len(X_batch)):\n",
    "                    label = int(y_batch[i])\n",
    "                    # Only write non-zero features (sparse format)\n",
    "                    features = ' '.join([f'{j+1}:{v}' for j, v in enumerate(X_batch[i]) if v != 0])\n",
    "                    f.write(f'{label} {features}\\n')\n",
    "                \n",
    "                rows_written += len(X_batch)\n",
    "                del batch, X_batch, y_batch\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"    Created LibSVM file with {rows_written:,} samples\")\n",
    "        return libsvm_file\n",
    "    \n",
    "    def calculate_per_class_metrics(self, predictions, true_labels):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics for each attack type.\n",
    "        Helps identify which attacks are well-detected vs problematic.\n",
    "        \"\"\"\n",
    "        print(\"\\n  Per-Class Performance:\")\n",
    "        print(\"  \" + \"-\"*40)\n",
    "        \n",
    "        # Attack type names for CICIDS\n",
    "        attack_names = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS GoldenEye',\n",
    "            4: 'DoS Hulk',\n",
    "            5: 'DoS Slowhttptest',\n",
    "            6: 'DoS slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web Attack - Brute Force',\n",
    "            13: 'Web Attack - SQL Injection',\n",
    "            14: 'Web Attack - XSS'\n",
    "        }\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        for class_id in np.unique(true_labels):\n",
    "            mask = true_labels == class_id\n",
    "            if mask.sum() > 0:\n",
    "                class_acc = (predictions[mask] == true_labels[mask]).mean()\n",
    "                class_name = attack_names.get(class_id, f'Class {class_id}')\n",
    "                print(f\"    {class_name:30s}: {class_acc:.4f} ({mask.sum()} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05e1fc",
   "metadata": {},
   "source": [
    "# ### Step 9: extended Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4002d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization Suite - Data Analysis Dashboards\n",
    "\"\"\"\n",
    "PURPOSE: Create interactive visualizations to understand data and results\n",
    "This class generates comprehensive visualizations for analysis insights.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates interactive charts using Plotly\n",
    "2. Visualizes attack distributions and patterns\n",
    "3. Shows model performance comparisons\n",
    "4. Generates network communication graphs\n",
    "5. Builds an integrated HTML dashboard\n",
    "\n",
    "VISUALIZATIONS CREATED:\n",
    "1. Protocol Distribution - Pie chart of TCP/UDP/ICMP traffic\n",
    "2. Attack Distribution - Bar chart of attack type frequencies  \n",
    "3. Flow Timeline - Scatter plot of traffic over time\n",
    "4. Port Heatmap - Communication patterns between ports\n",
    "5. Feature Correlation - Correlation matrix of top features\n",
    "6. Model Comparison - Bar chart of model accuracies\n",
    "7. CICIDS Features - Special visualizations for forward/backward features\n",
    "8. Dashboard - Combined HTML view of all visualizations\n",
    "\n",
    "WHY VISUALIZATION MATTERS:\n",
    "- Helps understand data patterns before modeling\n",
    "- Identifies class imbalance issues\n",
    "- Reveals temporal attack patterns\n",
    "- Shows which features are correlated\n",
    "- Makes results interpretable for stakeholders\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Uses data sampling (max 10K points per viz)\n",
    "- Loads only required columns\n",
    "- Generates static HTML files\n",
    "- Cleans up data after each visualization\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet for data loading\n",
    "- Added CICIDS-specific visualizations\n",
    "- Optimized for large datasets\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedVisualizer:\n",
    "    def __init__(self, features_dir, ml_results, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize visualizer with data source and output location.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            ml_results: Dictionary of ML model results\n",
    "            output_dir: Where to save visualization files\n",
    "        \"\"\"\n",
    "        self.features_dir = features_dir\n",
    "        self.ml_results = ml_results\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = min(Config.SAMPLE_SIZE, 10000)  # Max points for visualization\n",
    "    \n",
    "    def create_all_visualizations(self):\n",
    "        \"\"\"\n",
    "        Generate all visualizations and create dashboard.\n",
    "        Each visualization is saved as a separate HTML file.\n",
    "        \"\"\"\n",
    "        if not Config.GENERATE_VISUALS:\n",
    "            print(\"Visualization disabled in config\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERATING VISUALIZATIONS (SAMPLED DATA)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Creating visualizations with {self.sample_size:,} sample points\")\n",
    "        print(\"This provides insights without loading full dataset\")\n",
    "        \n",
    "        # Load sample data for visualization\n",
    "        sample_df = self.load_visualization_sample()\n",
    "        \n",
    "        if sample_df is None or sample_df.empty:\n",
    "            print(\"No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # List of visualizations to create\n",
    "        viz_functions = [\n",
    "            (\"Protocol Distribution\", lambda: self.create_protocol_distribution(sample_df)),\n",
    "            (\"Attack Distribution\", lambda: self.create_attack_distribution(sample_df)),\n",
    "            (\"Flow Timeline\", lambda: self.create_flow_timeline(sample_df)),\n",
    "            (\"Port Heatmap\", lambda: self.create_port_heatmap(sample_df)),\n",
    "            (\"Feature Correlation\", lambda: self.create_feature_correlation(sample_df)),\n",
    "            (\"CICIDS Features\", lambda: self.create_cicids_feature_analysis(sample_df)),\n",
    "            (\"Model Performance\", lambda: self.create_model_comparison())\n",
    "        ]\n",
    "        \n",
    "        # Create each visualization\n",
    "        for name, func in tqdm(viz_functions, desc=\"Creating visualizations\"):\n",
    "            try:\n",
    "                func()\n",
    "                print(f\"  Created: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed {name}: {e}\")\n",
    "        \n",
    "        # Create integrated dashboard\n",
    "        self.create_dashboard(sample_df)\n",
    "        \n",
    "        # Clean up sample data\n",
    "        del sample_df\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\nVisualizations saved to: {self.output_dir}\")\n",
    "        print(f\"  Open dashboard.html in browser to view all charts\")\n",
    "    \n",
    "    def load_visualization_sample(self):\n",
    "        \"\"\"\n",
    "        Load a representative sample of data for visualization.\n",
    "        Samples from different parts of dataset for better representation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: Sample of features for visualization\n",
    "        \"\"\"\n",
    "        print(f\"\\nLoading sample of {self.sample_size:,} flows for visualization...\")\n",
    "        \n",
    "        sample_dfs = []\n",
    "        remaining = self.sample_size\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(self.features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Sample from different parts of dataset for diversity\n",
    "        sample_interval = max(1, len(feature_files) // 10)  # Sample from 10 points\n",
    "        \n",
    "        for i in range(0, len(feature_files), sample_interval):\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            \n",
    "            file = feature_files[min(i, len(feature_files)-1)]\n",
    "            batch_sample_size = min(remaining, 1000)\n",
    "            \n",
    "            # Load sample from this batch\n",
    "            batch = pd.read_parquet(file)\n",
    "            if len(batch) > batch_sample_size:\n",
    "                batch = batch.sample(batch_sample_size, random_state=Config.RANDOM_STATE)\n",
    "            sample_dfs.append(batch)\n",
    "            remaining -= len(batch)\n",
    "        \n",
    "        if sample_dfs:\n",
    "            sample = pd.concat(sample_dfs, ignore_index=True)\n",
    "            print(f\"  Loaded {len(sample):,} flows from {len(sample_dfs)} batches\")\n",
    "            return sample\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def create_protocol_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create pie chart showing distribution of network protocols.\n",
    "        Helps understand traffic composition (TCP vs UDP vs other).\n",
    "        \"\"\"\n",
    "        if 'protocol' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Map protocol numbers to names\n",
    "        protocol_map = {\n",
    "            6: 'TCP',\n",
    "            17: 'UDP', \n",
    "            1: 'ICMP',\n",
    "            41: 'IPv6',\n",
    "            47: 'GRE'\n",
    "        }\n",
    "        \n",
    "        df['protocol_name'] = df['protocol'].map(protocol_map).fillna('Other')\n",
    "        protocol_counts = df['protocol_name'].value_counts()\n",
    "        \n",
    "        # Create pie chart\n",
    "        fig = go.Figure(data=[go.Pie(\n",
    "            labels=protocol_counts.index,\n",
    "            values=protocol_counts.values,\n",
    "            hole=0.3,  # Donut chart\n",
    "            marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96A6A6', '#FFA07A']),\n",
    "            textposition='auto',\n",
    "            textinfo='label+percent'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Protocol Distribution (Sample)\",\n",
    "            height=400,\n",
    "            width=600,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'protocol_distribution.html'))\n",
    "    \n",
    "    def create_attack_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create bar chart showing distribution of attack types.\n",
    "        Critical for understanding class balance in dataset.\n",
    "        \"\"\"\n",
    "        if 'attack_type' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_counts = df['attack_type'].value_counts().head(15)\n",
    "        \n",
    "        # Create bar chart with color coding\n",
    "        colors = ['green' if x == 'BENIGN' else 'red' for x in attack_counts.index]\n",
    "        \n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=attack_counts.index,\n",
    "            y=attack_counts.values,\n",
    "            text=attack_counts.values,\n",
    "            textposition='auto',\n",
    "            marker_color=colors,\n",
    "            hovertemplate='%{x}<br>Count: %{y}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Attack Type Distribution (Sample)\",\n",
    "            xaxis_title=\"Attack Type\",\n",
    "            yaxis_title=\"Number of Flows\",\n",
    "            xaxis_tickangle=-45,\n",
    "            height=500,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'attack_distribution.html'))\n",
    "    \n",
    "    def create_flow_timeline(self, df):\n",
    "        \"\"\"\n",
    "        Create scatter plot showing traffic patterns over time.\n",
    "        Reveals temporal patterns in attacks and normal traffic.\n",
    "        \"\"\"\n",
    "        if 'flow_duration' not in df.columns or 'total_bytes' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Further sample if still too large for smooth visualization\n",
    "        if len(df) > 1000:\n",
    "            df = df.sample(1000)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Color by attack type if available\n",
    "        if 'attack_type' in df.columns:\n",
    "            # Create trace for each attack type\n",
    "            for attack_type in df['attack_type'].unique():\n",
    "                mask = df['attack_type'] == attack_type\n",
    "                \n",
    "                # Determine color\n",
    "                color = 'green' if attack_type == 'BENIGN' else 'red'\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df[mask].index,\n",
    "                    y=df[mask]['total_bytes'],\n",
    "                    mode='markers',\n",
    "                    name=str(attack_type),\n",
    "                    marker=dict(\n",
    "                        size=np.log1p(df[mask]['total_packets']) * 2,  # Size by packet count\n",
    "                        color=color,\n",
    "                        opacity=0.6,\n",
    "                        line=dict(width=0)\n",
    "                    ),\n",
    "                    hovertemplate='Flow %{x}<br>Bytes: %{y}<br>Type: ' + str(attack_type) + '<extra></extra>'\n",
    "                ))\n",
    "        else:\n",
    "            # Single trace if no attack labels\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df['total_bytes'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='blue', opacity=0.6)\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Flow Timeline (Sample)\",\n",
    "            xaxis_title=\"Flow Index\",\n",
    "            yaxis_title=\"Total Bytes\",\n",
    "            yaxis_type=\"log\",  # Log scale for better visibility\n",
    "            height=500,\n",
    "            width=1200,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'flow_timeline.html'))\n",
    "    \n",
    "    def create_port_heatmap(self, df):\n",
    "        \"\"\"\n",
    "        Create heatmap showing communication patterns between ports.\n",
    "        Helps identify service interactions and potential scanning.\n",
    "        \"\"\"\n",
    "        if 'src_port' not in df.columns or 'dst_port' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Get top ports by frequency\n",
    "        top_src = df['src_port'].value_counts().head(15).index\n",
    "        top_dst = df['dst_port'].value_counts().head(15).index\n",
    "        \n",
    "        # Filter for top ports\n",
    "        df_filtered = df[df['src_port'].isin(top_src) & df['dst_port'].isin(top_dst)]\n",
    "        \n",
    "        # Create communication matrix\n",
    "        matrix = pd.crosstab(df_filtered['src_port'], df_filtered['dst_port'])\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=matrix.values,\n",
    "            x=[str(int(p)) for p in matrix.columns],\n",
    "            y=[str(int(p)) for p in matrix.index],\n",
    "            colorscale='Viridis',\n",
    "            hoverongaps=False,\n",
    "            hovertemplate='Src Port: %{y}<br>Dst Port: %{x}<br>Count: %{z}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Port Communication Heatmap (Top Ports)\",\n",
    "            xaxis_title=\"Destination Port\",\n",
    "            yaxis_title=\"Source Port\",\n",
    "            height=600,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'port_heatmap.html'))\n",
    "    \n",
    "    def create_feature_correlation(self, df):\n",
    "        \"\"\"\n",
    "        Create correlation matrix of top features.\n",
    "        Identifies redundant features and relationships.\n",
    "        \"\"\"\n",
    "        # Select numeric columns (excluding metadata)\n",
    "        exclude_cols = {'flow_id', 'label', 'attack_type', 'label_confidence'}\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols][:15]\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            return\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,  # Center colorscale at 0\n",
    "            colorbar=dict(title=\"Correlation\"),\n",
    "            hovertemplate='%{x}<br>%{y}<br>Correlation: %{z:.2f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Feature Correlation Matrix (Top Features)\",\n",
    "            height=700,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'feature_correlation.html'))\n",
    "    \n",
    "    def create_cicids_feature_analysis(self, df):\n",
    "        \"\"\"\n",
    "        Create CICIDS-specific visualizations for forward/backward features.\n",
    "        Shows directional traffic patterns important for CICIDS.\n",
    "        \"\"\"\n",
    "        # Check for CICIDS features\n",
    "        fwd_cols = [col for col in df.columns if 'fwd' in col.lower()]\n",
    "        bwd_cols = [col for col in df.columns if 'bwd' in col.lower()]\n",
    "        \n",
    "        if not fwd_cols or not bwd_cols:\n",
    "            return\n",
    "        \n",
    "        # Create subplot figure\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Forward vs Backward Packets', 'Forward vs Backward Bytes',\n",
    "                          'Active vs Idle Time', 'Flag Distribution'),\n",
    "            specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "                   [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "        )\n",
    "        \n",
    "        # Forward vs Backward packets\n",
    "        if 'fwd_packets' in df.columns and 'bwd_packets' in df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['fwd_packets'], y=df['bwd_packets'],\n",
    "                          mode='markers', marker=dict(size=3),\n",
    "                          name='Fwd vs Bwd Packets'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Forward vs Backward bytes\n",
    "        if 'fwd_bytes' in df.columns and 'bwd_bytes' in df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['fwd_bytes'], y=df['bwd_bytes'],\n",
    "                          mode='markers', marker=dict(size=3),\n",
    "                          name='Fwd vs Bwd Bytes'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Active vs Idle statistics\n",
    "        if 'active_mean' in df.columns and 'idle_mean' in df.columns:\n",
    "            active_idle_data = pd.DataFrame({\n",
    "                'Active': df['active_mean'].mean(),\n",
    "                'Idle': df['idle_mean'].mean()\n",
    "            }, index=[0])\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=['Active', 'Idle'],\n",
    "                      y=[active_idle_data['Active'].values[0], active_idle_data['Idle'].values[0]],\n",
    "                      name='Active/Idle'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Flag distribution\n",
    "        flag_cols = ['syn_count', 'ack_count', 'fin_count', 'rst_count', 'psh_count']\n",
    "        flag_means = []\n",
    "        flag_names = []\n",
    "        for col in flag_cols:\n",
    "            if col in df.columns:\n",
    "                flag_means.append(df[col].mean())\n",
    "                flag_names.append(col.replace('_count', '').upper())\n",
    "        \n",
    "        if flag_means:\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=flag_names, y=flag_means, name='Flags'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=800, width=1200, title=\"CICIDS Feature Analysis\")\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'cicids_features.html'))\n",
    "    \n",
    "    def create_model_comparison(self):\n",
    "        \"\"\"\n",
    "        Create bar chart comparing ML model performance.\n",
    "        Shows which algorithms work best for this data.\n",
    "        \"\"\"\n",
    "        if not self.ml_results:\n",
    "            return\n",
    "        \n",
    "        # Extract model names and accuracies\n",
    "        models = list(self.ml_results.keys())\n",
    "        accuracies = [self.ml_results[m]['accuracy'] for m in models]\n",
    "        \n",
    "        # Create bar chart\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=models,\n",
    "            y=accuracies,\n",
    "            text=[f\"{acc:.4f}\" for acc in accuracies],\n",
    "            textposition='auto',\n",
    "            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96A6A6'][:len(models)],\n",
    "            hovertemplate='%{x}<br>Accuracy: %{y:.4f}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Model Performance Comparison\",\n",
    "            xaxis_title=\"Model\",\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            yaxis_range=[0, 1],\n",
    "            height=400,\n",
    "            width=600\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'model_comparison.html'))\n",
    "    \n",
    "    def create_dashboard(self, sample_df):\n",
    "        \"\"\"\n",
    "        Create integrated HTML dashboard combining all visualizations.\n",
    "        Provides single-page overview of analysis results.\n",
    "        \"\"\"\n",
    "        # Calculate summary statistics\n",
    "        total_flows = self.get_total_flow_count()\n",
    "        attack_flows = sample_df[sample_df.get('label', 0) > 0].shape[0] if 'label' in sample_df.columns else 0\n",
    "        attack_pct = (attack_flows / len(sample_df) * 100) if len(sample_df) > 0 else 0\n",
    "        best_accuracy = max(r['accuracy'] for r in self.ml_results.values()) if self.ml_results else 0\n",
    "        \n",
    "        # Create dashboard HTML\n",
    "        dashboard_html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Network Analysis Dashboard</title>\n",
    "            <style>\n",
    "                body {{ \n",
    "                    font-family: 'Segoe UI', Arial, sans-serif; \n",
    "                    margin: 20px; \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    min-height: 100vh;\n",
    "                }}\n",
    "                .container {{\n",
    "                    max-width: 1400px;\n",
    "                    margin: 0 auto;\n",
    "                    background: rgba(255,255,255,0.95);\n",
    "                    border-radius: 20px;\n",
    "                    padding: 30px;\n",
    "                    box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "                }}\n",
    "                h1 {{ \n",
    "                    color: #2d3748; \n",
    "                    border-bottom: 3px solid #667eea; \n",
    "                    padding-bottom: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    font-size: 28px;\n",
    "                }}\n",
    "                .stats {{ \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    padding: 25px;\n",
    "                    border-radius: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    color: white;\n",
    "                    box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
    "                }}\n",
    "                .stats h2 {{\n",
    "                    margin-top: 0;\n",
    "                    font-size: 20px;\n",
    "                    opacity: 0.9;\n",
    "                }}\n",
    "                .stat-grid {{\n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "                    gap: 20px;\n",
    "                    margin-top: 15px;\n",
    "                }}\n",
    "                .stat-item {{\n",
    "                    background: rgba(255,255,255,0.1);\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    backdrop-filter: blur(10px);\n",
    "                }}\n",
    "                .stat-value {{\n",
    "                    font-size: 24px;\n",
    "                    font-weight: bold;\n",
    "                    margin-bottom: 5px;\n",
    "                }}\n",
    "                .stat-label {{\n",
    "                    font-size: 12px;\n",
    "                    opacity: 0.8;\n",
    "                    text-transform: uppercase;\n",
    "                }}\n",
    "                .warning {{ \n",
    "                    background: #fed7d7;\n",
    "                    color: #742a2a;\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    margin-bottom: 20px;\n",
    "                    border-left: 4px solid #fc8181;\n",
    "                }}\n",
    "                .grid {{ \n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(2, 1fr);\n",
    "                    gap: 25px;\n",
    "                    margin-top: 20px;\n",
    "                }}\n",
    "                .viz-frame {{ \n",
    "                    background: white;\n",
    "                    border-radius: 12px;\n",
    "                    padding: 15px;\n",
    "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                    transition: transform 0.3s ease;\n",
    "                }}\n",
    "                .viz-frame:hover {{\n",
    "                    transform: translateY(-5px);\n",
    "                    box-shadow: 0 8px 25px rgba(0,0,0,0.15);\n",
    "                }}\n",
    "                iframe {{ \n",
    "                    width: 100%;\n",
    "                    height: 400px;\n",
    "                    border: none;\n",
    "                    border-radius: 8px;\n",
    "                }}\n",
    "                .full-width {{ \n",
    "                    grid-column: span 2;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>CICIDS2017 Network Analysis Dashboard</h1>\n",
    "                \n",
    "                <div class=\"warning\">\n",
    "                    <strong>Note:</strong> Visualizations based on sampled data ({len(sample_df):,} flows).\n",
    "                    Full dataset contains {total_flows:,} flows.\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"stats\">\n",
    "                    <h2>Summary Statistics</h2>\n",
    "                    <div class=\"stat-grid\">\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{total_flows:,}</div>\n",
    "                            <div class=\"stat-label\">Total Flows Processed</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{attack_pct:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Attack Traffic (Sample)</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{best_accuracy:.2%}</div>\n",
    "                            <div class=\"stat-label\">Best Model Accuracy</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{psutil.virtual_memory().percent:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Memory Usage</div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"grid\">\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Protocol Distribution</h3>\n",
    "                        <iframe src=\"protocol_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Model Performance</h3>\n",
    "                        <iframe src=\"model_comparison.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Attack Type Distribution</h3>\n",
    "                        <iframe src=\"attack_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Flow Timeline Analysis</h3>\n",
    "                        <iframe src=\"flow_timeline.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Port Communication Patterns</h3>\n",
    "                        <iframe src=\"port_heatmap.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Feature Correlations</h3>\n",
    "                        <iframe src=\"feature_correlation.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>CICIDS Feature Analysis</h3>\n",
    "                        <iframe src=\"cicids_features.html\"></iframe>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <p style=\"text-align: center; color: #718096; margin-top: 30px; font-size: 14px;\">\n",
    "                    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                    Pipeline: Memory-Optimized Network Analysis V2.0\n",
    "                </p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save dashboard\n",
    "        with open(os.path.join(self.output_dir, 'dashboard.html'), 'w') as f:\n",
    "            f.write(dashboard_html)\n",
    "        \n",
    "        print(\"\\nDashboard created: dashboard.html\")\n",
    "        print(\"  Open in browser to view all visualizations\")\n",
    "    \n",
    "    def get_total_flow_count(self):\n",
    "        \"\"\"\n",
    "        Get total number of flows from Parquet files.\n",
    "        Counts rows without loading data into memory.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total flow count\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        import glob\n",
    "        feature_files = glob.glob(os.path.join(self.features_dir, 'batch_*.parquet'))\n",
    "        \n",
    "        for file in feature_files:\n",
    "            # Get row count from Parquet metadata\n",
    "            df = pd.read_parquet(file, columns=['flow_id'])\n",
    "            total += len(df)\n",
    "            del df\n",
    "            gc.collect()\n",
    "        \n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd9c7b",
   "metadata": {},
   "source": [
    "# ### Step 10: Export and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8010e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Result Exporter - Saving Analysis Outputs\n",
    "\"\"\"\n",
    "PURPOSE: Export all analysis results in various formats for use and sharing\n",
    "This class handles saving features, models, and reports to disk.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Exports extracted features to CSV format (in chunks)\n",
    "2. Saves trained ML models for reuse\n",
    "3. Generates comprehensive text report\n",
    "4. Creates metadata manifest for tracking\n",
    "5. Handles cleanup of temporary files\n",
    "\n",
    "OUTPUT FILES CREATED:\n",
    "- features_chunk_*.csv: Feature data split into manageable files\n",
    "- *.pkl: Serialized ML models\n",
    "- scaler.pkl: Feature scaler for inference\n",
    "- analysis_report.txt: Detailed text report\n",
    "- manifest.json: Metadata about the analysis\n",
    "- dashboard.html: Interactive visualization dashboard\n",
    "\n",
    "WHY CHUNKED EXPORT:\n",
    "- CSV files have size limits\n",
    "- Easier to transfer and share\n",
    "- Can be processed separately\n",
    "- Prevents memory overflow during export\n",
    "\n",
    "REPORT CONTENTS:\n",
    "- Processing statistics\n",
    "- Label distribution  \n",
    "- Feature importance\n",
    "- Model performance\n",
    "- Memory usage\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Reading from Parquet instead of HDF5\n",
    "- Exports feature scaler for inference\n",
    "- Better handling of large exports\n",
    "- Includes CICIDS-specific statistics\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedResultExporter:\n",
    "    def __init__(self, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize exporter with output directory.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory for saving all outputs\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def export_features_chunked(self, features_dir, chunk_size=100000):\n",
    "        \"\"\"\n",
    "        Export features from Parquet to CSV in manageable chunks.\n",
    "        Large datasets are split across multiple CSV files.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            chunk_size: Maximum rows per CSV file\n",
    "            \n",
    "        Creates:\n",
    "            features_chunk_000.csv, features_chunk_001.csv, etc.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING FEATURES IN CHUNKS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Exporting to CSV with max {chunk_size:,} rows per file\")\n",
    "        \n",
    "        chunk_counter = 0\n",
    "        total_rows = 0\n",
    "        file_list = []\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in tqdm(feature_files, desc=\"Exporting chunks\"):\n",
    "            # Load batch from Parquet\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            # Split batch if it's too large\n",
    "            for start_idx in range(0, len(batch), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(batch))\n",
    "                chunk = batch.iloc[start_idx:end_idx]\n",
    "                \n",
    "                # Save to CSV\n",
    "                chunk_file = os.path.join(\n",
    "                    self.output_dir, \n",
    "                    f'features_chunk_{chunk_counter:03d}.csv'\n",
    "                )\n",
    "                \n",
    "                # Save with compression to reduce file size\n",
    "                chunk.to_csv(chunk_file + '.gz', index=False, compression='gzip')\n",
    "                \n",
    "                # Track file info\n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(chunk_file) + '.gz',\n",
    "                    'rows': len(chunk),\n",
    "                    'size_mb': os.path.getsize(chunk_file + '.gz') / (1024*1024)\n",
    "                }\n",
    "                file_list.append(file_info)\n",
    "                \n",
    "                chunk_counter += 1\n",
    "                total_rows += len(chunk)\n",
    "            \n",
    "            # Clean up\n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"\\n✓ Exported {total_rows:,} flows in {chunk_counter} chunks\")\n",
    "        \n",
    "        # Create manifest file with export metadata\n",
    "        manifest_file = os.path.join(self.output_dir, 'manifest.json')\n",
    "        manifest = {\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'total_rows': total_rows,\n",
    "            'total_chunks': chunk_counter,\n",
    "            'chunk_pattern': 'features_chunk_*.csv.gz',\n",
    "            'compression': 'gzip',\n",
    "            'files': file_list,\n",
    "            'source': {\n",
    "                'pcap': Config.PCAP_FILE,\n",
    "                'analysis_mode': Config.ANALYSIS_MODE,\n",
    "                'chunk_size': Config.CHUNK_SIZE,\n",
    "                'storage_format': Config.STORAGE_FORMAT\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(manifest_file, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Manifest saved to: {manifest_file}\")\n",
    "    \n",
    "    def export_models(self, models, scaler=None, selected_features=None):\n",
    "        \"\"\"\n",
    "        Save trained ML models, scaler, and selected features for future use.\n",
    "        Models are serialized using pickle format.\n",
    "        \n",
    "        Args:\n",
    "            models: Dictionary of trained model objects\n",
    "            scaler: Feature scaler object\n",
    "            selected_features: List of selected feature names\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING ML MODELS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Export models\n",
    "        for model_name, model in models.items():\n",
    "            if model is not None:\n",
    "                # Create filename\n",
    "                model_path = os.path.join(self.output_dir, f'{model_name}.pkl')\n",
    "                \n",
    "                # Serialize model\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                \n",
    "                # Get file size\n",
    "                size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "                print(f\"✓ Saved {model_name}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # Export scaler (important for inference)\n",
    "        if scaler is not None:\n",
    "            scaler_path = os.path.join(self.output_dir, 'scaler.pkl')\n",
    "            with open(scaler_path, 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "            print(f\"✓ Saved feature scaler\")\n",
    "        \n",
    "        # Export selected features (needed for inference)\n",
    "        if selected_features is not None:\n",
    "            features_path = os.path.join(self.output_dir, 'selected_features.pkl')\n",
    "            with open(features_path, 'wb') as f:\n",
    "                pickle.dump(selected_features, f)\n",
    "            print(f\"✓ Saved {len(selected_features)} selected feature names\")\n",
    "    \n",
    "    def generate_report(self, features_dir, ml_results, selected_features):\n",
    "        \"\"\"\n",
    "        Generate comprehensive text report of analysis results.\n",
    "        Provides human-readable summary of entire pipeline.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature files\n",
    "            ml_results: ML performance results\n",
    "            selected_features: List of selected features\n",
    "        \"\"\"\n",
    "        report_path = os.path.join(self.output_dir, 'analysis_report.txt')\n",
    "        \n",
    "        # Collect statistics without loading all data\n",
    "        stats = self.collect_statistics(features_dir)\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            # Header\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"NETWORK PACKET ANALYSIS REPORT\\n\")\n",
    "            f.write(\"Memory-Optimized Pipeline V2.0 for CICIDS2017\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            # System Information\n",
    "            f.write(\"SYSTEM INFORMATION\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\\n\")\n",
    "            f.write(f\"CPU: {psutil.cpu_count()} cores\\n\")\n",
    "            f.write(f\"Storage Format: {Config.STORAGE_FORMAT}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Input Information\n",
    "            f.write(\"INPUT CONFIGURATION\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"PCAP File: {os.path.basename(Config.PCAP_FILE)}\\n\")\n",
    "            if os.path.exists(Config.PCAP_FILE):\n",
    "                f.write(f\"File Size: {os.path.getsize(Config.PCAP_FILE) / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Analysis Mode: {Config.ANALYSIS_MODE}\\n\")\n",
    "            f.write(f\"Deep Inspection: {Config.DEEP_INSPECTION}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Processing Statistics\n",
    "            f.write(\"PROCESSING STATISTICS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total Flows: {stats['total_flows']:,}\\n\")\n",
    "            f.write(f\"Processing Strategy: {'Disk-based' if Config.USE_DISK_CACHE else 'Memory-based'}\\n\")\n",
    "            f.write(f\"Max Memory Setting: {Config.MAX_MEMORY_GB:.1f} GB\\n\")\n",
    "            f.write(f\"Chunk Size: {Config.CHUNK_SIZE:,} packets\\n\")\n",
    "            f.write(f\"Batch Size: {Config.BATCH_SIZE:,} flows\\n\")\n",
    "            f.write(f\"Max Flows in Memory: {Config.MAX_FLOWS_IN_MEMORY:,}\\n\")\n",
    "            f.write(f\"Max Packets per Flow: {Config.MAX_PACKETS_PER_FLOW:,}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Label Distribution\n",
    "            if stats.get('label_distribution'):\n",
    "                f.write(\"ATTACK TYPE DISTRIBUTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                total = sum(stats['label_distribution'].values())\n",
    "                for label, count in sorted(stats['label_distribution'].items(), \n",
    "                                          key=lambda x: x[1], reverse=True):\n",
    "                    pct = count / total * 100 if total > 0 else 0\n",
    "                    f.write(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\\n\")\n",
    "                f.write(f\"\\nTotal Labeled Flows: {total:,}\\n\")\n",
    "                \n",
    "                # CICIDS-specific insights\n",
    "                benign_ratio = stats['label_distribution'].get('BENIGN', 0) / total\n",
    "                attack_ratio = 1 - benign_ratio\n",
    "                f.write(f\"Attack Traffic Ratio: {attack_ratio:.2%}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Feature Selection\n",
    "            if selected_features:\n",
    "                f.write(\"FEATURE SELECTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                f.write(f\"Features Analyzed: {stats.get('total_features', 'Unknown')}\\n\")\n",
    "                f.write(f\"Features Selected: {len(selected_features)}\\n\")\n",
    "                \n",
    "                # Categorize CICIDS features\n",
    "                cicids_features = [f for f in selected_features if any(x in f.lower() \n",
    "                                  for x in ['fwd', 'bwd', 'active', 'idle', 'iat'])]\n",
    "                f.write(f\"CICIDS-specific Features: {len(cicids_features)}\\n\")\n",
    "                \n",
    "                f.write(\"\\nTop 10 Selected Features:\\n\")\n",
    "                for i, feature in enumerate(selected_features[:10], 1):\n",
    "                    f.write(f\"  {i:2d}. {feature}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # ML Results\n",
    "            if ml_results:\n",
    "                f.write(\"MACHINE LEARNING RESULTS\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                \n",
    "                # Find best model\n",
    "                best_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "                \n",
    "                for model_name, results in ml_results.items():\n",
    "                    is_best = model_name == best_model[0]\n",
    "                    marker = \" ⭐\" if is_best else \"\"\n",
    "                    \n",
    "                    f.write(f\"\\n{model_name.upper()}{marker}:\\n\")\n",
    "                    f.write(f\"  Test Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "                    \n",
    "                    if 'test_samples' in results:\n",
    "                        f.write(f\"  Test Samples: {results['test_samples']:,}\\n\")\n",
    "                    \n",
    "                    if 'unique_predictions' in results:\n",
    "                        f.write(f\"  Unique Predictions: {results['unique_predictions']}\\n\")\n",
    "                \n",
    "                f.write(f\"\\nBest Model: {best_model[0]} (Accuracy: {best_model[1]['accuracy']:.4f})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Memory Usage\n",
    "            mem = psutil.virtual_memory()\n",
    "            f.write(\"MEMORY USAGE\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Current Memory: {mem.percent:.1f}%\\n\")\n",
    "            f.write(f\"Available: {mem.available / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Used: {mem.used / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Total System: {mem.total / (1024**3):.2f} GB\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"ANALYSIS INSIGHTS & RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            \n",
    "            # Check for class imbalance\n",
    "            if stats.get('label_distribution'):\n",
    "                benign_ratio = stats['label_distribution'].get('BENIGN', 0) / stats['total_flows']\n",
    "                if benign_ratio > 0.9:\n",
    "                    f.write(\"⚠️  High class imbalance detected (>90% BENIGN)\\n\")\n",
    "                    f.write(\"   Consider: Oversampling attacks or using weighted loss\\n\\n\")\n",
    "                elif benign_ratio > 0.8:\n",
    "                    f.write(\"✓ Moderate class imbalance (80-90% BENIGN)\\n\")\n",
    "                    f.write(\"   This is typical for CICIDS2017 dataset\\n\\n\")\n",
    "            \n",
    "            # Check accuracy\n",
    "            if ml_results:\n",
    "                avg_accuracy = np.mean([r['accuracy'] for r in ml_results.values()])\n",
    "                if avg_accuracy < 0.8:\n",
    "                    f.write(\"⚠️  Low average accuracy (<80%)\\n\")\n",
    "                    f.write(\"   Consider: Increasing features, tuning hyperparameters\\n\\n\")\n",
    "                elif avg_accuracy > 0.95:\n",
    "                    f.write(\"✓ Excellent model performance (>95%)\\n\")\n",
    "                    f.write(\"   Ready for deployment consideration\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\"✓ Good model performance (80-95%)\\n\")\n",
    "                    f.write(\"   Consider ensemble methods for improvement\\n\\n\")\n",
    "            \n",
    "            # Storage recommendations\n",
    "            f.write(\"Storage Format: \")\n",
    "            if Config.STORAGE_FORMAT == 'parquet':\n",
    "                f.write(\"✓ Using Parquet (optimal for Windows)\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"Consider using Parquet for better performance\\n\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            f.write(\"END OF REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"\\n✓ Report saved to: {report_path}\")\n",
    "    \n",
    "    def collect_statistics(self, features_dir):\n",
    "        \"\"\"\n",
    "        Collect statistics from Parquet files without loading all data.\n",
    "        Uses metadata and sampling for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature files\n",
    "            \n",
    "        Returns:\n",
    "            dict: Statistics dictionary\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_flows': 0,\n",
    "            'total_features': 0,\n",
    "            'label_distribution': Counter()\n",
    "        }\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in feature_files:\n",
    "            # Get row count\n",
    "            df_sample = pd.read_parquet(file, columns=['flow_id'])\n",
    "            stats['total_flows'] += len(df_sample)\n",
    "            \n",
    "            # Get feature count from first file\n",
    "            if stats['total_features'] == 0:\n",
    "                df_first = pd.read_parquet(file)\n",
    "                stats['total_features'] = len(df_first.columns)\n",
    "            \n",
    "            # Sample for label distribution\n",
    "            if 'attack_type' in pd.read_parquet(file, columns=[]).columns:\n",
    "                df_labels = pd.read_parquet(file, columns=['attack_type'])\n",
    "                stats['label_distribution'].update(\n",
    "                    df_labels['attack_type'].value_counts().to_dict()\n",
    "                )\n",
    "            \n",
    "            del df_sample\n",
    "            gc.collect()\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def export_all(self, features_dir, models, ml_results, selected_features, scaler=None):\n",
    "        \"\"\"\n",
    "        Main export function - coordinates all export operations.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature files\n",
    "            models: Trained ML models\n",
    "            ml_results: Model performance results\n",
    "            selected_features: Selected feature names\n",
    "            scaler: Feature scaler\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPORTING ALL RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Export features if requested\n",
    "        if Config.ML_EXPORT:\n",
    "            self.export_features_chunked(features_dir)\n",
    "        \n",
    "        # Export models, scaler, and selected features\n",
    "        if models:\n",
    "            self.export_models(models, scaler, selected_features)\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_report(features_dir, ml_results, selected_features)\n",
    "        \n",
    "        print(f\"\\n✓ All results exported to: {self.output_dir}\")\n",
    "        print(\"  Files created:\")\n",
    "        print(\"    - features_chunk_*.csv.gz (data)\")\n",
    "        print(\"    - *.pkl (models)\")\n",
    "        print(\"    - scaler.pkl (feature scaler)\")\n",
    "        print(\"    - selected_features.pkl (feature names)\")\n",
    "        print(\"    - analysis_report.txt (summary)\")\n",
    "        print(\"    - manifest.json (metadata)\")\n",
    "        print(\"    - dashboard.html (visualizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a469471",
   "metadata": {},
   "source": [
    "# ### Step 11: Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b494efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "READY TO RUN MEMORY-OPTIMIZED ANALYSIS V2.0\n",
      "======================================================================\n",
      "\n",
      "This pipeline is optimized for:\n",
      "• Large PCAP files (10+ GB)\n",
      "• CICIDS2017 dataset\n",
      "• Windows systems (no HDF5 file locking)\n",
      "• AMD Ryzen CPUs (parallel processing)\n",
      "• 32GB RAM systems\n",
      "\n",
      "----------------------------------------\n",
      "KEY IMPROVEMENTS:\n",
      "----------------------------------------\n",
      "✓ Parquet storage (no file locking)\n",
      "✓ Fixed bidirectional flow identification\n",
      "✓ CICIDS-specific features (Fwd/Bwd, Active/Idle)\n",
      "✓ LightGBM support (faster on CPU)\n",
      "✓ Better memory management\n",
      "✓ Parallel processing support\n",
      "\n",
      "----------------------------------------\n",
      "INSTRUCTIONS:\n",
      "----------------------------------------\n",
      "1. Ensure configuration is complete (Cell 2)\n",
      "2. Verify PCAP file path is correct\n",
      "3. Check available disk space for temp files\n",
      "4. Validate environment: validate_environment()\n",
      "5. Run: results = run_memory_optimized_pipeline()\n",
      "6. Monitor memory with: monitor_memory()\n",
      "\n",
      "----------------------------------------\n",
      "QUICK START:\n",
      "----------------------------------------\n",
      "# Validate environment first:\n",
      "validate_environment()\n",
      "\n",
      "# To run the analysis:\n",
      "# results = run_memory_optimized_pipeline()\n",
      "\n",
      "# To check memory anytime:\n",
      "# monitor_memory()\n",
      "\n",
      "----------------------------------------\n",
      "CURRENT SYSTEM STATUS:\n",
      "----------------------------------------\n",
      "\n",
      "Memory Status :\n",
      "   Used: 17.30 GB (55.1%)\n",
      "   Available: 14.08 GB\n",
      "\n",
      "Disk Space:\n",
      "   Available: 325.90 GB\n",
      "   Used: 65.0%\n",
      "\n",
      "✓ Ready to start analysis!\n",
      "   Run: results = run_memory_optimized_pipeline()\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Main Pipeline - Complete Analysis Orchestration\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate the entire analysis pipeline from start to finish\n",
    "This is the main execution function that coordinates all components.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Validates configuration and checks system resources\n",
    "2. Extracts features from PCAP file\n",
    "3. Matches with CICIDS labels if available\n",
    "4. Analyzes and selects best features\n",
    "5. Trains ML models incrementally\n",
    "6. Generates visualizations\n",
    "7. Exports all results\n",
    "8. Cleans up temporary files\n",
    "\n",
    "PIPELINE FLOW:\n",
    "Config → Extract → Label → Analyze → Train → Visualize → Export → Cleanup\n",
    "\n",
    "ERROR HANDLING:\n",
    "- Saves partial results on failure\n",
    "- Cleans up temporary files\n",
    "- Provides detailed error messages\n",
    "- Suggests fixes for common issues\n",
    "\n",
    "MEMORY MANAGEMENT:\n",
    "- Monitors memory throughout execution\n",
    "- Switches strategies based on usage\n",
    "- Cleans up after each phase\n",
    "- Reports memory statistics\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet throughout pipeline\n",
    "- Better error recovery with checkpoints\n",
    "- Parallel processing where applicable\n",
    "- Optimized for AMD Ryzen CPU\n",
    "\"\"\"\n",
    "\n",
    "def run_memory_optimized_pipeline():\n",
    "    \"\"\"\n",
    "    Main execution function for the complete analysis pipeline.\n",
    "    Orchestrates all components while managing memory efficiently.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results dictionary with paths and metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Pipeline Version: 2.0 (Parquet-based, Windows-optimized)\")\n",
    "    print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"System: {psutil.virtual_memory().total / (1024**3):.1f}GB RAM, {psutil.cpu_count()} CPU cores\")\n",
    "    \n",
    "    # Check if ML should be skipped\n",
    "    if Config.SKIP_ML:\n",
    "        print(\"\\nML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "    \n",
    "    # Check configuration and memory\n",
    "    strategy = Config.check_memory_requirements()\n",
    "    \n",
    "    if strategy is False:\n",
    "        print(\"Error: Invalid PCAP file or configuration\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "    print(f\"Memory Limit: {Config.MAX_MEMORY_GB:.1f} GB\")\n",
    "    print(f\"Storage Format: {Config.STORAGE_FORMAT}\")\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'start_time': datetime.now(),\n",
    "        'strategy': strategy,\n",
    "        'temp_dirs': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ========== STEP 1: FEATURE EXTRACTION ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 1: STREAMING FEATURE EXTRACTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Extracting features from network packets...\")\n",
    "        print(\"Using Parquet for reliable storage (no file locking)\")\n",
    "        \n",
    "        pipeline = MemoryOptimizedFeaturePipeline()\n",
    "        features_dir = pipeline.extract_all_features(\n",
    "            Config.PCAP_FILE,\n",
    "            mode=Config.ANALYSIS_MODE\n",
    "        )\n",
    "        \n",
    "        if not features_dir:\n",
    "            print(\"Error: No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        results['temp_dirs'].append(features_dir)\n",
    "        results['features_dir'] = features_dir\n",
    "        \n",
    "        # Memory checkpoint\n",
    "        monitor_memory(\"After feature extraction\")\n",
    "        \n",
    "        # ========== STEP 2: LABEL MATCHING ==========\n",
    "        if Config.USE_CICIDS_LABELS and Config.LABEL_FILES:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 2: INCREMENTAL LABEL MATCHING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Matching flows with CICIDS ground truth labels...\")\n",
    "            \n",
    "            label_matcher = MemoryOptimizedLabelMatcher()\n",
    "            labeled_dir = label_matcher.process_features_with_labels(\n",
    "                features_dir, Config.LABEL_FILES\n",
    "            )\n",
    "            \n",
    "            features_dir = labeled_dir\n",
    "            results['temp_dirs'].append(labeled_dir)\n",
    "            results['features_dir'] = labeled_dir\n",
    "            \n",
    "            monitor_memory(\"After label matching\")\n",
    "        else:\n",
    "            print(\"\\nStep 2: Skipping label matching (no labels provided)\")\n",
    "        \n",
    "        # ========== STEP 3: FEATURE ANALYSIS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 3: INCREMENTAL FEATURE ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Analyzing feature importance and selecting best features...\")\n",
    "        \n",
    "        analyzer = MemoryOptimizedFeatureAnalyzer()\n",
    "        selected_features = analyzer.analyze_features_incrementally(features_dir)\n",
    "        results['selected_features'] = selected_features\n",
    "        \n",
    "        # Get feature insights\n",
    "        insights = analyzer.get_feature_insights()\n",
    "        results['feature_insights'] = insights\n",
    "        \n",
    "        monitor_memory(\"After feature analysis\")\n",
    "        \n",
    "        # ========== STEP 4: MACHINE LEARNING ==========\n",
    "        if not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: INCREMENTAL MACHINE LEARNING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Training ML models with incremental learning...\")\n",
    "            print(f\"Models to train: {Config.SELECTED_MODELS}\")\n",
    "            \n",
    "            ml_pipeline = MemoryOptimizedMLPipeline()\n",
    "            models, ml_results = ml_pipeline.train_models_incrementally(\n",
    "                features_dir, selected_features\n",
    "            )\n",
    "            \n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "            results['scaler'] = ml_pipeline.scaler\n",
    "            \n",
    "            monitor_memory(\"After ML training\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: SKIPPING ML TRAINING (Test Mode)\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"ML training skipped for pipeline validation\")\n",
    "            print(\"Feature extraction and labeling completed successfully\")\n",
    "            models = {}\n",
    "            ml_results = {}\n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "            results['scaler'] = None\n",
    "        \n",
    "        # ========== STEP 5: VISUALIZATION ==========\n",
    "        if Config.GENERATE_VISUALS and not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 5: SAMPLED VISUALIZATIONS\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Creating interactive visualizations...\")\n",
    "            \n",
    "            visualizer = MemoryOptimizedVisualizer(\n",
    "                features_dir, ml_results, Config.OUTPUT_DIR\n",
    "            )\n",
    "            visualizer.create_all_visualizations()\n",
    "            \n",
    "            monitor_memory(\"After visualization\")\n",
    "        else:\n",
    "            if Config.SKIP_ML:\n",
    "                print(\"\\nStep 5: Skipping visualizations (ML was skipped)\")\n",
    "            else:\n",
    "                print(\"\\nStep 5: Skipping visualizations (disabled)\")\n",
    "        \n",
    "        # ========== STEP 6: EXPORT RESULTS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 6: CHUNKED EXPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Exporting all results to disk...\")\n",
    "        \n",
    "        exporter = MemoryOptimizedResultExporter(Config.OUTPUT_DIR)\n",
    "        exporter.export_all(\n",
    "            features_dir, \n",
    "            models, \n",
    "            ml_results, \n",
    "            selected_features,\n",
    "            results.get('scaler')\n",
    "        )\n",
    "        \n",
    "        monitor_memory(\"After export\")\n",
    "        \n",
    "        # ========== STEP 7: CLEANUP ==========\n",
    "        if Config.CLEANUP_TEMP:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 7: CLEANUP\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Removing temporary files...\")\n",
    "            \n",
    "            import shutil\n",
    "            for temp_dir in results['temp_dirs']:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    try:\n",
    "                        shutil.rmtree(temp_dir)\n",
    "                        print(f\"  Removed: {os.path.basename(temp_dir)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Failed to remove {temp_dir}: {e}\")\n",
    "        \n",
    "        # ========== FINAL SUMMARY ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PIPELINE COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calculate execution time\n",
    "        results['end_time'] = datetime.now()\n",
    "        duration = (results['end_time'] - results['start_time']).total_seconds() / 60\n",
    "        \n",
    "        print(f\"Analysis completed successfully\")\n",
    "        print(f\"  Duration: {duration:.1f} minutes\")\n",
    "        print(f\"  Output Directory: {Config.OUTPUT_DIR}\")\n",
    "        print(f\"  Storage Format: {Config.STORAGE_FORMAT}\")\n",
    "        \n",
    "        # Memory summary\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\nMemory Summary:\")\n",
    "        print(f\"  Peak Usage: ~{Config.MAX_MEMORY_GB:.1f} GB (estimated)\")\n",
    "        print(f\"  Current Usage: {mem.percent:.1f}%\")\n",
    "        print(f\"  Strategy: {strategy}\")\n",
    "        \n",
    "        # ML summary\n",
    "        if ml_results and not Config.SKIP_ML:\n",
    "            best_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "            print(f\"\\nBest Model: {best_model[0]}\")\n",
    "            print(f\"  Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
    "        elif Config.SKIP_ML:\n",
    "            print(f\"\\nML Training: Skipped (Test Mode)\")\n",
    "            print(f\"  Features extracted: {len(selected_features)}\")\n",
    "        \n",
    "        # Create success summary\n",
    "        results['summary'] = {\n",
    "            'success': True,\n",
    "            'duration_minutes': duration,\n",
    "            'strategy': strategy,\n",
    "            'memory_limit_gb': Config.MAX_MEMORY_GB,\n",
    "            'best_accuracy': max(r['accuracy'] for r in ml_results.values()) if ml_results else 0,\n",
    "            'features_selected': len(selected_features),\n",
    "            'output_dir': Config.OUTPUT_DIR,\n",
    "            'ml_skipped': Config.SKIP_ML,\n",
    "            'storage_format': Config.STORAGE_FORMAT\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✓ Pipeline execution successful!\")\n",
    "        print(f\"   View results in: {Config.OUTPUT_DIR}\")\n",
    "        if not Config.SKIP_ML:\n",
    "            print(f\"   Open dashboard.html for visualizations\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling\n",
    "        print(f\"\\n✗ Pipeline Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Try to save partial results\n",
    "        print(\"\\nAttempting to save partial results...\")\n",
    "        if 'features_dir' in results and results['features_dir']:\n",
    "            try:\n",
    "                emergency_dir = os.path.join(Config.OUTPUT_DIR, 'emergency_features')\n",
    "                import shutil\n",
    "                shutil.copytree(results['features_dir'], emergency_dir)\n",
    "                print(f\"Partial results saved to: {emergency_dir}\")\n",
    "            except:\n",
    "                print(\"Could not save partial results\")\n",
    "        \n",
    "        # Cleanup on error\n",
    "        print(\"\\nCleaning up after error...\")\n",
    "        import shutil\n",
    "        for temp_dir in results.get('temp_dirs', []):\n",
    "            if os.path.exists(temp_dir):\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Error summary\n",
    "        results['summary'] = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'partial_results': 'emergency_features' if 'features_dir' in results else None\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def monitor_memory(checkpoint_name=\"\"):\n",
    "    \"\"\"\n",
    "    Monitor and display current memory usage.\n",
    "    Helps track memory consumption throughout pipeline.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_name: Description of current pipeline stage\n",
    "    \"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nMemory Status {checkpoint_name}:\")\n",
    "    print(f\"   Used: {mem.used / (1024**3):.2f} GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Warning if memory usage is high\n",
    "    if mem.percent > 85:\n",
    "        print(\"   ⚠ WARNING: High memory usage detected!\")\n",
    "        print(\"      Consider reducing chunk/batch sizes\")\n",
    "    elif mem.percent > 95:\n",
    "        print(\"   ⛔ CRITICAL: Very high memory usage!\")\n",
    "        print(\"      Pipeline may fail - reduce settings immediately\")\n",
    "\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"\n",
    "    Validate that all required libraries and settings are correct.\n",
    "    Run this before starting the pipeline.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENVIRONMENT VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check required libraries\n",
    "    try:\n",
    "        import pyarrow\n",
    "        print(\"✓ PyArrow installed\")\n",
    "    except:\n",
    "        issues.append(\"PyArrow not installed (required for Parquet)\")\n",
    "    \n",
    "    try:\n",
    "        import lightgbm\n",
    "        print(\"✓ LightGBM installed\")\n",
    "    except:\n",
    "        print(\"  LightGBM not installed (optional but recommended)\")\n",
    "    \n",
    "    # Check configuration\n",
    "    if not Config.PCAP_FILE:\n",
    "        issues.append(\"PCAP file not specified\")\n",
    "    elif not os.path.exists(Config.PCAP_FILE):\n",
    "        issues.append(f\"PCAP file not found: {Config.PCAP_FILE}\")\n",
    "    \n",
    "    # Check directories\n",
    "    if not os.path.exists(Config.OUTPUT_DIR):\n",
    "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "        print(f\"✓ Created output directory: {Config.OUTPUT_DIR}\")\n",
    "    \n",
    "    if not os.path.exists(Config.TEMP_DIR):\n",
    "        os.makedirs(Config.TEMP_DIR, exist_ok=True)\n",
    "        print(f\"✓ Created temp directory: {Config.TEMP_DIR}\")\n",
    "    \n",
    "    # Check memory\n",
    "    if Config.MAX_MEMORY_GB > psutil.virtual_memory().available / (1024**3):\n",
    "        issues.append(f\"MAX_MEMORY_GB ({Config.MAX_MEMORY_GB}) exceeds available memory\")\n",
    "    \n",
    "    # Check disk space\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    if disk_usage.free < 10 * (1024**3):  # Less than 10GB free\n",
    "        issues.append(f\"Low disk space: {disk_usage.free / (1024**3):.1f} GB free\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n✗ Validation Failed:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n✓ All checks passed!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# ========== EXECUTION CELL ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY TO RUN MEMORY-OPTIMIZED ANALYSIS V2.0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nThis pipeline is optimized for:\")\n",
    "print(\"• Large PCAP files (10+ GB)\")\n",
    "print(\"• CICIDS2017 dataset\")\n",
    "print(\"• Windows systems (no HDF5 file locking)\")\n",
    "print(\"• AMD Ryzen CPUs (parallel processing)\")\n",
    "print(\"• 32GB RAM systems\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"KEY IMPROVEMENTS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"✓ Parquet storage (no file locking)\")\n",
    "print(\"✓ Fixed bidirectional flow identification\")\n",
    "print(\"✓ CICIDS-specific features (Fwd/Bwd, Active/Idle)\")\n",
    "print(\"✓ LightGBM support (faster on CPU)\")\n",
    "print(\"✓ Better memory management\")\n",
    "print(\"✓ Parallel processing support\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"INSTRUCTIONS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"1. Ensure configuration is complete (Cell 2)\")\n",
    "print(\"2. Verify PCAP file path is correct\")\n",
    "print(\"3. Check available disk space for temp files\")\n",
    "print(\"4. Validate environment: validate_environment()\")\n",
    "print(\"5. Run: results = run_memory_optimized_pipeline()\")\n",
    "print(\"6. Monitor memory with: monitor_memory()\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"QUICK START:\")\n",
    "print(\"-\"*40)\n",
    "print(\"# Validate environment first:\")\n",
    "print(\"validate_environment()\")\n",
    "print(\"\")\n",
    "print(\"# To run the analysis:\")\n",
    "print(\"# results = run_memory_optimized_pipeline()\")\n",
    "print(\"\")\n",
    "print(\"# To check memory anytime:\")\n",
    "print(\"# monitor_memory()\")\n",
    "\n",
    "# Display current system status\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"CURRENT SYSTEM STATUS:\")\n",
    "print(\"-\"*40)\n",
    "monitor_memory()\n",
    "\n",
    "disk_usage = psutil.disk_usage('/')\n",
    "print(f\"\\nDisk Space:\")\n",
    "print(f\"   Available: {disk_usage.free / (1024**3):.2f} GB\")\n",
    "print(f\"   Used: {disk_usage.percent:.1f}%\")\n",
    "\n",
    "if Config.SKIP_ML:\n",
    "    print(\"\\n⚠ ML Training is currently DISABLED\")\n",
    "    print(\"   Uncheck 'Skip ML Training' in config to enable\")\n",
    "\n",
    "print(\"\\n✓ Ready to start analysis!\")\n",
    "print(\"   Run: results = run_memory_optimized_pipeline()\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = run_memory_optimized_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089b568",
   "metadata": {},
   "source": [
    "# ### Step 11: run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b64e7d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\n",
      "======================================================================\n",
      "Pipeline Version: 2.0 (Parquet-based, Windows-optimized)\n",
      "Start Time: 2025-08-30 04:50:20\n",
      "System: 31.4GB RAM, 12 CPU cores\n",
      "\n",
      "============================================================\n",
      "MEMORY ASSESSMENT\n",
      "============================================================\n",
      "PCAP Size: 10.08 GB\n",
      "Total RAM: 31.37 GB\n",
      "Available RAM: 13.98 GB\n",
      "Max Memory Setting: 8.00 GB\n",
      "\n",
      "RECOMMENDATION: Large file detected\n",
      "- Using disk-based processing with Parquet\n",
      "- Enabling flow timeout mechanism\n",
      "- Using incremental learning\n",
      "\n",
      "CPU Optimization: 12 threads available\n",
      "\n",
      "Estimated Processing Time: 30 minutes\n",
      "\n",
      "Processing Strategy: DISK_BASED\n",
      "Memory Limit: 8.0 GB\n",
      "Storage Format: parquet\n",
      "\n",
      "======================================================================\n",
      "STEP 1: STREAMING FEATURE EXTRACTION\n",
      "======================================================================\n",
      "Extracting features from network packets...\n",
      "Using Parquet for reliable storage (no file locking)\n",
      "\n",
      "============================================================\n",
      "PHASE 1: MEMORY-OPTIMIZED FLOW EXTRACTION\n",
      "============================================================\n",
      "Extracting statistical features from network flows...\n",
      "This analyzes packet timing, sizes, and patterns\n",
      "Including CICIDS-specific features (Active/Idle, Fwd/Bwd)\n",
      "\n",
      "Processing PCAP with memory optimization: M:\\TUD\\ATU\\Masters_project\\datasets\\CICIDS2017\\Raw_PCAP\\Monday-WorkingHours.pcap\n",
      "Max flows in memory: 75,000\n",
      "Max packets per flow: 1,000\n",
      "Flow timeout: 120 seconds\n",
      "Output format: Parquet (Windows-optimized)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f55c0aeaf947b8910dc5305be11afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting flow features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Memory limit reached at packet 6,817,527\n",
      "  Flushed 71,332 flows to ./temp_processing\\flow_features\\batch_0000.parquet\n",
      "\n",
      "  Memory limit reached at packet 8,545,770\n",
      "  Flushed 73,935 flows to ./temp_processing\\flow_features\\batch_0001.parquet\n",
      "\n",
      "  Memory limit reached at packet 10,245,812\n",
      "  Flushed 73,522 flows to ./temp_processing\\flow_features\\batch_0002.parquet\n",
      "\n",
      "Flushing remaining flows...\n",
      "  Flushed 60,148 flows to ./temp_processing\\flow_features\\batch_0003.parquet\n",
      "\n",
      "Processed 11,709,971 packets\n",
      "Total flows: 278,937\n",
      "Features saved to: ./temp_processing\\flow_features\n",
      "✓ Flow features extracted to: ./temp_processing\\flow_features\n",
      "\n",
      "============================================================\n",
      "PHASE 2: MEMORY-OPTIMIZED SEMANTIC EXTRACTION\n",
      "============================================================\n",
      "Analyzing packet payloads for malicious content...\n",
      "This performs deep packet inspection and NLP analysis\n",
      "\n",
      "Semantic analysis (memory-optimized): M:\\TUD\\ATU\\Masters_project\\datasets\\CICIDS2017\\Raw_PCAP\\Monday-WorkingHours.pcap\n",
      "Output format: Parquet (Windows-optimized)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcbea633d4042dd89bbed8ae6682fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting semantic features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flushed 75,000 semantic features to ./temp_processing\\semantic_features\\batch_0000.parquet\n",
      "  Flushed 75,000 semantic features to ./temp_processing\\semantic_features\\batch_0001.parquet\n",
      "  Flushed 75,000 semantic features to ./temp_processing\\semantic_features\\batch_0002.parquet\n",
      "  Flushed 55,863 semantic features to ./temp_processing\\semantic_features\\batch_0003.parquet\n",
      "Processed 11,709,971 packets\n",
      "Semantic features saved to: ./temp_processing\\semantic_features\n",
      "✓ Semantic features extracted to: ./temp_processing\\semantic_features\n",
      "\n",
      "============================================================\n",
      "PHASE 3: MERGING FEATURES (DISK-BASED)\n",
      "============================================================\n",
      "Combining flow and semantic features...\n",
      "Merging features using disk-based operations...\n",
      "This preserves memory by processing in batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1615bea7537048cc94ecf61d80305fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Features merged and saved to: ./temp_processing\\combined_features\n",
      "  Total batches processed: 4\n",
      "\n",
      "Memory Status After feature extraction:\n",
      "   Used: 15.96 GB (50.9%)\n",
      "   Available: 15.41 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 2: INCREMENTAL LABEL MATCHING\n",
      "======================================================================\n",
      "Matching flows with CICIDS ground truth labels...\n",
      "\n",
      "Building label cache from CSV files...\n",
      "\n",
      "============================================================\n",
      "LOADING CICIDS LABELS (MEMORY-OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "Processing label file 1/1: Monday-WorkingHours.pcap_ISCX.csv\n",
      "  Completed: 6 chunks\n",
      "\n",
      "----------------------------------------\n",
      "LABEL DISTRIBUTION:\n",
      "----------------------------------------\n",
      "  BENIGN                        :    529,918 (100.0%)\n",
      "\n",
      "Total rows processed: 529,918\n",
      "Port-label mappings created: 30,238\n",
      "IP+Port mappings created: 117,900\n",
      "\n",
      "============================================================\n",
      "APPLYING LABELS TO FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e36c715fc7640d68a7873791e034420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "LABELING RESULTS:\n",
      "----------------------------------------\n",
      "  BENIGN                        :    358,789 (100.0%)\n",
      "\n",
      "Labeled features saved to: ./temp_processing\\labeled_features\n",
      "\n",
      " Warning: >90% flows labeled as BENIGN\n",
      "   This might indicate labeling issues or imbalanced dataset\n",
      "   Consider using stratified sampling for ML training\n",
      "\n",
      "Memory Status After label matching:\n",
      "   Used: 16.10 GB (51.3%)\n",
      "   Available: 15.28 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 3: INCREMENTAL FEATURE ANALYSIS\n",
      "======================================================================\n",
      "Analyzing feature importance and selecting best features...\n",
      "\n",
      "============================================================\n",
      "FEATURE ANALYSIS (MEMORY-OPTIMIZED)\n",
      "============================================================\n",
      "Pass 1: Collecting feature statistics...\n",
      "  This helps understand feature distributions\n",
      "  Analyzing 82 features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebce0b8315945f7b5f26716d1cd3b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting stats:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass 2: Calculating feature importance on sample...\n",
      "  This identifies most informative features\n",
      "  Loading sample of 100,000 flows...\n",
      "  Sample loaded: 40,000 flows\n",
      "  Calculating mutual information scores...\n",
      "  Using 10 CPU cores\n",
      "\n",
      "  ⚠️  Found 75 uninformative features (MI < 0.001)\n",
      "     Examples: ['flow_duration', 'total_packets', 'total_bytes', 'fwd_packets', 'fwd_bytes']\n",
      "\n",
      "========================================\n",
      "TOP 15 FEATURES:\n",
      "========================================\n",
      "Rank  Feature                             Importance Type\n",
      "-----------------------------------------------------------------\n",
      "1     is_well_known_port                      0.0107 Port\n",
      "2     protocol                                0.0052 Other\n",
      "3     is_common_target_port                   0.0032 Port\n",
      "4     syn_count                               0.0016 Flag\n",
      "5     has_dns                                 0.0014 Binary\n",
      "6     avg_ttl                                 0.0012 Other\n",
      "7     fin_count                               0.0010 Other\n",
      "8     fwd_packet_length_min                   0.0006 Forward\n",
      "9     dst_port                                0.0005 Port\n",
      "10    bwd_packet_length_min                   0.0003 Backward\n",
      "11    bwd_packets                             0.0002 Backward\n",
      "12    is_suspicious                           0.0002 Binary\n",
      "13    fwd_packets                             0.0002 Forward\n",
      "14    fwd_psh_flags                           0.0002 Forward\n",
      "15    rst_count                               0.0001 Other\n",
      "\n",
      "Total features analyzed: 82\n",
      "Features selected: 50\n",
      "\n",
      "CICIDS-specific features selected: 29\n",
      "  Examples: ['fwd_packet_length_min', 'bwd_packet_length_min', 'bwd_packets']\n",
      "\n",
      "Memory Status After feature analysis:\n",
      "   Used: 16.09 GB (51.3%)\n",
      "   Available: 15.28 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 4: INCREMENTAL MACHINE LEARNING\n",
      "======================================================================\n",
      "Training ML models with incremental learning...\n",
      "Models to train: ['sgd']\n",
      "\n",
      "============================================================\n",
      "INCREMENTAL MACHINE LEARNING TRAINING\n",
      "============================================================\n",
      "Training models: ['sgd']\n",
      "Using 50 selected features\n",
      "CPU cores available: 10\n",
      "\n",
      "Initializing SGD Classifier...\n",
      "\n",
      "Data split:\n",
      "  Training batches: 3\n",
      "  Testing batches: 1\n",
      "\n",
      "----------------------------------------\n",
      "TRAINING PHASE\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700ade2798174228ac766aca1e7228d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training incrementally:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fitting scaler on first batch (91198 samples)\n",
      "    SGD Batch   0 accuracy: 1.0000\n",
      "\n",
      "Training complete: 3 batches processed\n",
      "\n",
      "----------------------------------------\n",
      "TESTING PHASE\n",
      "----------------------------------------\n",
      "Evaluating on held-out test batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adf0017121844d4a82c970e9dddb98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete: 1 batches processed\n",
      "\n",
      "----------------------------------------\n",
      "MODEL PERFORMANCE\n",
      "----------------------------------------\n",
      "\n",
      "SGD:\n",
      "  Test Accuracy: 1.0000\n",
      "  Test Samples: 79,725\n",
      "  Unique Predictions: 1\n",
      "\n",
      "Memory Status After ML training:\n",
      "   Used: 16.10 GB (51.3%)\n",
      "   Available: 15.27 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 5: SAMPLED VISUALIZATIONS\n",
      "======================================================================\n",
      "Creating interactive visualizations...\n",
      "\n",
      "============================================================\n",
      "GENERATING VISUALIZATIONS (SAMPLED DATA)\n",
      "============================================================\n",
      "Creating visualizations with 10,000 sample points\n",
      "This provides insights without loading full dataset\n",
      "\n",
      "Loading sample of 10,000 flows for visualization...\n",
      "  Loaded 4,000 flows from 4 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23776c4394514e12a5f87ca045388e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating visualizations:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created: Protocol Distribution\n",
      "  Created: Attack Distribution\n",
      "  Created: Flow Timeline\n",
      "  Created: Port Heatmap\n",
      "  Created: Feature Correlation\n",
      "  Created: CICIDS Features\n",
      "  Created: Model Performance\n",
      "\n",
      "Dashboard created: dashboard.html\n",
      "  Open in browser to view all visualizations\n",
      "\n",
      "Visualizations saved to: ./analysis_output\n",
      "  Open dashboard.html in browser to view all charts\n",
      "\n",
      "Memory Status After visualization:\n",
      "   Used: 16.09 GB (51.3%)\n",
      "   Available: 15.28 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 6: CHUNKED EXPORT\n",
      "======================================================================\n",
      "Exporting all results to disk...\n",
      "\n",
      "======================================================================\n",
      "EXPORTING ALL RESULTS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "EXPORTING FEATURES IN CHUNKS\n",
      "============================================================\n",
      "Exporting to CSV with max 100,000 rows per file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d904d7f164405db59ef83397283d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting chunks:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Exported 358,789 flows in 4 chunks\n",
      "✓ Manifest saved to: ./analysis_output\\manifest.json\n",
      "\n",
      "============================================================\n",
      "EXPORTING ML MODELS\n",
      "============================================================\n",
      "✓ Saved sgd: 0.00 MB\n",
      "✓ Saved feature scaler\n",
      "✓ Saved 50 selected feature names\n",
      "\n",
      "✓ Report saved to: ./analysis_output\\analysis_report.txt\n",
      "\n",
      "✓ All results exported to: ./analysis_output\n",
      "  Files created:\n",
      "    - features_chunk_*.csv.gz (data)\n",
      "    - *.pkl (models)\n",
      "    - scaler.pkl (feature scaler)\n",
      "    - selected_features.pkl (feature names)\n",
      "    - analysis_report.txt (summary)\n",
      "    - manifest.json (metadata)\n",
      "    - dashboard.html (visualizations)\n",
      "\n",
      "Memory Status After export:\n",
      "   Used: 16.06 GB (51.2%)\n",
      "   Available: 15.31 GB\n",
      "\n",
      "======================================================================\n",
      "STEP 7: CLEANUP\n",
      "======================================================================\n",
      "Removing temporary files...\n",
      "  Removed: combined_features\n",
      "  Removed: labeled_features\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE\n",
      "======================================================================\n",
      "Analysis completed successfully\n",
      "  Duration: 211.1 minutes\n",
      "  Output Directory: ./analysis_output\n",
      "  Storage Format: parquet\n",
      "\n",
      "Memory Summary:\n",
      "  Peak Usage: ~8.0 GB (estimated)\n",
      "  Current Usage: 51.2%\n",
      "  Strategy: DISK_BASED\n",
      "\n",
      "Best Model: sgd\n",
      "  Accuracy: 1.0000\n",
      "\n",
      "✓ Pipeline execution successful!\n",
      "   View results in: ./analysis_output\n",
      "   Open dashboard.html for visualizations\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to run the feature extracotr / ML trainer:\n",
    "results = run_memory_optimized_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b40635",
   "metadata": {},
   "source": [
    "# ### Step 12: Inference Mode - Classify New PCAP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e017e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Inference Mode - Classify New PCAP Files\n",
    "\"\"\"\n",
    "PURPOSE: Use trained models to classify new PCAP files\n",
    "This cell loads previously trained models and applies them to analyze\n",
    "new network traffic without retraining.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads saved models and scaler from pickle files\n",
    "2. Extracts features from new PCAP file\n",
    "3. Applies trained model to classify flows\n",
    "4. Outputs attack distribution and statistics\n",
    "5. Optionally exports predictions to CSV\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Must have completed training run with saved models\n",
    "- Models saved as: sgd.pkl, lightgbm.pkl, scaler.pkl\n",
    "- Feature names saved as: selected_features.pkl\n",
    "\n",
    "USAGE:\n",
    "1. First run the main pipeline to train and save models\n",
    "2. Configure NEW_PCAP_FILE path\n",
    "3. Run classify_new_pcap() function\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class InferenceEngine:\n",
    "    \"\"\"\n",
    "    Engine for classifying new PCAP files using trained models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir='./analysis_output'):\n",
    "        \"\"\"\n",
    "        Initialize inference engine with model directory\n",
    "        \n",
    "        Args:\n",
    "            model_dir: Directory containing saved models and scaler\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.scaler = None\n",
    "        self.selected_features = None\n",
    "        self.attack_mapping = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS GoldenEye',\n",
    "            4: 'DoS Hulk',\n",
    "            5: 'DoS Slowhttptest',\n",
    "            6: 'DoS slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web Attack - Brute Force',\n",
    "            13: 'Web Attack - SQL Injection',\n",
    "            14: 'Web Attack - XSS'\n",
    "        }\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        Load trained models and preprocessing objects from disk\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"LOADING TRAINED MODELS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load scaler\n",
    "        scaler_path = os.path.join(self.model_dir, 'scaler.pkl')\n",
    "        if os.path.exists(scaler_path):\n",
    "            with open(scaler_path, 'rb') as f:\n",
    "                self.scaler = pickle.load(f)\n",
    "            print(f\"✓ Loaded scaler from {scaler_path}\")\n",
    "        else:\n",
    "            print(f\"✗ Scaler not found at {scaler_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load selected features\n",
    "        features_path = os.path.join(self.model_dir, 'selected_features.pkl')\n",
    "        if os.path.exists(features_path):\n",
    "            with open(features_path, 'rb') as f:\n",
    "                self.selected_features = pickle.load(f)\n",
    "            print(f\"✓ Loaded {len(self.selected_features)} selected features\")\n",
    "        else:\n",
    "            print(f\"✗ Selected features not found at {features_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load available models\n",
    "        model_files = {\n",
    "            'sgd': 'sgd.pkl',\n",
    "            'lightgbm': 'lightgbm.pkl',\n",
    "            'xgboost': 'xgboost_incremental.pkl'\n",
    "        }\n",
    "        \n",
    "        for model_name, filename in model_files.items():\n",
    "            model_path = os.path.join(self.model_dir, filename)\n",
    "            if os.path.exists(model_path):\n",
    "                with open(model_path, 'rb') as f:\n",
    "                    self.models[model_name] = pickle.load(f)\n",
    "                print(f\"✓ Loaded {model_name} model\")\n",
    "        \n",
    "        if not self.models:\n",
    "            print(\"✗ No models found!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"\\nModels ready for inference: {list(self.models.keys())}\")\n",
    "        return True\n",
    "    \n",
    "    def extract_features(self, pcap_file):\n",
    "        \"\"\"\n",
    "        Extract features from new PCAP file\n",
    "        \n",
    "        Args:\n",
    "            pcap_file: Path to PCAP file to analyze\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to directory containing extracted features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXTRACTING FEATURES FROM NEW PCAP\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP: {pcap_file}\")\n",
    "        \n",
    "        # Use the existing pipeline for feature extraction\n",
    "        pipeline = MemoryOptimizedFeaturePipeline()\n",
    "        \n",
    "        # Extract only flow features (faster, usually sufficient)\n",
    "        # Change to 'combined' if you need semantic features too\n",
    "        features_dir = pipeline.extract_all_features(\n",
    "            pcap_file,\n",
    "            mode='flow'  # or 'combined' for flow + semantic\n",
    "        )\n",
    "        \n",
    "        return features_dir\n",
    "    \n",
    "    def classify_flows(self, features_dir, model_name='lightgbm'):\n",
    "        \"\"\"\n",
    "        Classify flows using specified model\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            model_name: Which model to use for classification\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predictions array, flow metadata DataFrame)\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            print(f\"✗ Model {model_name} not loaded\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"\\nClassifying with {model_name}...\")\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        all_predictions = []\n",
    "        all_metadata = []\n",
    "        \n",
    "        # Process each batch file\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in tqdm(feature_files, desc=\"Processing batches\"):\n",
    "            # Load batch\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            # Extract available features\n",
    "            available_features = [f for f in self.selected_features if f in batch.columns]\n",
    "            \n",
    "            if not available_features:\n",
    "                print(f\"✗ No matching features in {file}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare features\n",
    "            X = batch[available_features].fillna(0).values.astype(np.float32)\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            \n",
    "            # Predict\n",
    "            if model_name == 'lightgbm':\n",
    "                predictions = model.predict(X_scaled, num_iteration=model.best_iteration)\n",
    "            else:\n",
    "                predictions = model.predict(X_scaled)\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            \n",
    "            # Store metadata (flow IDs, ports, etc.)\n",
    "            metadata = batch[['flow_id', 'src_port', 'dst_port', 'total_bytes', 'total_packets']].copy()\n",
    "            metadata['prediction'] = predictions\n",
    "            metadata['attack_type'] = [self.attack_mapping.get(int(p), 'Unknown') for p in predictions]\n",
    "            all_metadata.append(metadata)\n",
    "        \n",
    "        # Combine results\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_metadata = pd.concat(all_metadata, ignore_index=True) if all_metadata else pd.DataFrame()\n",
    "        \n",
    "        return all_predictions, all_metadata\n",
    "    \n",
    "    def analyze_results(self, predictions, metadata):\n",
    "        \"\"\"\n",
    "        Analyze and display classification results\n",
    "        \n",
    "        Args:\n",
    "            predictions: Array of predicted labels\n",
    "            metadata: DataFrame with flow details\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLASSIFICATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_counts = Counter(metadata['attack_type'].values)\n",
    "        total_flows = len(predictions)\n",
    "        \n",
    "        print(f\"\\nTotal flows analyzed: {total_flows:,}\")\n",
    "        print(\"\\nAttack Distribution:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for attack_type, count in attack_counts.most_common():\n",
    "            percentage = (count / total_flows) * 100\n",
    "            print(f\"{attack_type:30s}: {count:7,} ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # Calculate attack vs benign ratio\n",
    "        benign_count = attack_counts.get('BENIGN', 0)\n",
    "        attack_count = total_flows - benign_count\n",
    "        attack_ratio = (attack_count / total_flows) * 100 if total_flows > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(f\"Benign traffic: {benign_count:,} ({100-attack_ratio:.1f}%)\")\n",
    "        print(f\"Attack traffic: {attack_count:,} ({attack_ratio:.1f}%)\")\n",
    "        \n",
    "        # Find most suspicious flows\n",
    "        if attack_count > 0:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"Sample Attack Flows Detected:\")\n",
    "            attacks = metadata[metadata['attack_type'] != 'BENIGN'].head(10)\n",
    "            for _, flow in attacks.iterrows():\n",
    "                print(f\"  {flow['attack_type']}: Port {flow['dst_port']}, \"\n",
    "                      f\"{flow['total_bytes']} bytes, {flow['total_packets']} packets\")\n",
    "        \n",
    "        return attack_counts\n",
    "    \n",
    "    def export_results(self, metadata, output_file='predictions.csv'):\n",
    "        \"\"\"\n",
    "        Export classification results to CSV\n",
    "        \n",
    "        Args:\n",
    "            metadata: DataFrame with predictions\n",
    "            output_file: Output CSV filename\n",
    "        \"\"\"\n",
    "        output_path = os.path.join(self.model_dir, output_file)\n",
    "        metadata.to_csv(output_path, index=False)\n",
    "        print(f\"\\n✓ Results exported to: {output_path}\")\n",
    "\n",
    "\n",
    "def classify_new_pcap(pcap_file, model_name='lightgbm', export=True):\n",
    "    \"\"\"\n",
    "    Main function to classify a new PCAP file\n",
    "    \n",
    "    Args:\n",
    "        pcap_file: Path to PCAP file to analyze\n",
    "        model_name: Which model to use ('sgd', 'lightgbm', 'xgboost')\n",
    "        export: Whether to export results to CSV\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification results\n",
    "    \"\"\"\n",
    "    # Initialize inference engine\n",
    "    engine = InferenceEngine()\n",
    "    \n",
    "    # Load models\n",
    "    if not engine.load_models():\n",
    "        print(\"✗ Failed to load models. Ensure training has been completed.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract features from new PCAP\n",
    "    features_dir = engine.extract_features(pcap_file)\n",
    "    \n",
    "    if not features_dir:\n",
    "        print(\"✗ Failed to extract features\")\n",
    "        return None\n",
    "    \n",
    "    # Classify flows\n",
    "    predictions, metadata = engine.classify_flows(features_dir, model_name)\n",
    "    \n",
    "    if predictions is None:\n",
    "        print(\"✗ Classification failed\")\n",
    "        return None\n",
    "    \n",
    "    # Analyze results\n",
    "    attack_counts = engine.analyze_results(predictions, metadata)\n",
    "    \n",
    "    # Export if requested\n",
    "    if export:\n",
    "        engine.export_results(metadata)\n",
    "    \n",
    "    # Clean up temporary features\n",
    "    if Config.CLEANUP_TEMP:\n",
    "        import shutil\n",
    "        shutil.rmtree(features_dir)\n",
    "        print(\"\\n✓ Cleaned up temporary files\")\n",
    "    \n",
    "    return {\n",
    "        'total_flows': len(predictions),\n",
    "        'attack_distribution': dict(attack_counts),\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "\n",
    "# ========== USAGE EXAMPLE ==========\n",
    "print(\"=\"*60)\n",
    "print(\"INFERENCE MODE READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo classify a new PCAP file:\")\n",
    "print(\"1. Ensure you've completed a training run\")\n",
    "print(\"   (Models and features are saved automatically)\")\n",
    "print(\"2. Run classification on new PCAP:\")\n",
    "print(\"\")\n",
    "print(\"   # Example for Thursday's PCAP:\")\n",
    "print(\"   results = classify_new_pcap(\")\n",
    "print(\"       pcap_file='path/to/thursday.pcap',\")\n",
    "print(\"       model_name='lightgbm',  # or 'sgd'\")\n",
    "print(\"       export=True\")\n",
    "print(\"   )\")\n",
    "print(\"\")\n",
    "print(\"The function will:\")\n",
    "print(\"- Extract features from the new PCAP\")\n",
    "print(\"- Apply the trained model\")\n",
    "print(\"- Show attack distribution\")\n",
    "print(\"- Export results to CSV\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
