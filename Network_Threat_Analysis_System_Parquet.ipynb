{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c63a702",
   "metadata": {},
   "source": [
    "# # Network_Threat_Analysis_System\n",
    "# ## Comprehensive Feature Extraction and Machine Learning Framework\n",
    "# \n",
    "# This notebook integrates three analysis approaches:\n",
    "# 1. Statistical flow analysis\n",
    "# 2. Semantic content analysis  \n",
    "# 3. Visual pattern generation\n",
    "#\n",
    "# Output includes both human-readable reports and ML-ready datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565ecd",
   "metadata": {},
   "source": [
    "# ### Step 1: Environment Setup and Dependency Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9752aa6",
   "metadata": {},
   "source": [
    "# ### Step 1:  Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae86d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports - Complete Environment Initialization\n",
    "\"\"\"\n",
    "PURPOSE: Initialize the complete analysis environment with all required libraries\n",
    "This cell sets up the entire working environment for network packet analysis.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Imports all necessary Python libraries for:\n",
    "   - Network packet parsing (scapy, pyshark)\n",
    "   - Data processing (pandas, numpy)\n",
    "   - Machine learning (sklearn, xgboost, lightgbm)\n",
    "   - Visualization (plotly, matplotlib, seaborn)\n",
    "   - System monitoring (psutil for memory tracking)\n",
    "   - File operations (parquet for efficient storage, no HDF5)\n",
    "2. Configures display settings for better readability\n",
    "3. Shows system information (total RAM, available memory)\n",
    "4. Timestamps when analysis starts\n",
    "\n",
    "WHY THESE LIBRARIES:\n",
    "- scapy/pyshark: Parse PCAP files and extract packet information\n",
    "- pandas/numpy: Handle large datasets efficiently with DataFrames\n",
    "- sklearn: Provides ML algorithms and feature selection tools\n",
    "- SGDClassifier: Enables incremental learning for large datasets\n",
    "- xgboost/lightgbm: High-performance gradient boosting for classification\n",
    "- plotly: Creates interactive visualizations\n",
    "- psutil: Monitors memory usage to prevent crashes\n",
    "- pyarrow: Enables Parquet storage without file locking issues on Windows\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Removed h5py/HDF5 (unreliable on Windows)\n",
    "- Added pyarrow for Parquet support (better compression, no locking)\n",
    "- Added lightgbm as alternative to xgboost\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import re\n",
    "import psutil\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter, deque\n",
    "from typing import Dict, List, Tuple, Optional, Any, Generator\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Parquet support (replacing HDF5)\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier  # For incremental learning when data doesn't fit in memory\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Gradient boosting libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb  # Often faster than XGBoost on CPU\n",
    "\n",
    "# Network analysis libraries\n",
    "from scapy.all import *  # For packet parsing\n",
    "import pyshark  # Alternative packet parser with better protocol support\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Jupyter notebook specific\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from tqdm.notebook import tqdm  # Progress bars for loops\n",
    "\n",
    "# Interactive widgets for configuration\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Global configuration\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "pd.set_option('display.max_columns', 50)  # Show more columns in DataFrame display\n",
    "pd.set_option('display.max_rows', 100)  # Show more rows in DataFrame display\n",
    "\n",
    "# Display system information\n",
    "print(\"=\"*70)\n",
    "print(\"MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\")\n",
    "print(\"=\"*70)\n",
    "print(f\"System Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()} ({multiprocessing.cpu_count()} usable)\")\n",
    "print(f\"Storage Backend: Parquet (no file locking issues)\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0e5b7",
   "metadata": {},
   "source": [
    "# ### Step 1.5: Clear Temporary Files Before Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4436719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: Cleanup - Clear Temporary Files Before Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Clean up temporary files from previous runs\n",
    "This cell ensures a clean start by removing old temporary files that could\n",
    "interfere with the current analysis.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Checks for existing temporary directories\n",
    "2. Removes old flow, semantic, combined, and labeled features\n",
    "3. Clears any cached files from previous runs\n",
    "4. Recreates clean directory structure\n",
    "5. Optionally clears output directory\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Prevents mixing data from different PCAP files\n",
    "- Avoids file naming conflicts\n",
    "- Ensures consistent results\n",
    "- Frees up disk space\n",
    "- Prevents loading old data accidentally\n",
    "\n",
    "RUN THIS:\n",
    "- Before starting a new analysis\n",
    "- After a failed/interrupted run\n",
    "- When switching between different PCAP files\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import psutil\n",
    "\n",
    "def cleanup_temp_files(clear_output=False):\n",
    "    \"\"\"\n",
    "    Remove temporary files from previous pipeline runs.\n",
    "    \n",
    "    Args:\n",
    "        clear_output: If True, also clears the output directory (default: False)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CLEANING UP TEMPORARY FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if Config is defined\n",
    "    try:\n",
    "        temp_dir = Config.TEMP_DIR\n",
    "        output_dir = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        # Use default values if Config not yet defined\n",
    "        temp_dir = './temp_processing'\n",
    "        output_dir = './analysis_output'\n",
    "        print(\"⚠ Config not defined, using default directories\")\n",
    "    \n",
    "    # List of temporary directories to clean\n",
    "    temp_dirs = [\n",
    "        os.path.join(temp_dir, 'flow_features'),\n",
    "        os.path.join(temp_dir, 'semantic_features'),\n",
    "        os.path.join(temp_dir, 'combined_features'),\n",
    "        os.path.join(temp_dir, 'labeled_features')\n",
    "    ]\n",
    "    \n",
    "    # Clean each temporary directory\n",
    "    for dir_path in temp_dirs:\n",
    "        if os.path.exists(dir_path):\n",
    "            try:\n",
    "                # Count files before deletion\n",
    "                file_count = sum(len(files) for _, _, files in os.walk(dir_path))\n",
    "                size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                             for dirpath, _, filenames in os.walk(dir_path)\n",
    "                             for filename in filenames) / (1024*1024)\n",
    "                \n",
    "                # Remove directory\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"✓ Removed: {os.path.basename(dir_path)}\")\n",
    "                print(f\"  Deleted {file_count} files ({size_mb:.1f} MB)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to remove {dir_path}: {e}\")\n",
    "    \n",
    "    # Clean root temp directory if empty or recreate it\n",
    "    if os.path.exists(temp_dir):\n",
    "        try:\n",
    "            # Check if temp directory has any other files\n",
    "            remaining_files = os.listdir(temp_dir)\n",
    "            if not remaining_files:\n",
    "                shutil.rmtree(temp_dir)\n",
    "                print(f\"✓ Removed empty temp directory\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not clean temp directory: {e}\")\n",
    "    \n",
    "    # Recreate clean directory structure\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    for dir_name in ['flow_features', 'semantic_features', 'combined_features', 'labeled_features']:\n",
    "        os.makedirs(os.path.join(temp_dir, dir_name), exist_ok=True)\n",
    "    print(\"✓ Created clean directory structure\")\n",
    "    \n",
    "    # Optionally clear output directory\n",
    "    if clear_output and os.path.exists(output_dir):\n",
    "        try:\n",
    "            # Ask for confirmation\n",
    "            response = input(f\"\\n⚠ Clear output directory '{output_dir}'? (y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                shutil.rmtree(output_dir)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                print(f\"✓ Cleared output directory\")\n",
    "            else:\n",
    "                print(\"✓ Output directory preserved\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to clear output directory: {e}\")\n",
    "    \n",
    "    # Report disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"\\nDisk space available: {disk.free / (1024**3):.1f} GB ({100-disk.percent:.1f}% free)\")\n",
    "    \n",
    "    print(\"\\n✓ Cleanup complete - ready for new analysis\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def check_temp_files():\n",
    "    \"\"\"\n",
    "    Check what temporary files currently exist without deleting them.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CHECKING TEMPORARY FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if Config is defined\n",
    "    try:\n",
    "        temp_dir = Config.TEMP_DIR\n",
    "    except NameError:\n",
    "        temp_dir = './temp_processing'\n",
    "        print(\"⚠ Config not defined, checking default directory\")\n",
    "    \n",
    "    if not os.path.exists(temp_dir):\n",
    "        print(\"No temporary directory found - clean start\")\n",
    "        return\n",
    "    \n",
    "    # Check each subdirectory\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        if files:\n",
    "            dir_size = sum(os.path.getsize(os.path.join(root, f)) for f in files)\n",
    "            total_files += len(files)\n",
    "            total_size += dir_size\n",
    "            \n",
    "            rel_path = os.path.relpath(root, temp_dir)\n",
    "            print(f\"\\n{rel_path}:\")\n",
    "            print(f\"  Files: {len(files)}\")\n",
    "            print(f\"  Size: {dir_size / (1024*1024):.1f} MB\")\n",
    "            \n",
    "            # Show sample files\n",
    "            sample_files = files[:3]\n",
    "            for f in sample_files:\n",
    "                print(f\"    - {f}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"    ... and {len(files)-3} more files\")\n",
    "    \n",
    "    if total_files > 0:\n",
    "        print(f\"\\nTotal: {total_files} files, {total_size / (1024*1024):.1f} MB\")\n",
    "        print(\"\\n⚠ Temporary files exist from previous run\")\n",
    "        print(\"  Run cleanup_temp_files() to remove them\")\n",
    "    else:\n",
    "        print(\"\\n✓ No temporary files found - clean start\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Create interactive cleanup interface\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_cleanup_ui():\n",
    "    \"\"\"\n",
    "    Create an interactive cleanup interface with buttons.\n",
    "    \"\"\"\n",
    "    display(HTML(\"<h3>Cleanup Manager</h3>\"))\n",
    "    \n",
    "    # Output area for messages\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Check button\n",
    "    check_btn = widgets.Button(\n",
    "        description='Check Temp Files',\n",
    "        button_style='info',\n",
    "        tooltip='Check what temporary files exist',\n",
    "        icon='search'\n",
    "    )\n",
    "    \n",
    "    # Clean temp button\n",
    "    clean_temp_btn = widgets.Button(\n",
    "        description='Clean Temp Files',\n",
    "        button_style='warning',\n",
    "        tooltip='Remove temporary processing files',\n",
    "        icon='trash'\n",
    "    )\n",
    "    \n",
    "    # Clean all button\n",
    "    clean_all_btn = widgets.Button(\n",
    "        description='Clean Everything',\n",
    "        button_style='danger',\n",
    "        tooltip='Remove all temporary and output files',\n",
    "        icon='exclamation-triangle'\n",
    "    )\n",
    "    \n",
    "    def on_check(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            check_temp_files()\n",
    "    \n",
    "    def on_clean_temp(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            cleanup_temp_files(clear_output=False)\n",
    "    \n",
    "    def on_clean_all(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            cleanup_temp_files(clear_output=True)\n",
    "    \n",
    "    check_btn.on_click(on_check)\n",
    "    clean_temp_btn.on_click(on_clean_temp)\n",
    "    clean_all_btn.on_click(on_clean_all)\n",
    "    \n",
    "    # Layout\n",
    "    button_box = widgets.HBox([check_btn, clean_temp_btn, clean_all_btn])\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<p>Manage temporary files from previous runs:</p>\"),\n",
    "        button_box,\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "# Display the cleanup UI\n",
    "create_cleanup_ui()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK COMMANDS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"# Check what files exist:\")\n",
    "print(\"check_temp_files()\")\n",
    "print(\"\")\n",
    "print(\"# Clean only temp files (recommended before each run):\")\n",
    "print(\"cleanup_temp_files()\")\n",
    "print(\"\")\n",
    "print(\"# Clean everything including output:\")\n",
    "print(\"cleanup_temp_files(clear_output=True)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5606",
   "metadata": {},
   "source": [
    "# ### Step 2: Configuration Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Class - Interactive Settings & Memory Management with Multi-PCAP Support\n",
    "\"\"\"\n",
    "PURPOSE: Central configuration hub for all analysis parameters with multi-day CICIDS support\n",
    "This cell creates an interactive configuration interface using Jupyter widgets.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Defines all configurable parameters for the analysis pipeline\n",
    "2. Creates an interactive UI with sliders, text boxes, and checkboxes\n",
    "3. Automatically determines optimal processing strategy based on file size\n",
    "4. Validates user inputs and checks system resources\n",
    "5. Sets up directory structure for outputs and temporary files\n",
    "6. ENHANCED: Supports multiple PCAP files for balanced training data\n",
    "\n",
    "KEY FUNCTIONALITY:\n",
    "- Multi-PCAP Management: Handles multiple PCAP files for balanced attack representation\n",
    "- Memory Management: Sets limits on RAM usage and flow storage\n",
    "- Processing Strategy: Chooses between disk-based or memory-based processing\n",
    "- ML Configuration: Selects which models to train and sampling strategy\n",
    "- Feature Settings: Determines number of features and data types to use\n",
    "\n",
    "CONFIGURATION CATEGORIES:\n",
    "1. Input/Output: Multiple PCAP files, label files, output directory\n",
    "2. Memory Settings: MAX_MEMORY_GB, MAX_FLOWS_IN_MEMORY, batch sizes\n",
    "3. Processing: Chunk sizes, flow timeouts, packet limits\n",
    "4. Analysis Modes: Flow analysis, semantic analysis, NLP depth\n",
    "5. ML Settings: Model selection, train/test split, sampling\n",
    "6. Storage: Parquet format usage, compression, temporary file cleanup\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Support for multiple PCAP files for balanced training\n",
    "- Enhanced Random Forest configuration\n",
    "- Better evaluation metrics beyond accuracy\n",
    "- Improved class balancing options\n",
    "\n",
    "The class uses class methods (@classmethod) so configuration is global\n",
    "and accessible throughout the entire pipeline.\n",
    "\"\"\"\n",
    "\n",
    "class Config:\n",
    "    # ============= FILE PATHS (ENHANCED FOR MULTI-PCAP) =============\n",
    "    PCAP_FILES = []  # List of PCAP files for multi-day training\n",
    "    PCAP_FILE = ''  # Primary PCAP file (backward compatibility)\n",
    "    LABEL_FILE = ''  # Single label file path (will be set if multiple CSVs combined)\n",
    "    LABEL_FILES = []  # List of CICIDS2017 CSV label files\n",
    "    OUTPUT_DIR = './analysis_output'  # Where to save results\n",
    "    \n",
    "    # ============= MEMORY OPTIMIZATION (Optimized for 32GB System) =============\n",
    "    MAX_MEMORY_GB = 8.0  # Maximum RAM to use (25% of 32GB, prevents system freeze)\n",
    "    MAX_FLOWS_IN_MEMORY = 100000  # Flows kept in RAM before disk flush (increased for 32GB)\n",
    "    MAX_PACKETS_PER_FLOW = 1000  # Cap packets per flow to prevent memory bloat\n",
    "    USE_DISK_CACHE = True  # Use Parquet disk storage for large files\n",
    "    TEMP_DIR = './temp_processing'  # Temporary storage location\n",
    "    \n",
    "    # ============= PROCESSING PARAMETERS (Optimized for Ryzen 5) =============\n",
    "    CHUNK_SIZE = 50000  # Packets processed at once (increased for better CPU utilization)\n",
    "    BATCH_SIZE = 200000  # Flows per ML training batch (increased for 32GB RAM)\n",
    "    MAX_PACKETS = 0  # Limit packets to process (0 = unlimited)\n",
    "    FLOW_TIMEOUT = 120  # Seconds before flow considered complete\n",
    "    N_JOBS = -1  # Number of parallel jobs (-1 = use all CPU cores)\n",
    "    \n",
    "    # ============= SAMPLING STRATEGY (ENHANCED FOR IMBALANCED DATA) =============\n",
    "    USE_SAMPLING = True  # Sample data if too large for ML\n",
    "    SAMPLE_SIZE = 1000000  # Maximum flows for ML training (increased for 32GB)\n",
    "    STRATIFY_SAMPLE = True  # Maintain attack type distribution in sample\n",
    "    USE_CLASS_BALANCING = True  # Apply class weights to handle imbalanced data\n",
    "    \n",
    "    # ============= ANALYSIS MODES =============\n",
    "    ANALYSIS_MODE = 'combined'  # Options: 'flow', 'semantic', 'combined'\n",
    "    DEEP_INSPECTION = True  # Enable NLP payload analysis (slower but better detection)\n",
    "    USE_CICIDS_LABELS = False  # Whether to use CICIDS ground truth\n",
    "    GENERATE_VISUALS = True  # Create visualization charts\n",
    "    ML_EXPORT = True  # Export ML models and features\n",
    "    SKIP_ML = False  # Skip ML training for pipeline testing\n",
    "    \n",
    "    # ============= FEATURE ENGINEERING =============\n",
    "    TOP_FEATURES = 50  # Number of best features to select (increased for better accuracy)\n",
    "    FEATURE_DTYPE = np.float32  # Data type (float32 saves memory vs float64)\n",
    "    EXTRACT_CICIDS_FEATURES = True  # Extract CICIDS-specific features\n",
    "    \n",
    "    # ============= MACHINE LEARNING (ENHANCED WITH BALANCED MODELS) =============\n",
    "    TEST_SIZE = 0.2  # Fraction of data for testing\n",
    "    RANDOM_STATE = 42  # Random seed for reproducibility\n",
    "    SELECTED_MODELS = ['random_forest']  # Single model selection\n",
    "    USE_INCREMENTAL_LEARNING = True  # Train in batches for large datasets\n",
    "    USE_STRATIFIED_SPLIT = True  # Maintain class distribution in train/test split\n",
    "    \n",
    "    # ============= EVALUATION METRICS (ENHANCED BEYOND ACCURACY) =============\n",
    "    EVALUATION_METRICS = ['accuracy', 'f1_macro', 'f1_weighted', 'precision_macro', 'recall_macro', 'per_class']\n",
    "    CALCULATE_PER_CLASS_METRICS = True  # Detailed per-attack-type performance\n",
    "    MINIMUM_ATTACK_DETECTION_RATE = 0.7  # Minimum acceptable attack detection rate\n",
    "    \n",
    "    # ============= STORAGE SETTINGS (Using Parquet) =============\n",
    "    STORAGE_FORMAT = 'parquet'  # Storage format (parquet recommended over hdf5)\n",
    "    USE_PARQUET = True  # Use Parquet format for efficient disk storage\n",
    "    COMPRESSION = 'snappy'  # Fast compression algorithm\n",
    "    CLEANUP_TEMP = True  # Delete temporary files after completion\n",
    "    \n",
    "    @classmethod\n",
    "    def check_memory_requirements(cls):\n",
    "        \"\"\"\n",
    "        Analyzes PCAP file sizes and available RAM to determine processing strategy.\n",
    "        Enhanced to handle multiple PCAP files.\n",
    "        \n",
    "        Returns:\n",
    "            str: 'DISK_BASED' for large files, 'MEMORY_BASED' for small files\n",
    "        \"\"\"\n",
    "        total_file_size_gb = 0\n",
    "        valid_files = 0\n",
    "        \n",
    "        # Check all PCAP files\n",
    "        pcap_files = cls.PCAP_FILES if cls.PCAP_FILES else [cls.PCAP_FILE] if cls.PCAP_FILE else []\n",
    "        \n",
    "        for pcap_file in pcap_files:\n",
    "            if pcap_file and os.path.exists(pcap_file):\n",
    "                file_size_gb = os.path.getsize(pcap_file) / (1024**3)\n",
    "                total_file_size_gb += file_size_gb\n",
    "                valid_files += 1\n",
    "        \n",
    "        if valid_files == 0:\n",
    "            return False\n",
    "            \n",
    "        available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "        total_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MEMORY ASSESSMENT (MULTI-PCAP)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP Files: {valid_files}\")\n",
    "        print(f\"Total PCAP Size: {total_file_size_gb:.2f} GB\")\n",
    "        print(f\"Average PCAP Size: {total_file_size_gb/valid_files:.2f} GB\")\n",
    "        print(f\"Total RAM: {total_gb:.2f} GB\")\n",
    "        print(f\"Available RAM: {available_gb:.2f} GB\")\n",
    "        print(f\"Max Memory Setting: {cls.MAX_MEMORY_GB:.2f} GB\")\n",
    "        \n",
    "        # Decision logic for processing strategy\n",
    "        if total_file_size_gb > available_gb * 0.3:  # Total files > 30% of available RAM\n",
    "            print(\"\\nRECOMMENDATION: Large dataset detected\")\n",
    "            print(\"- Using disk-based processing with Parquet\")\n",
    "            print(\"- Enabling flow timeout mechanism\")\n",
    "            print(\"- Using incremental learning\")\n",
    "            print(\"- Processing files sequentially for memory efficiency\")\n",
    "            cls.USE_DISK_CACHE = True\n",
    "            cls.USE_INCREMENTAL_LEARNING = True\n",
    "            cls.USE_SAMPLING = True\n",
    "            processing_strategy = \"DISK_BASED\"\n",
    "        else:\n",
    "            print(\"\\nRECOMMENDATION: Files can fit in memory\")\n",
    "            print(\"- Using hybrid processing\")\n",
    "            processing_strategy = \"MEMORY_BASED\"\n",
    "        \n",
    "        # Time estimation (empirical: 1GB ≈ 2-4 minutes with optimizations)\n",
    "        estimated_time = total_file_size_gb * (3 if cls.USE_DISK_CACHE else 2)\n",
    "        print(f\"\\nEstimated Processing Time: {estimated_time:.0f} minutes\")\n",
    "        \n",
    "        return processing_strategy\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_interactive_ui(cls):\n",
    "        \"\"\"\n",
    "        Creates an interactive configuration interface using Jupyter widgets.\n",
    "        Enhanced to support multiple PCAP files for balanced training.\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(\"<h2>Memory-Optimized Network Analysis Configuration (V2.0 - Multi-PCAP)</h2>\"))\n",
    "        \n",
    "        # ========== MULTI-PCAP FILE SELECTION SECTION ==========\n",
    "        display(HTML(\"<h3>Multi-Day PCAP Configuration</h3>\"))\n",
    "        display(HTML(\"<p><strong>Recommendation:</strong> Use Tuesday-Friday CICIDS data for balanced attack representation</p>\"))\n",
    "        \n",
    "        pcap_input = widgets.Textarea(\n",
    "            placeholder='Enter PCAP paths (one per line):\\n/path/to/Tuesday-WorkingHours.pcap\\n/path/to/Wednesday-workingHours.pcap\\n/path/to/Thursday-WorkingHours.pcap',\n",
    "            description='PCAP Files:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='80%', height='100px')\n",
    "        )\n",
    "        \n",
    "        label_input = widgets.Textarea(\n",
    "            placeholder='Enter corresponding CSV paths (one per line):\\n/path/to/Tuesday-WorkingHours.csv\\n/path/to/Wednesday-workingHours.csv\\n/path/to/Thursday-WorkingHours.csv',\n",
    "            description='Label Files:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='80%', height='100px')\n",
    "        )\n",
    "        \n",
    "        output_dir = widgets.Text(\n",
    "            value='./analysis_output',\n",
    "            description='Output Dir:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # ========== MEMORY SETTINGS SECTION ==========\n",
    "        display(HTML(\"<h3>Memory Optimization Settings</h3>\"))\n",
    "        \n",
    "        memory_slider = widgets.FloatSlider(\n",
    "            value=8.0,\n",
    "            min=4.0,\n",
    "            max=psutil.virtual_memory().total / (1024**3),\n",
    "            step=0.5,\n",
    "            description='Max RAM (GB):',\n",
    "            style={'description_width': 'initial'},\n",
    "            readout_format='.1f'\n",
    "        )\n",
    "        \n",
    "        max_flows = widgets.IntText(\n",
    "            value=100000,\n",
    "            description='Max Flows in Memory:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        use_sampling = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Sampling for ML (recommended for large files)',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        sample_size = widgets.IntText(\n",
    "            value=1000000,\n",
    "            description='Sample Size:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Class balancing option\n",
    "        use_class_balancing = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Enable Class Balancing (critical for imbalanced data)',\n",
    "            indent=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # ========== PROCESSING OPTIONS ==========\n",
    "        display(HTML(\"<h3>Processing Configuration</h3>\"))\n",
    "        \n",
    "        chunk_slider = widgets.IntSlider(\n",
    "            value=50000,\n",
    "            min=10000,\n",
    "            max=100000,\n",
    "            step=10000,\n",
    "            description='Chunk Size:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        batch_slider = widgets.IntSlider(\n",
    "            value=200000,\n",
    "            min=50000,\n",
    "            max=500000,\n",
    "            step=50000,\n",
    "            description='Batch Size:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        analysis_mode = widgets.RadioButtons(\n",
    "            options=['flow', 'semantic', 'combined'],\n",
    "            value='combined',\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        storage_format = widgets.RadioButtons(\n",
    "            options=['parquet', 'csv'],\n",
    "            value='parquet',\n",
    "            description='Storage Format:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        skip_ml = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Skip ML Training (for testing pipeline)',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        # ========== ML MODEL SELECTION ==========\n",
    "        display(HTML(\"<h3>Machine Learning Model Selection</h3>\"))\n",
    "        \n",
    "        model_selector = widgets.RadioButtons(\n",
    "            options=[\n",
    "                ('SGD (Linear, Fast)', 'sgd'),\n",
    "                ('Random Forest (Robust, Balanced)', 'random_forest'),\n",
    "                ('LightGBM (High Performance)', 'lightgbm'),\n",
    "                ('XGBoost (Alternative Boosting)', 'xgboost_incremental')\n",
    "            ],\n",
    "            value='random_forest',\n",
    "            description='ML Model:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        use_stratified = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Stratified Train/Test Split',\n",
    "            indent=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # ========== EVALUATION METRICS ==========\n",
    "        display(HTML(\"<h3>Evaluation Metrics Selection</h3>\"))\n",
    "        display(HTML(\"<p>Select which metrics to calculate for model evaluation:</p>\"))\n",
    "        \n",
    "        accuracy_check = widgets.Checkbox(value=True, description='Accuracy')\n",
    "        f1_macro_check = widgets.Checkbox(value=True, description='F1-Score (Macro)')\n",
    "        f1_weighted_check = widgets.Checkbox(value=True, description='F1-Score (Weighted)')\n",
    "        precision_check = widgets.Checkbox(value=True, description='Precision (Macro)')\n",
    "        recall_check = widgets.Checkbox(value=True, description='Recall (Macro)')\n",
    "        per_class_check = widgets.Checkbox(value=True, description='Per-Class Metrics')\n",
    "        \n",
    "        metrics_box = widgets.VBox([\n",
    "            widgets.HBox([accuracy_check, f1_macro_check]),\n",
    "            widgets.HBox([f1_weighted_check, precision_check]),\n",
    "            widgets.HBox([recall_check, per_class_check])\n",
    "        ])\n",
    "        \n",
    "        # Progress output area\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # ========== VALIDATE BUTTON ==========\n",
    "        validate_btn = widgets.Button(\n",
    "            description='Validate Multi-PCAP Config',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='250px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def validate_config(b):\n",
    "            \"\"\"\n",
    "            Enhanced validation for multi-PCAP configuration\n",
    "            \"\"\"\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Process PCAP files\n",
    "                pcap_lines = pcap_input.value.strip().split('\\n')\n",
    "                pcap_files = [f.strip().strip('\"') for f in pcap_lines if f.strip()]\n",
    "                \n",
    "                # Process CSV files\n",
    "                csv_lines = label_input.value.strip().split('\\n')\n",
    "                csv_files = [f.strip().strip('\"') for f in csv_lines if f.strip()]\n",
    "                \n",
    "                # Set configuration values\n",
    "                cls.PCAP_FILES = pcap_files\n",
    "                cls.PCAP_FILE = pcap_files[0] if pcap_files else ''  # Backward compatibility\n",
    "                cls.LABEL_FILES = csv_files\n",
    "                cls.OUTPUT_DIR = output_dir.value\n",
    "                cls.MAX_MEMORY_GB = memory_slider.value\n",
    "                cls.MAX_FLOWS_IN_MEMORY = max_flows.value\n",
    "                cls.CHUNK_SIZE = chunk_slider.value\n",
    "                cls.BATCH_SIZE = batch_slider.value\n",
    "                cls.USE_SAMPLING = use_sampling.value\n",
    "                cls.SAMPLE_SIZE = sample_size.value\n",
    "                cls.USE_CLASS_BALANCING = use_class_balancing.value\n",
    "                cls.ANALYSIS_MODE = analysis_mode.value\n",
    "                cls.SELECTED_MODELS = [model_selector.value]  # Single model selection\n",
    "                cls.USE_STRATIFIED_SPLIT = use_stratified.value\n",
    "                cls.SKIP_ML = skip_ml.value\n",
    "                cls.STORAGE_FORMAT = storage_format.value\n",
    "                cls.USE_PARQUET = (storage_format.value == 'parquet')\n",
    "                \n",
    "                # Handle evaluation metrics checkboxes\n",
    "                selected_metrics = []\n",
    "                if accuracy_check.value:\n",
    "                    selected_metrics.append('accuracy')\n",
    "                if f1_macro_check.value:\n",
    "                    selected_metrics.append('f1_macro')\n",
    "                if f1_weighted_check.value:\n",
    "                    selected_metrics.append('f1_weighted')\n",
    "                if precision_check.value:\n",
    "                    selected_metrics.append('precision_macro')\n",
    "                if recall_check.value:\n",
    "                    selected_metrics.append('recall_macro')\n",
    "                if per_class_check.value:\n",
    "                    selected_metrics.append('per_class')\n",
    "                \n",
    "                cls.EVALUATION_METRICS = selected_metrics if selected_metrics else ['accuracy']\n",
    "                \n",
    "                # Validate PCAP files\n",
    "                if not pcap_files:\n",
    "                    print(\"Error: No PCAP files specified\")\n",
    "                    return\n",
    "                \n",
    "                valid_pcaps = []\n",
    "                total_size = 0\n",
    "                for pcap_file in pcap_files:\n",
    "                    if os.path.exists(pcap_file):\n",
    "                        size_gb = os.path.getsize(pcap_file) / (1024**3)\n",
    "                        total_size += size_gb\n",
    "                        valid_pcaps.append((pcap_file, size_gb))\n",
    "                        print(f\"Found: {os.path.basename(pcap_file)} ({size_gb:.2f} GB)\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Not found: {pcap_file}\")\n",
    "                \n",
    "                if not valid_pcaps:\n",
    "                    print(\"Error: No valid PCAP files found\")\n",
    "                    return\n",
    "                \n",
    "                cls.PCAP_FILES = [p[0] for p in valid_pcaps]\n",
    "                \n",
    "                # Validate CSV files\n",
    "                if csv_files:\n",
    "                    cls.USE_CICIDS_LABELS = True\n",
    "                    valid_csvs = []\n",
    "                    for csv_file in csv_files:\n",
    "                        if os.path.exists(csv_file):\n",
    "                            valid_csvs.append(csv_file)\n",
    "                            print(f\"Found: {os.path.basename(csv_file)}\")\n",
    "                        else:\n",
    "                            print(f\"Warning: CSV not found: {csv_file}\")\n",
    "                    \n",
    "                    if valid_csvs:\n",
    "                        cls.LABEL_FILES = valid_csvs\n",
    "                \n",
    "                # Analyze memory requirements\n",
    "                strategy = cls.check_memory_requirements()\n",
    "                \n",
    "                # Create directories\n",
    "                os.makedirs(cls.OUTPUT_DIR, exist_ok=True)\n",
    "                os.makedirs(cls.TEMP_DIR, exist_ok=True)\n",
    "                \n",
    "                # Display configuration summary\n",
    "                cls.display_config()\n",
    "                \n",
    "                print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "                print(f\"Class Balancing: {'ENABLED' if cls.USE_CLASS_BALANCING else 'DISABLED'}\")\n",
    "                print(f\"Stratified Split: {'ENABLED' if cls.USE_STRATIFIED_SPLIT else 'DISABLED'}\")\n",
    "                print(f\"Evaluation Metrics: {', '.join(cls.EVALUATION_METRICS)}\")\n",
    "                \n",
    "                if len(valid_pcaps) > 1:\n",
    "                    print(f\"\\nMulti-day training will provide balanced attack examples\")\n",
    "                    print(\"This should resolve the 'always predict BENIGN' problem\")\n",
    "                else:\n",
    "                    print(f\"\\nWarning: Single PCAP may have imbalanced data\")\n",
    "                    print(\"Consider adding more days for better attack representation\")\n",
    "                \n",
    "                if cls.SKIP_ML:\n",
    "                    print(\"ML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "                \n",
    "                print(\"\\nConfiguration validated - ready for multi-day analysis!\")\n",
    "        \n",
    "        validate_btn.on_click(validate_config)\n",
    "        \n",
    "        # ========== LAYOUT ORGANIZATION ==========\n",
    "        pcap_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>PCAP File Selection</h4>\"),\n",
    "            pcap_input,\n",
    "            label_input,\n",
    "            output_dir\n",
    "        ])\n",
    "        \n",
    "        memory_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Memory & Sampling</h4>\"),\n",
    "            memory_slider,\n",
    "            max_flows,\n",
    "            use_sampling,\n",
    "            sample_size,\n",
    "            use_class_balancing\n",
    "        ])\n",
    "        \n",
    "        ml_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Machine Learning Configuration</h4>\"),\n",
    "            model_selector,\n",
    "            use_stratified\n",
    "        ])\n",
    "        \n",
    "        processing_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Processing Settings</h4>\"),\n",
    "            chunk_slider,\n",
    "            batch_slider,\n",
    "            analysis_mode,\n",
    "            storage_format,\n",
    "            skip_ml\n",
    "        ])\n",
    "        \n",
    "        # Display complete interface\n",
    "        display(widgets.VBox([\n",
    "            pcap_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            memory_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            ml_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Evaluation Metrics</h4>\"),\n",
    "            metrics_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            processing_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            validate_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "        \n",
    "        return output_area\n",
    "    \n",
    "    @classmethod\n",
    "    def display_config(cls):\n",
    "        \"\"\"Enhanced configuration summary with multi-PCAP information\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED CONFIGURATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP Files: {len(cls.PCAP_FILES)}\")\n",
    "        for i, pcap in enumerate(cls.PCAP_FILES, 1):\n",
    "            print(f\"  {i}. {os.path.basename(pcap)}\")\n",
    "        print(f\"Label Files: {len(cls.LABEL_FILES)}\")\n",
    "        print(f\"Output: {cls.OUTPUT_DIR}\")\n",
    "        print(f\"Memory Limit: {cls.MAX_MEMORY_GB} GB\")\n",
    "        print(f\"Class Balancing: {'ENABLED' if cls.USE_CLASS_BALANCING else 'DISABLED'}\")\n",
    "        print(f\"Stratified Split: {'ENABLED' if cls.USE_STRATIFIED_SPLIT else 'DISABLED'}\")\n",
    "        print(f\"Model: {', '.join(cls.SELECTED_MODELS)}\")\n",
    "        print(f\"Metrics: {', '.join(cls.EVALUATION_METRICS)}\")\n",
    "        if cls.SKIP_ML:\n",
    "            print(\"ML Training: DISABLED (Test Mode)\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Run the enhanced interactive UI\n",
    "output = Config.setup_interactive_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0a899",
   "metadata": {},
   "source": [
    "# ### Step 2.5: (OPTIONAL) Dataset Preparation (For Testing & Debugging) Use this to make smaller PCAP file and csv to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.5: Test Dataset Preparation - Quick Validation Before Full Run\n",
    "\"\"\"\n",
    "PURPOSE: Create small test datasets to validate pipeline functionality\n",
    "This cell helps you test the entire pipeline in minutes instead of hours.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates a smaller PCAP file from your main file\n",
    "2. Extracts corresponding CSV label rows  \n",
    "3. Provides UI to customize test size\n",
    "4. Validates pipeline works before committing to full analysis\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Catches errors in 5 minutes instead of 4 hours\n",
    "- Validates file locking fixes work\n",
    "- Tests memory settings are appropriate\n",
    "- Confirms CSV label matching functions\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class TestDatasetCreator:\n",
    "    @staticmethod\n",
    "    def create_test_ui():\n",
    "        \"\"\"\n",
    "        Creates an interactive UI for generating test datasets\n",
    "        \"\"\"\n",
    "        display(HTML(\"<h2>Test Dataset Generator</h2>\"))\n",
    "        display(HTML(\"<p>Create small test files to validate pipeline before full run</p>\"))\n",
    "        \n",
    "        # Packet count slider\n",
    "        packet_count = widgets.IntSlider(\n",
    "            value=100000,\n",
    "            min=10000,\n",
    "            max=500000,\n",
    "            step=10000,\n",
    "            description='Test Packets:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of packets for test PCAP'\n",
    "        )\n",
    "        \n",
    "        # CSV row count\n",
    "        csv_rows = widgets.IntText(\n",
    "            value=5000,\n",
    "            description='CSV Rows:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of label rows to extract'\n",
    "        )\n",
    "        \n",
    "        # Source file input\n",
    "        source_pcap = widgets.Text(\n",
    "            value=Config.PCAP_FILE if Config.PCAP_FILE else '',\n",
    "            description='Source PCAP:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # Source CSV input  \n",
    "        source_csv = widgets.Textarea(\n",
    "            value='\\n'.join(Config.LABEL_FILES) if Config.LABEL_FILES else '',\n",
    "            description='Source CSVs:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%', height='80px'),\n",
    "            placeholder='Enter CSV paths (one per line)'\n",
    "        )\n",
    "        \n",
    "        # Output directory\n",
    "        test_dir = widgets.Text(\n",
    "            value='./test_data',\n",
    "            description='Test Directory:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Progress output\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # Create button\n",
    "        create_btn = widgets.Button(\n",
    "            description='Create Test Datasets',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def create_test_data(b):\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Get values\n",
    "                pcap_path = source_pcap.value.strip()\n",
    "                csv_paths = [f.strip() for f in source_csv.value.strip().split('\\n') if f.strip()]\n",
    "                test_path = test_dir.value\n",
    "                num_packets = packet_count.value\n",
    "                num_rows = csv_rows.value\n",
    "                \n",
    "                # Validate inputs\n",
    "                if not os.path.exists(pcap_path):\n",
    "                    print(f\" Error: PCAP file not found: {pcap_path}\")\n",
    "                    return\n",
    "                \n",
    "                # Create test directory\n",
    "                os.makedirs(test_path, exist_ok=True)\n",
    "                \n",
    "                print(\"=\"*60)\n",
    "                print(\"CREATING TEST DATASETS\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Create test PCAP\n",
    "                test_pcap = os.path.join(test_path, f'test_{num_packets}_packets.pcap')\n",
    "                print(f\"\\n1. Creating test PCAP with {num_packets:,} packets...\")\n",
    "                \n",
    "                try:\n",
    "                    # Use tcpdump to extract packets\n",
    "                    cmd = f'tcpdump -r \"{pcap_path}\" -w \"{test_pcap}\" -c {num_packets}'\n",
    "                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "                    \n",
    "                    if os.path.exists(test_pcap):\n",
    "                        size_mb = os.path.getsize(test_pcap) / (1024*1024)\n",
    "                        print(f\"   ✓ Created: {test_pcap}\")\n",
    "                        print(f\"   Size: {size_mb:.2f} MB\")\n",
    "                    else:\n",
    "                        # Fallback to Python method if tcpdump fails\n",
    "                        print(\"   tcpdump failed, using Python method...\")\n",
    "                        TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   Error creating PCAP: {e}\")\n",
    "                    print(\"   Trying Python-based extraction...\")\n",
    "                    TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                \n",
    "                # Create test CSVs\n",
    "                if csv_paths:\n",
    "                    print(f\"\\n2. Creating test CSVs with {num_rows:,} rows each...\")\n",
    "                    test_csvs = []\n",
    "                    \n",
    "                    for csv_path in csv_paths:\n",
    "                        if not os.path.exists(csv_path):\n",
    "                            print(f\"   CSV not found: {csv_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Read and subset CSV\n",
    "                        csv_name = os.path.basename(csv_path)\n",
    "                        test_csv = os.path.join(test_path, f'test_{csv_name}')\n",
    "                        \n",
    "                        try:\n",
    "                            df = pd.read_csv(csv_path, encoding='latin-1', nrows=num_rows)\n",
    "                            df.to_csv(test_csv, index=False)\n",
    "                            test_csvs.append(test_csv)\n",
    "                            \n",
    "                            # Show label distribution\n",
    "                            label_col = None\n",
    "                            for col in ['Label', 'label', ' Label']:\n",
    "                                if col in df.columns:\n",
    "                                    label_col = col\n",
    "                                    break\n",
    "                            \n",
    "                            if label_col:\n",
    "                                print(f\"\\n   ✓ Created: {test_csv}\")\n",
    "                                print(f\"   Label distribution:\")\n",
    "                                for label, count in df[label_col].value_counts().head(5).items():\n",
    "                                    print(f\"      {label}: {count}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"   Error processing {csv_name}: {e}\")\n",
    "                else:\n",
    "                    test_csvs = []\n",
    "                    print(\"\\n2. No CSV files provided, skipping label extraction\")\n",
    "                \n",
    "                # Generate configuration code\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"TEST CONFIGURATION\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"\\nAdd this to your Config or use directly:\\n\")\n",
    "                print(f\"Config.PCAP_FILE = r'{test_pcap}'\")\n",
    "                print(f\"Config.LABEL_FILES = {test_csvs}\")\n",
    "                print(f\"Config.MAX_PACKETS = 0  # Process all packets in test file\")\n",
    "                print(f\"Config.SAMPLE_SIZE = {min(50000, num_packets // 2)}\")\n",
    "                \n",
    "                # Estimate time\n",
    "                est_time = num_packets / 20000  # ~20K packets per minute\n",
    "                print(f\"\\nEstimated test time: {est_time:.1f} minutes\")\n",
    "                print(\"\\n Test datasets created successfully!\")\n",
    "                print(\"   Run the pipeline with these files to validate before full analysis\")\n",
    "        \n",
    "        create_btn.on_click(create_test_data)\n",
    "        \n",
    "        # Layout\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Test Size Configuration</h4>\"),\n",
    "            packet_count,\n",
    "            csv_rows,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Source Files</h4>\"),\n",
    "            source_pcap,\n",
    "            source_csv,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Output Location</h4>\"),\n",
    "            test_dir,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            create_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_test_pcap_python(source_pcap, output_pcap, packet_count):\n",
    "        \"\"\"\n",
    "        Python fallback method to create test PCAP if tcpdump unavailable\n",
    "        \"\"\"\n",
    "        from scapy.all import PcapReader, PcapWriter\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(source_pcap) as reader:\n",
    "                with PcapWriter(output_pcap) as writer:\n",
    "                    for i, packet in enumerate(reader):\n",
    "                        if i >= packet_count:\n",
    "                            break\n",
    "                        writer.write(packet)\n",
    "            \n",
    "            if os.path.exists(output_pcap):\n",
    "                size_mb = os.path.getsize(output_pcap) / (1024*1024)\n",
    "                print(f\"   ✓ Created using Python: {output_pcap}\")\n",
    "                print(f\"   Size: {size_mb:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error with Python method: {e}\")\n",
    "\n",
    "# Run the UI\n",
    "TestDatasetCreator.create_test_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01c0ab",
   "metadata": {},
   "source": [
    "# ### Step 3:  Flow Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8900f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Flow Feature Extractor - Network Traffic Statistical Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Extract statistical features from network flows\n",
    "This class analyzes network packets and groups them into flows, then extracts\n",
    "statistical features that help identify malicious traffic patterns.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Groups packets into bidirectional flows (conversations between hosts)\n",
    "2. Extracts per-packet features (size, flags, ports, timing)\n",
    "3. Aggregates packet features into flow-level statistics\n",
    "4. Implements CICFlowMeter-style feature extraction\n",
    "5. Manages memory by flushing old flows to disk\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FLOW: A sequence of packets between two endpoints (identified by 5-tuple)\n",
    "- 5-TUPLE: (src_ip, src_port, dst_ip, dst_port, protocol)\n",
    "- BIDIRECTIONAL: Treats A→B and B→A as the same flow\n",
    "- FLOW TIMEOUT: After 120 seconds of inactivity, flow is considered complete\n",
    "\n",
    "FEATURES EXTRACTED:\n",
    "1. Timing Features:\n",
    "   - Flow duration\n",
    "   - Inter-arrival times (IAT) statistics\n",
    "   - Packets/bytes per second\n",
    "   - Active/Idle time statistics (CICIDS-specific)\n",
    "\n",
    "2. Size Features:\n",
    "   - Packet length statistics (min, max, mean, std)\n",
    "   - Total bytes/packets\n",
    "   - Payload sizes\n",
    "   - Forward/Backward packet statistics\n",
    "\n",
    "3. TCP Flag Features:\n",
    "   - SYN, ACK, FIN, RST, PSH, URG counts\n",
    "   - Used to detect scanning, flooding attacks\n",
    "\n",
    "4. Protocol Features:\n",
    "   - TTL values\n",
    "   - Port numbers\n",
    "   - Protocol type\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Flushes flows to Parquet when memory limit reached\n",
    "- Uses flow timeout to prevent infinite accumulation\n",
    "- Stores features as float32 instead of float64\n",
    "- Caps packets per flow to prevent memory bloat\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Fixed bidirectional flow identification bug\n",
    "- Using Parquet instead of HDF5 (no file locking)\n",
    "- Added CICIDS-specific features (Active/Idle, Fwd/Bwd separation)\n",
    "- Optimized entropy calculation with numpy\n",
    "- Added packet count cap per flow\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFlowExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000, flow_timeout=120):\n",
    "        \"\"\"\n",
    "        Initialize flow feature extractor with memory management.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum concurrent flows before flushing to disk\n",
    "            flow_timeout: Seconds of inactivity before flow is complete\n",
    "        \"\"\"\n",
    "        self.flows = {}  # Dictionary to store active flows\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.flow_timeout = flow_timeout\n",
    "        self.flow_counter = 0  # Total flows seen\n",
    "        self.batch_counter = 0  # Number of batches written to disk\n",
    "        \n",
    "        # Create Parquet storage directory (replacing HDF5)\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'flow_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pre-compile optimizations\n",
    "        self.setup_optimizations()\n",
    "    \n",
    "    def setup_optimizations(self):\n",
    "        \"\"\"\n",
    "        Pre-compile and optimize frequently used operations.\n",
    "        This improves performance significantly.\n",
    "        \"\"\"\n",
    "        # Pre-calculate byte frequency table for entropy\n",
    "        self.byte_frequencies = np.zeros(256, dtype=np.float32)\n",
    "        \n",
    "        # Maximum packets per flow to prevent memory issues\n",
    "        self.max_packets_per_flow = Config.MAX_PACKETS_PER_FLOW\n",
    "    \n",
    "    def get_flow_id(self, packet):\n",
    "        \"\"\"\n",
    "        Generate unique identifier for a network flow.\n",
    "        FIXED: Properly handles bidirectional flow identification.\n",
    "        \n",
    "        Uses 5-tuple (IPs, ports, protocol) to identify flows.\n",
    "        Makes flows bidirectional by sorting endpoints correctly.\n",
    "        \n",
    "        Returns:\n",
    "            str: 16-character hash identifying the flow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if IP not in packet:\n",
    "                return None\n",
    "                \n",
    "            src_ip = packet[IP].src\n",
    "            dst_ip = packet[IP].dst\n",
    "            proto = packet[IP].proto\n",
    "            \n",
    "            # Extract ports if TCP/UDP\n",
    "            src_port = dst_port = 0\n",
    "            if TCP in packet:\n",
    "                src_port = packet[TCP].sport\n",
    "                dst_port = packet[TCP].dport\n",
    "            elif UDP in packet:\n",
    "                src_port = packet[UDP].sport\n",
    "                dst_port = packet[UDP].dport\n",
    "            \n",
    "            # FIXED: Proper bidirectional flow identification\n",
    "            # Sort by IP first, then by port if IPs are equal\n",
    "            if src_ip < dst_ip:\n",
    "                flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{proto}\"\n",
    "            elif src_ip > dst_ip:\n",
    "                flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{proto}\"\n",
    "            else:  # Same IP (rare but possible)\n",
    "                if src_port <= dst_port:\n",
    "                    flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{proto}\"\n",
    "                else:\n",
    "                    flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{proto}\"\n",
    "            \n",
    "            # Create hash for efficient lookup\n",
    "            return hashlib.md5(flow_key.encode()).hexdigest()[:16]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_packet_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract features from a single packet.\n",
    "        These features will be aggregated into flow statistics.\n",
    "        \n",
    "        Features include:\n",
    "        - Basic: timestamp, packet length, direction\n",
    "        - IP: TTL, protocol, IPs (for direction determination)\n",
    "        - TCP: flags, window size, ports\n",
    "        - Payload: size and entropy (randomness)\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            # Basic packet features\n",
    "            features['timestamp'] = float(packet.time)\n",
    "            features['packet_length'] = len(packet)\n",
    "            \n",
    "            # IP layer features\n",
    "            if IP in packet:\n",
    "                features['ttl'] = packet[IP].ttl\n",
    "                features['protocol'] = packet[IP].proto\n",
    "                features['src_ip'] = packet[IP].src  # For direction determination\n",
    "                features['dst_ip'] = packet[IP].dst\n",
    "                \n",
    "            # TCP layer features\n",
    "            if TCP in packet:\n",
    "                features['tcp_flags'] = int(packet[TCP].flags)\n",
    "                features['window_size'] = packet[TCP].window\n",
    "                features['src_port'] = packet[TCP].sport\n",
    "                features['dst_port'] = packet[TCP].dport\n",
    "                \n",
    "                # Individual flag extraction (for detecting attacks)\n",
    "                tcp_flags = packet[TCP].flags\n",
    "                features['flag_syn'] = bool(tcp_flags & 2)   # SYN flag\n",
    "                features['flag_ack'] = bool(tcp_flags & 16)  # ACK flag\n",
    "                features['flag_fin'] = bool(tcp_flags & 1)   # FIN flag\n",
    "                features['flag_rst'] = bool(tcp_flags & 4)   # RST flag\n",
    "                features['flag_psh'] = bool(tcp_flags & 8)   # PSH flag\n",
    "                features['flag_urg'] = bool(tcp_flags & 32)  # URG flag (CICIDS feature)\n",
    "                \n",
    "            # UDP layer features\n",
    "            elif UDP in packet:\n",
    "                features['src_port'] = packet[UDP].sport\n",
    "                features['dst_port'] = packet[UDP].dport\n",
    "                \n",
    "            # Payload features (important for detecting malware)\n",
    "            if Raw in packet:\n",
    "                payload = bytes(packet[Raw])\n",
    "                features['payload_size'] = len(payload)\n",
    "                features['payload_entropy'] = self.fast_entropy(payload)\n",
    "            else:\n",
    "                features['payload_size'] = 0\n",
    "                features['payload_entropy'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def fast_entropy(self, data):\n",
    "        \"\"\"\n",
    "        OPTIMIZED: Calculate Shannon entropy using numpy for speed.\n",
    "        High entropy suggests encryption/compression (possibly malware).\n",
    "        Low entropy suggests plain text.\n",
    "        \n",
    "        Returns:\n",
    "            float: Entropy value (0-8 bits)\n",
    "        \"\"\"\n",
    "        if not data or len(data) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Use numpy for fast calculation\n",
    "        counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)\n",
    "        probs = counts[counts > 0] / len(data)\n",
    "        return -np.sum(probs * np.log2(probs))\n",
    "    \n",
    "    def flush_old_flows(self, current_time, force_all=False):\n",
    "        \"\"\"\n",
    "        Save completed flows to disk using Parquet format.\n",
    "        A flow is complete if it hasn't seen packets for flow_timeout seconds.\n",
    "        \n",
    "        IMPORTANT: Using Parquet instead of HDF5 for Windows reliability.\n",
    "        \n",
    "        Args:\n",
    "            current_time: Timestamp of current packet\n",
    "            force_all: Force flush all flows regardless of timeout\n",
    "        \"\"\"\n",
    "        flows_to_flush = []\n",
    "        \n",
    "        # Identify flows that have timed out\n",
    "        for flow_id, flow_data in self.flows.items():\n",
    "            packets = flow_data['packets']\n",
    "            if not packets:\n",
    "                continue\n",
    "                \n",
    "            last_packet_time = packets[-1]['timestamp']\n",
    "            time_since_last = current_time - last_packet_time\n",
    "            \n",
    "            if force_all or time_since_last > self.flow_timeout:\n",
    "                # Aggregate packet features into flow features\n",
    "                features = self.aggregate_flow_features(flow_data)\n",
    "                if features:\n",
    "                    features['flow_id'] = flow_id\n",
    "                    flows_to_flush.append(features)\n",
    "        \n",
    "        # Save to disk if we have flows to flush\n",
    "        if flows_to_flush:\n",
    "            # Convert to DataFrame\n",
    "            df_batch = pd.DataFrame(flows_to_flush)\n",
    "            \n",
    "            # Optimize memory usage with proper dtypes\n",
    "            df_batch = self.optimize_dtypes(df_batch)\n",
    "            \n",
    "            # Save to Parquet (no file locking issues!)\n",
    "            batch_file = os.path.join(\n",
    "                self.output_dir, \n",
    "                f'batch_{self.batch_counter:04d}.parquet'\n",
    "            )\n",
    "            df_batch.to_parquet(\n",
    "                batch_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            self.batch_counter += 1\n",
    "            \n",
    "            # Remove flushed flows from memory\n",
    "            for features in flows_to_flush:\n",
    "                del self.flows[features['flow_id']]\n",
    "            \n",
    "            print(f\"  Flushed {len(flows_to_flush):,} flows to {batch_file}\")\n",
    "            \n",
    "            # Force garbage collection to free memory\n",
    "            gc.collect()\n",
    "    \n",
    "    def optimize_dtypes(self, df):\n",
    "        \"\"\"\n",
    "        Optimize DataFrame dtypes for memory efficiency and storage.\n",
    "        Specifically optimized for CICIDS features.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            \n",
    "            # Flag counts are small integers\n",
    "            if 'flag' in col_lower or '_count' in col_lower:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            # Ports fit in int16\n",
    "            elif 'port' in col_lower:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            # Packet/byte counts\n",
    "            elif 'packet' in col_lower or 'byte' in col_lower:\n",
    "                if 'rate' not in col_lower and '/s' not in col_lower:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "            # Float features\n",
    "            elif df[col].dtype == np.float64:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def aggregate_flow_features(self, flow_data):\n",
    "        \"\"\"\n",
    "        Aggregate packet-level features into flow-level statistics.\n",
    "        This is where we calculate the actual features used for ML.\n",
    "        \n",
    "        ENHANCED: Now includes CICIDS-specific features like Active/Idle times\n",
    "        and Forward/Backward packet separation.\n",
    "        \n",
    "        Creates statistical summaries that capture flow behavior:\n",
    "        - Duration and timing patterns\n",
    "        - Size distributions\n",
    "        - Flag patterns\n",
    "        - Rate calculations\n",
    "        - Active/Idle statistics (CICIDS)\n",
    "        - Forward/Backward statistics (CICIDS)\n",
    "        \"\"\"\n",
    "        packets = flow_data['packets']\n",
    "        if not packets:\n",
    "            return None\n",
    "            \n",
    "        # Sort packets by timestamp\n",
    "        packets = sorted(packets, key=lambda x: x.get('timestamp', 0))\n",
    "        \n",
    "        # Determine flow direction based on first packet\n",
    "        first_packet = packets[0]\n",
    "        flow_src_ip = first_packet.get('src_ip', '')\n",
    "        \n",
    "        # Separate forward and backward packets (CICIDS feature)\n",
    "        fwd_packets = [p for p in packets if p.get('src_ip') == flow_src_ip]\n",
    "        bwd_packets = [p for p in packets if p.get('src_ip') != flow_src_ip]\n",
    "        \n",
    "        # Extract timestamp array\n",
    "        timestamps = [p['timestamp'] for p in packets]\n",
    "        duration = max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0\n",
    "        \n",
    "        # Calculate inter-arrival times (time between packets)\n",
    "        iats = np.diff(timestamps) if len(timestamps) > 1 else [0]\n",
    "        \n",
    "        # Active/Idle time calculation (CICIDS-specific)\n",
    "        active_times = []\n",
    "        idle_times = []\n",
    "        if len(iats) > 0:\n",
    "            active_times = [t for t in iats if t < 1.0]  # Active if IAT < 1 second\n",
    "            idle_times = [t for t in iats if t >= 1.0]    # Idle if IAT >= 1 second\n",
    "        \n",
    "        # Build feature dictionary\n",
    "        features = {\n",
    "            # ===== BASIC FLOW STATISTICS =====\n",
    "            'flow_duration': duration,\n",
    "            'total_packets': len(packets),\n",
    "            'total_bytes': sum(p.get('packet_length', 0) for p in packets),\n",
    "            \n",
    "            # ===== FORWARD DIRECTION STATISTICS (CICIDS) =====\n",
    "            'fwd_packets': len(fwd_packets),\n",
    "            'fwd_bytes': sum(p.get('packet_length', 0) for p in fwd_packets),\n",
    "            'fwd_packet_length_max': max([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_min': min([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_mean': np.mean([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            'fwd_packet_length_std': np.std([p.get('packet_length', 0) for p in fwd_packets]) if fwd_packets else 0,\n",
    "            \n",
    "            # ===== BACKWARD DIRECTION STATISTICS (CICIDS) =====\n",
    "            'bwd_packets': len(bwd_packets),\n",
    "            'bwd_bytes': sum(p.get('packet_length', 0) for p in bwd_packets),\n",
    "            'bwd_packet_length_max': max([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_min': min([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_mean': np.mean([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            'bwd_packet_length_std': np.std([p.get('packet_length', 0) for p in bwd_packets]) if bwd_packets else 0,\n",
    "            \n",
    "            # ===== INTER-ARRIVAL TIME STATISTICS =====\n",
    "            'flow_iat_mean': np.mean(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_std': np.std(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_max': max(iats) if len(iats) > 0 else 0,\n",
    "            'flow_iat_min': min(iats) if len(iats) > 0 else 0,\n",
    "            \n",
    "            # ===== ACTIVE/IDLE TIME STATISTICS (CICIDS) =====\n",
    "            'active_mean': np.mean(active_times) if active_times else 0,\n",
    "            'active_std': np.std(active_times) if active_times else 0,\n",
    "            'active_max': max(active_times) if active_times else 0,\n",
    "            'active_min': min(active_times) if active_times else 0,\n",
    "            'idle_mean': np.mean(idle_times) if idle_times else 0,\n",
    "            'idle_std': np.std(idle_times) if idle_times else 0,\n",
    "            'idle_max': max(idle_times) if idle_times else 0,\n",
    "            'idle_min': min(idle_times) if idle_times else 0,\n",
    "            \n",
    "            # ===== FLOW RATE FEATURES =====\n",
    "            'packets_per_second': len(packets) / duration if duration > 0 else 0,\n",
    "            'bytes_per_second': sum(p.get('packet_length', 0) for p in packets) / duration if duration > 0 else 0,\n",
    "            \n",
    "            # ===== PROTOCOL FEATURES =====\n",
    "            'avg_ttl': np.mean([p.get('ttl', 0) for p in packets]),\n",
    "            'protocol': packets[0].get('protocol', 0) if packets else 0,\n",
    "            \n",
    "            # ===== TCP FLAG STATISTICS (Enhanced for CICIDS) =====\n",
    "            'syn_count': sum(p.get('flag_syn', 0) for p in packets),\n",
    "            'ack_count': sum(p.get('flag_ack', 0) for p in packets),\n",
    "            'fin_count': sum(p.get('flag_fin', 0) for p in packets),\n",
    "            'rst_count': sum(p.get('flag_rst', 0) for p in packets),\n",
    "            'psh_count': sum(p.get('flag_psh', 0) for p in packets),\n",
    "            'urg_count': sum(p.get('flag_urg', 0) for p in packets),  # Added URG flag\n",
    "            \n",
    "            # Forward/Backward flag counts (CICIDS)\n",
    "            'fwd_psh_flags': sum(p.get('flag_psh', 0) for p in fwd_packets),\n",
    "            'bwd_psh_flags': sum(p.get('flag_psh', 0) for p in bwd_packets),\n",
    "            'fwd_urg_flags': sum(p.get('flag_urg', 0) for p in fwd_packets),\n",
    "            'bwd_urg_flags': sum(p.get('flag_urg', 0) for p in bwd_packets),\n",
    "            \n",
    "            # ===== PAYLOAD STATISTICS =====\n",
    "            'total_payload_bytes': sum(p.get('payload_size', 0) for p in packets),\n",
    "            'avg_payload_size': np.mean([p.get('payload_size', 0) for p in packets]),\n",
    "            'avg_entropy': np.mean([p.get('payload_entropy', 0) for p in packets]),\n",
    "            \n",
    "            # ===== PORT INFORMATION =====\n",
    "            'src_port': packets[0].get('src_port', 0) if packets else 0,\n",
    "            'dst_port': packets[0].get('dst_port', 0) if packets else 0,\n",
    "        }\n",
    "        \n",
    "        # Convert to float32 for memory efficiency\n",
    "        for key in features:\n",
    "            if isinstance(features[key], (int, float)):\n",
    "                features[key] = np.float32(features[key])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Main processing function - reads PCAP and extracts flow features.\n",
    "        Implements streaming processing to handle files larger than RAM.\n",
    "        \n",
    "        ENHANCED: Better memory management with packet count limits per flow.\n",
    "        \n",
    "        Processing steps:\n",
    "        1. Read packets one by one\n",
    "        2. Group into flows (with packet limit per flow)\n",
    "        3. Extract features\n",
    "        4. Flush to Parquet when memory limit reached\n",
    "        5. Handle timeouts for inactive flows\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory containing Parquet files\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing PCAP with memory optimization: {pcap_file}\")\n",
    "        print(f\"Max flows in memory: {self.max_flows_in_memory:,}\")\n",
    "        print(f\"Max packets per flow: {self.max_packets_per_flow:,}\")\n",
    "        print(f\"Flow timeout: {self.flow_timeout} seconds\")\n",
    "        print(f\"Output format: Parquet (Windows-optimized)\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        last_flush_time = None\n",
    "        \n",
    "        try:\n",
    "            # Open PCAP file for streaming read\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                # Process packets one by one\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting flow features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    # Check packet limit\n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow identifier\n",
    "                    flow_id = self.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract packet features\n",
    "                    packet_features = self.extract_packet_features(packet)\n",
    "                    current_time = packet_features.get('timestamp', 0)\n",
    "                    \n",
    "                    # Initialize flow if new\n",
    "                    if flow_id not in self.flows:\n",
    "                        self.flows[flow_id] = {\n",
    "                            'packets': [],\n",
    "                            'stats': defaultdict(float)  # Running statistics\n",
    "                        }\n",
    "                        self.flow_counter += 1\n",
    "                    \n",
    "                    # Add packet to flow (with limit to prevent memory bloat)\n",
    "                    if len(self.flows[flow_id]['packets']) < self.max_packets_per_flow:\n",
    "                        self.flows[flow_id]['packets'].append(packet_features)\n",
    "                    else:\n",
    "                        # Update statistics without storing packet\n",
    "                        self.update_flow_stats(self.flows[flow_id]['stats'], packet_features)\n",
    "                    \n",
    "                    # Check if memory limit reached\n",
    "                    if len(self.flows) >= self.max_flows_in_memory:\n",
    "                        print(f\"\\n  Memory limit reached at packet {packet_count:,}\")\n",
    "                        self.flush_old_flows(current_time)\n",
    "                    \n",
    "                    # Periodic timeout check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        # Check for timed-out flows\n",
    "                        if last_flush_time and (current_time - last_flush_time) > self.flow_timeout:\n",
    "                            self.flush_old_flows(current_time)\n",
    "                            last_flush_time = current_time\n",
    "                        \n",
    "                        # Monitor system memory\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory usage ({mem_percent:.1f}%), flushing flows...\")\n",
    "                            self.flush_old_flows(current_time, force_all=True)\n",
    "                        \n",
    "                        gc.collect()\n",
    "            \n",
    "            # Flush all remaining flows\n",
    "            print(\"\\nFlushing remaining flows...\")\n",
    "            self.flush_old_flows(float('inf'), force_all=True)\n",
    "            \n",
    "            print(f\"\\nProcessed {packet_count:,} packets\")\n",
    "            print(f\"Total flows: {self.flow_counter:,}\")\n",
    "            print(f\"Features saved to: {self.output_dir}\")\n",
    "            \n",
    "            return self.output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PCAP: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def update_flow_stats(self, stats, packet_features):\n",
    "        \"\"\"\n",
    "        Update running statistics for a flow when packet limit is exceeded.\n",
    "        This allows us to track flow statistics without storing all packets.\n",
    "        \"\"\"\n",
    "        stats['packet_count'] += 1\n",
    "        stats['byte_count'] += packet_features.get('packet_length', 0)\n",
    "        stats['payload_total'] += packet_features.get('payload_size', 0)\n",
    "        \n",
    "        # Update flag counts\n",
    "        for flag in ['syn', 'ack', 'fin', 'rst', 'psh', 'urg']:\n",
    "            if packet_features.get(f'flag_{flag}', False):\n",
    "                stats[f'{flag}_count'] += 1\n",
    "    \n",
    "    def load_features_iterator(self, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Generator to load features in batches from Parquet files.\n",
    "        Allows processing results without loading all data into memory.\n",
    "        \n",
    "        Yields:\n",
    "            DataFrame: Batch of flow features\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        parquet_files = sorted(glob.glob(os.path.join(self.output_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        accumulated = []\n",
    "        accumulated_size = 0\n",
    "        \n",
    "        for file in parquet_files:\n",
    "            batch_df = pd.read_parquet(file)\n",
    "            accumulated.append(batch_df)\n",
    "            accumulated_size += len(batch_df)\n",
    "            \n",
    "            if accumulated_size >= batch_size:\n",
    "                combined_df = pd.concat(accumulated, ignore_index=True)\n",
    "                yield combined_df\n",
    "                accumulated = []\n",
    "                accumulated_size = 0\n",
    "        \n",
    "        # Yield remaining data\n",
    "        if accumulated:\n",
    "            yield pd.concat(accumulated, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaa6bc",
   "metadata": {},
   "source": [
    "# ### Step4: Semantic Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Semantic Feature Extractor - Deep Packet Inspection & NLP Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Analyze packet payloads for malicious content using pattern matching and NLP\n",
    "This class performs deep packet inspection to detect attack signatures in payload content.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Examines packet payloads (actual data being transmitted)\n",
    "2. Detects attack patterns (SQL injection, XSS, command injection)\n",
    "3. Performs NLP analysis to find obfuscated attacks\n",
    "4. Calculates entropy and encoding detection\n",
    "5. Identifies suspicious URLs and domain names\n",
    "\n",
    "KEY DETECTION CAPABILITIES:\n",
    "1. SQL Injection: SELECT, UNION, DROP TABLE patterns\n",
    "2. Cross-Site Scripting (XSS): <script>, javascript:, alert()\n",
    "3. Command Injection: bash commands, system calls\n",
    "4. Directory Traversal: ../, /etc/passwd\n",
    "5. Encoding Detection: Base64, hex, URL encoding\n",
    "6. Obfuscation: Unusual character patterns, high entropy\n",
    "\n",
    "WHY SEMANTIC ANALYSIS MATTERS:\n",
    "- Flow features only see traffic patterns, not content\n",
    "- Many attacks hide in seemingly normal traffic\n",
    "- Attackers use encoding/obfuscation to evade detection\n",
    "- NLP helps detect variations of known attacks\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Processes payloads in streaming fashion\n",
    "- Limits payload analysis to first 1000 characters\n",
    "- Flushes results to Parquet periodically\n",
    "- Uses simplified NLP for speed\n",
    "- Pre-compiles regex patterns\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Pre-compiled regex patterns for better performance\n",
    "- Optimized entropy calculation with numpy\n",
    "- Added more comprehensive attack patterns\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedSemanticExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000):\n",
    "        \"\"\"\n",
    "        Initialize semantic analyzer with pattern databases and NLP components.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum flows before flushing to disk\n",
    "        \"\"\"\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.semantic_data = {}  # Stores semantic features per flow\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "        # Create Parquet storage directory\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'semantic_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Pre-compile all regex patterns for performance\n",
    "        self.compile_attack_patterns()\n",
    "        \n",
    "        # Initialize NLP analyzer (simplified version for memory efficiency)\n",
    "        self.nlp_analyzer = SimplifiedNLPAnalyzer()\n",
    "    \n",
    "    def compile_attack_patterns(self):\n",
    "        \"\"\"\n",
    "        Pre-compile regex patterns for better performance.\n",
    "        Compiled patterns are much faster than compiling on each use.\n",
    "        \"\"\"\n",
    "        # ===== SQL INJECTION PATTERNS =====\n",
    "        sql_patterns = [\n",
    "            r'SELECT.*FROM',      # Basic SELECT query\n",
    "            r'INSERT.*INTO',      # INSERT injection\n",
    "            r'UPDATE.*SET',       # UPDATE injection\n",
    "            r'DELETE.*FROM',      # DELETE injection\n",
    "            r'DROP.*TABLE',       # Table dropping\n",
    "            r'UNION.*SELECT',     # UNION-based injection\n",
    "            r'OR\\s+1\\s*=\\s*1',   # Classic bypass: OR 1=1\n",
    "            r'--\\s*$',           # SQL comment injection\n",
    "            r';\\s*EXEC',         # Command execution\n",
    "            r'xp_cmdshell',      # SQL Server command execution\n",
    "            r'WAITFOR\\s+DELAY',  # Time-based blind SQL\n",
    "            r'BENCHMARK\\s*\\(',   # MySQL benchmark attack\n",
    "        ]\n",
    "        self.sql_patterns = [re.compile(p, re.IGNORECASE) for p in sql_patterns]\n",
    "        \n",
    "        # ===== COMMAND INJECTION PATTERNS =====\n",
    "        cmd_patterns = [\n",
    "            r';\\s*ls\\s+',        # List directory (Linux)\n",
    "            r';\\s*cat\\s+',       # Read file (Linux)\n",
    "            r';\\s*wget\\s+',      # Download file\n",
    "            r';\\s*curl\\s+',      # HTTP request tool\n",
    "            r';\\s*nc\\s+',        # Netcat (backdoor tool)\n",
    "            r'/etc/passwd',      # Common target file\n",
    "            r'/etc/shadow',      # Password hashes\n",
    "            r'cmd\\.exe',         # Windows command prompt\n",
    "            r'powershell',       # Windows PowerShell\n",
    "            r'bash\\s+-c',        # Bash command execution\n",
    "            r'sh\\s+-c',          # Shell command execution\n",
    "            r'eval\\s*\\(',        # Code evaluation\n",
    "            r'exec\\s*\\(',        # Code execution\n",
    "            r'system\\s*\\(',      # System call\n",
    "        ]\n",
    "        self.cmd_patterns = [re.compile(p, re.IGNORECASE) for p in cmd_patterns]\n",
    "        \n",
    "        # ===== XSS/SCRIPT INJECTION PATTERNS =====\n",
    "        script_patterns = [\n",
    "            r'<script',          # Script tag injection\n",
    "            r'javascript:',      # JavaScript protocol\n",
    "            r'onerror\\s*=',     # Event handler injection\n",
    "            r'onclick\\s*=',     # Click event injection\n",
    "            r'onload\\s*=',      # Load event injection\n",
    "            r'alert\\s*\\(',      # JavaScript alert\n",
    "            r'document\\.cookie', # Cookie theft\n",
    "            r'document\\.location', # Redirection\n",
    "            r'eval\\s*\\(',       # Code evaluation\n",
    "            r'<iframe',         # IFrame injection\n",
    "            r'<embed',          # Embed tag injection\n",
    "            r'<object',         # Object tag injection\n",
    "            r'<img.*src.*=',    # Image injection\n",
    "        ]\n",
    "        self.script_patterns = [re.compile(p, re.IGNORECASE) for p in script_patterns]\n",
    "        \n",
    "        # ===== DIRECTORY TRAVERSAL PATTERNS =====\n",
    "        traversal_patterns = [\n",
    "            r'\\.\\./\\.\\./\\.\\./', # Multiple traversals\n",
    "            r'\\.\\.\\\\\\.\\.\\\\\\.\\.\\\\'  # Windows traversals\n",
    "        ]\n",
    "        self.traversal_patterns = [re.compile(p) for p in traversal_patterns]\n",
    "    \n",
    "    def flush_semantic_features(self):\n",
    "        \"\"\"\n",
    "        Save accumulated semantic features to Parquet and free memory.\n",
    "        Called when memory limit is reached.\n",
    "        \n",
    "        Using Parquet instead of HDF5 for Windows reliability.\n",
    "        \"\"\"\n",
    "        if not self.semantic_data:\n",
    "            return\n",
    "        \n",
    "        # Convert dictionary to DataFrame\n",
    "        df_batch = pd.DataFrame.from_dict(self.semantic_data, orient='index')\n",
    "        df_batch['flow_id'] = df_batch.index\n",
    "        df_batch = df_batch.reset_index(drop=True)\n",
    "        \n",
    "        # Optimize data types\n",
    "        for col in df_batch.select_dtypes(include=[np.float64]).columns:\n",
    "            df_batch[col] = df_batch[col].astype(np.float32)\n",
    "        \n",
    "        # Save to Parquet\n",
    "        batch_file = os.path.join(\n",
    "            self.output_dir,\n",
    "            f'batch_{self.batch_counter:04d}.parquet'\n",
    "        )\n",
    "        df_batch.to_parquet(\n",
    "            batch_file,\n",
    "            engine='pyarrow',\n",
    "            compression='snappy',\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        self.batch_counter += 1\n",
    "        print(f\"  Flushed {len(df_batch):,} semantic features to {batch_file}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        self.semantic_data.clear()\n",
    "        gc.collect()\n",
    "    \n",
    "    def extract_semantic_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract semantic features from packet payload.\n",
    "        Analyzes actual data content for attack signatures.\n",
    "        \n",
    "        OPTIMIZED: Uses pre-compiled patterns and limits payload analysis.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Semantic feature values\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            # ===== PROTOCOL INDICATORS =====\n",
    "            'has_http': 0,       # HTTP traffic\n",
    "            'has_dns': 0,        # DNS queries\n",
    "            'has_smtp': 0,       # Email traffic\n",
    "            'has_tls': 0,        # TLS/SSL traffic\n",
    "            \n",
    "            # ===== ATTACK INDICATORS =====\n",
    "            'has_sql': 0,        # SQL injection detected\n",
    "            'has_cmd': 0,        # Command injection detected\n",
    "            'has_script': 0,     # Script injection detected\n",
    "            'has_traversal': 0,  # Directory traversal detected\n",
    "            \n",
    "            # ===== SCORING =====\n",
    "            'suspicious_score': 0,  # Overall suspicion level\n",
    "            'content_length': 0,    # Payload size\n",
    "            'attack_pattern_count': 0,  # Total patterns matched\n",
    "            \n",
    "            # ===== NLP FEATURES =====\n",
    "            'nlp_malicious_confidence': 0,  # NLP-based threat score\n",
    "            'nlp_pattern_score': 0,         # Pattern matching score\n",
    "            'nlp_entropy_score': 0,         # Randomness score\n",
    "            'nlp_encoding_detected': 0      # Encoding/obfuscation detected\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check for HTTP\n",
    "            if packet.haslayer('HTTP'):\n",
    "                features['has_http'] = 1\n",
    "                \n",
    "                # Extract HTTP-specific features\n",
    "                if hasattr(packet['HTTP'], 'Method'):\n",
    "                    # POST/PUT methods often carry attack payloads\n",
    "                    method = packet['HTTP'].Method\n",
    "                    if method in [b'POST', b'PUT', b'PATCH']:\n",
    "                        features['suspicious_score'] += 1\n",
    "                    \n",
    "                    # Check for suspicious headers\n",
    "                    if hasattr(packet['HTTP'], 'Host'):\n",
    "                        host = str(packet['HTTP'].Host)\n",
    "                        # Check for IP addresses in Host header (suspicious)\n",
    "                        if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', host):\n",
    "                            features['suspicious_score'] += 2\n",
    "            \n",
    "            # Check for DNS\n",
    "            if packet.haslayer('DNS'):\n",
    "                features['has_dns'] = 1\n",
    "                # DNS tunneling detection\n",
    "                if hasattr(packet['DNS'], 'qd') and packet['DNS'].qd:\n",
    "                    query_name = str(packet['DNS'].qd.qname)\n",
    "                    # Long DNS names might indicate tunneling\n",
    "                    if len(query_name) > 50:\n",
    "                        features['suspicious_score'] += 2\n",
    "            \n",
    "            # Check for TLS/SSL\n",
    "            if packet.haslayer('TLS'):\n",
    "                features['has_tls'] = 1\n",
    "            \n",
    "            # Extract and analyze payload\n",
    "            if packet.haslayer('Raw'):\n",
    "                payload = str(packet['Raw'].load)\n",
    "                features['content_length'] = len(payload)\n",
    "                \n",
    "                # Limit payload analysis for performance (first 1000 chars)\n",
    "                payload_sample = payload[:1000]\n",
    "                \n",
    "                # ===== PATTERN DETECTION WITH PRE-COMPILED PATTERNS =====\n",
    "                # Check for SQL injection (check top 5 patterns for speed)\n",
    "                for pattern in self.sql_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_sql'] = 1\n",
    "                        features['suspicious_score'] += 3\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for command injection\n",
    "                for pattern in self.cmd_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_cmd'] = 1\n",
    "                        features['suspicious_score'] += 5  # Higher score for OS commands\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for script injection\n",
    "                for pattern in self.script_patterns[:5]:\n",
    "                    if pattern.search(payload_sample):\n",
    "                        features['has_script'] = 1\n",
    "                        features['suspicious_score'] += 2\n",
    "                        features['attack_pattern_count'] += 1\n",
    "                        break\n",
    "                \n",
    "                # Check for directory traversal\n",
    "                if '../' in payload or '..\\\\' in payload:\n",
    "                    features['has_traversal'] = 1\n",
    "                    features['suspicious_score'] += 2\n",
    "                    features['attack_pattern_count'] += 1\n",
    "                \n",
    "                # ===== NLP ANALYSIS =====\n",
    "                # Perform deeper analysis if enabled\n",
    "                if Config.DEEP_INSPECTION and len(payload) > 10:\n",
    "                    nlp_features = self.nlp_analyzer.quick_analyze(payload_sample)\n",
    "                    features.update(nlp_features)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap_streaming(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Process PCAP file for semantic analysis with streaming.\n",
    "        Analyzes packet payloads for malicious content.\n",
    "        \n",
    "        Uses Parquet for storage instead of HDF5.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory with Parquet files containing semantic features\n",
    "        \"\"\"\n",
    "        print(f\"\\nSemantic analysis (memory-optimized): {pcap_file}\")\n",
    "        print(f\"Output format: Parquet (Windows-optimized)\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        flow_extractor = MemoryOptimizedFlowExtractor()  # Reuse flow ID logic\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting semantic features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow ID to group semantic features\n",
    "                    flow_id = flow_extractor.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract semantic features from payload\n",
    "                    features = self.extract_semantic_features(packet)\n",
    "                    \n",
    "                    # Aggregate features by flow\n",
    "                    if flow_id not in self.semantic_data:\n",
    "                        self.semantic_data[flow_id] = defaultdict(float)\n",
    "                    \n",
    "                    # Sum up features for the flow\n",
    "                    for key, value in features.items():\n",
    "                        self.semantic_data[flow_id][key] += value\n",
    "                    \n",
    "                    # Check memory limit\n",
    "                    if len(self.semantic_data) >= self.max_flows_in_memory:\n",
    "                        self.flush_semantic_features()\n",
    "                    \n",
    "                    # Periodic memory check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory ({mem_percent:.1f}%), flushing...\")\n",
    "                            self.flush_semantic_features()\n",
    "                        gc.collect()\n",
    "            \n",
    "            # Final flush\n",
    "            self.flush_semantic_features()\n",
    "            \n",
    "            print(f\"Processed {packet_count:,} packets\")\n",
    "            print(f\"Semantic features saved to: {self.output_dir}\")\n",
    "            \n",
    "            return self.output_dir\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic processing: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class SimplifiedNLPAnalyzer:\n",
    "    \"\"\"\n",
    "    Lightweight NLP analyzer for payload inspection.\n",
    "    Optimized for speed and memory efficiency.\n",
    "    \n",
    "    DETECTION METHODS:\n",
    "    1. Keyword density analysis\n",
    "    2. Entropy calculation (randomness)\n",
    "    3. Encoding detection\n",
    "    4. Character distribution analysis\n",
    "    \n",
    "    OPTIMIZATIONS:\n",
    "    - Pre-compiled patterns\n",
    "    - Numpy-based calculations\n",
    "    - Limited analysis scope\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reduced keyword sets for memory efficiency\n",
    "        self.sql_keywords = {'select', 'union', 'insert', 'drop', 'exec', 'declare', 'cast', 'convert', 'table', 'database'}\n",
    "        self.xss_keywords = {'script', 'javascript', 'alert', 'onerror', 'onclick', 'document', 'cookie', 'eval', 'iframe'}\n",
    "        self.cmd_keywords = {'bash', 'cmd', 'wget', 'curl', 'nc', 'telnet', 'ssh', 'powershell', 'exec', 'system'}\n",
    "        \n",
    "        # Pre-compiled encoding patterns for performance\n",
    "        self.encoding_patterns = {\n",
    "            'base64': re.compile(r'^[A-Za-z0-9+/]+=*$'),\n",
    "            'hex': re.compile(r'^[0-9A-Fa-f]+$'),\n",
    "            'url': re.compile(r'%[0-9A-Fa-f]{2}')\n",
    "        }\n",
    "    \n",
    "    def quick_analyze(self, payload):\n",
    "        \"\"\"\n",
    "        Perform quick NLP analysis on payload.\n",
    "        Focuses on key indicators of malicious content.\n",
    "        \n",
    "        OPTIMIZED: Uses set operations and numpy for speed.\n",
    "        \n",
    "        Returns:\n",
    "            dict: NLP feature scores\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Limit analysis to first 1000 characters for speed\n",
    "        payload_sample = payload[:1000].lower()\n",
    "        \n",
    "        # ===== KEYWORD ANALYSIS (Optimized with sets) =====\n",
    "        # Tokenize efficiently\n",
    "        words = set(re.findall(r'\\b\\w+\\b', payload_sample))\n",
    "        \n",
    "        # Count suspicious keywords using set intersection\n",
    "        sql_score = len(words & self.sql_keywords)\n",
    "        xss_score = len(words & self.xss_keywords)\n",
    "        cmd_score = len(words & self.cmd_keywords)\n",
    "        \n",
    "        # Normalize scores (0-1 range)\n",
    "        features['nlp_pattern_score'] = min((sql_score + xss_score + cmd_score) / 10, 1.0)\n",
    "        \n",
    "        # ===== ENTROPY ANALYSIS (Using numpy) =====\n",
    "        # High entropy suggests encryption/obfuscation\n",
    "        if len(payload) > 0:\n",
    "            # Fast entropy calculation\n",
    "            byte_counts = np.bincount(np.frombuffer(payload[:100].encode('utf-8', errors='ignore'), dtype=np.uint8), minlength=256)\n",
    "            byte_probs = byte_counts[byte_counts > 0] / min(len(payload), 100)\n",
    "            features['nlp_entropy_score'] = -np.sum(byte_probs * np.log2(byte_probs)) / 8  # Normalize to 0-1\n",
    "        else:\n",
    "            features['nlp_entropy_score'] = 0\n",
    "        \n",
    "        # ===== ENCODING DETECTION =====\n",
    "        # Check for common encoding schemes\n",
    "        encoding_detected = 0\n",
    "        sample = payload_sample[:50]\n",
    "        \n",
    "        for pattern_name, pattern in self.encoding_patterns.items():\n",
    "            if pattern.search(sample):\n",
    "                encoding_detected = 1\n",
    "                break\n",
    "        \n",
    "        features['nlp_encoding_detected'] = encoding_detected\n",
    "        \n",
    "        # ===== CHARACTER DISTRIBUTION ANALYSIS =====\n",
    "        # Suspicious if too many special characters\n",
    "        if len(payload_sample) > 0:\n",
    "            special_chars = sum(1 for c in payload_sample if not c.isalnum() and not c.isspace())\n",
    "            special_ratio = special_chars / len(payload_sample)\n",
    "            features['nlp_special_char_ratio'] = min(special_ratio, 1.0)\n",
    "        else:\n",
    "            features['nlp_special_char_ratio'] = 0\n",
    "        \n",
    "        # ===== OVERALL MALICIOUS CONFIDENCE =====\n",
    "        # Weighted combination of all indicators\n",
    "        features['nlp_malicious_confidence'] = min(\n",
    "            features['nlp_pattern_score'] * 0.4 +      # Pattern matching weight\n",
    "            features['nlp_entropy_score'] * 0.2 +      # Entropy weight\n",
    "            features['nlp_encoding_detected'] * 0.2 +  # Encoding weight\n",
    "            features['nlp_special_char_ratio'] * 0.2,  # Special char weight\n",
    "            1.0\n",
    "        )\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067b9ac",
   "metadata": {},
   "source": [
    "# ### Step 5: Combined Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Combined Feature Pipeline - Merging Flow & Semantic Features\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate feature extraction and merge different feature types\n",
    "This class combines flow statistics with semantic analysis to create a comprehensive\n",
    "feature set for machine learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Coordinates flow and semantic feature extraction\n",
    "2. Merges features from multiple sources using flow_id\n",
    "3. Performs feature engineering (creates new features from existing ones)\n",
    "4. Handles the entire extraction pipeline end-to-end\n",
    "5. Manages disk-based merging for large datasets\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FEATURE FUSION: Combining statistical and content-based features\n",
    "- FEATURE ENGINEERING: Creating derived features that better capture patterns\n",
    "- DISK-BASED MERGE: Joining large datasets without loading into memory\n",
    "\n",
    "FEATURE TYPES COMBINED:\n",
    "1. Flow Features (from Cell 3):\n",
    "   - Timing statistics\n",
    "   - Packet sizes\n",
    "   - TCP flags\n",
    "   - Flow rates\n",
    "   - Forward/Backward statistics (CICIDS)\n",
    "\n",
    "2. Semantic Features (from Cell 4):\n",
    "   - Attack pattern detection\n",
    "   - NLP analysis scores\n",
    "   - Entropy measurements\n",
    "   - Protocol indicators\n",
    "\n",
    "3. Engineered Features (created here):\n",
    "   - Packet rate (packets/duration)\n",
    "   - Average packet size\n",
    "   - Port categories (well-known, registered)\n",
    "   - Flag ratios (flags/total packets)\n",
    "\n",
    "WHY COMBINE FEATURES:\n",
    "- Flow features detect behavioral anomalies\n",
    "- Semantic features detect content anomalies\n",
    "- Combined view provides better attack detection\n",
    "- Some attacks only visible through combination\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Improved merging strategy for Windows\n",
    "- Added CICIDS-specific engineered features\n",
    "- Parallel processing support for feature engineering\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeaturePipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the combined feature pipeline with both extractors.\n",
    "        \"\"\"\n",
    "        # Initialize component extractors\n",
    "        self.flow_extractor = MemoryOptimizedFlowExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY,\n",
    "            flow_timeout=Config.FLOW_TIMEOUT\n",
    "        )\n",
    "        self.semantic_extractor = MemoryOptimizedSemanticExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY\n",
    "        )\n",
    "        \n",
    "        # Output directory for combined features\n",
    "        self.output_dir = os.path.join(Config.TEMP_DIR, 'combined_features')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def extract_all_features(self, pcap_file, mode='combined'):\n",
    "        \"\"\"\n",
    "        Main orchestration function - manages entire feature extraction process.\n",
    "        \n",
    "        Args:\n",
    "            pcap_file: Path to PCAP file\n",
    "            mode: 'flow' (statistics only), 'semantic' (content only), or 'combined' (both)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to directory containing all features\n",
    "        \"\"\"\n",
    "        feature_dirs = []  # List to track generated feature directories\n",
    "        \n",
    "        # ===== PHASE 1: FLOW FEATURE EXTRACTION =====\n",
    "        if mode in ['flow', 'combined']:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 1: MEMORY-OPTIMIZED FLOW EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Extracting statistical features from network flows...\")\n",
    "            print(\"This analyzes packet timing, sizes, and patterns\")\n",
    "            print(\"Including CICIDS-specific features (Active/Idle, Fwd/Bwd)\")\n",
    "            \n",
    "            flow_dir = self.flow_extractor.process_pcap(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            feature_dirs.append(('flow', flow_dir))\n",
    "            print(f\"✓ Flow features extracted to: {flow_dir}\")\n",
    "        \n",
    "        # ===== PHASE 2: SEMANTIC FEATURE EXTRACTION =====\n",
    "        if mode in ['semantic', 'combined'] and Config.DEEP_INSPECTION:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 2: MEMORY-OPTIMIZED SEMANTIC EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Analyzing packet payloads for malicious content...\")\n",
    "            print(\"This performs deep packet inspection and NLP analysis\")\n",
    "            \n",
    "            semantic_dir = self.semantic_extractor.process_pcap_streaming(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            feature_dirs.append(('semantic', semantic_dir))\n",
    "            print(f\"✓ Semantic features extracted to: {semantic_dir}\")\n",
    "        \n",
    "        # ===== PHASE 3: FEATURE MERGING =====\n",
    "        if len(feature_dirs) > 1:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 3: MERGING FEATURES (DISK-BASED)\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Combining flow and semantic features...\")\n",
    "            return self.merge_features_on_disk(feature_dirs)\n",
    "        elif feature_dirs:\n",
    "            return feature_dirs[0][1]  # Return single feature directory if only one type\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def merge_features_on_disk(self, feature_dirs):\n",
    "        \"\"\"\n",
    "        Merge features from multiple Parquet directories without loading all into memory.\n",
    "        Uses flow_id as the join key to combine features.\n",
    "        \n",
    "        OPTIMIZED: Uses Parquet's columnar format for efficient merging.\n",
    "        \n",
    "        Process:\n",
    "        1. Read flow features in batches\n",
    "        2. Find matching semantic features for each batch\n",
    "        3. Merge on flow_id\n",
    "        4. Apply feature engineering\n",
    "        5. Save merged batch to new Parquet file\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to merged feature directory\n",
    "        \"\"\"\n",
    "        print(\"Merging features using disk-based operations...\")\n",
    "        print(\"This preserves memory by processing in batches\")\n",
    "        \n",
    "        # Extract directory paths\n",
    "        flow_dir = feature_dirs[0][1]  # Flow features directory\n",
    "        semantic_dir = feature_dirs[1][1] if len(feature_dirs) > 1 else None\n",
    "        \n",
    "        # Get all Parquet files\n",
    "        import glob\n",
    "        flow_files = sorted(glob.glob(os.path.join(flow_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        batch_counter = 0\n",
    "        \n",
    "        for flow_file in tqdm(flow_files, desc=\"Merging batches\"):\n",
    "            # Load batch of flow features\n",
    "            flow_batch = pd.read_parquet(flow_file)\n",
    "            \n",
    "            # Merge with semantic features if available\n",
    "            if semantic_dir:\n",
    "                # Get flow IDs from this batch\n",
    "                flow_ids = set(flow_batch['flow_id'].values)\n",
    "                \n",
    "                # Load matching semantic features\n",
    "                semantic_batch = self.load_matching_semantic_features(\n",
    "                    semantic_dir, flow_ids\n",
    "                )\n",
    "                \n",
    "                # Merge on flow_id (left join to keep all flows)\n",
    "                if semantic_batch is not None and not semantic_batch.empty:\n",
    "                    combined_batch = pd.merge(\n",
    "                        flow_batch, semantic_batch,\n",
    "                        on='flow_id', how='left'\n",
    "                    )\n",
    "                    \n",
    "                    # Fill missing semantic features with zeros\n",
    "                    semantic_cols = semantic_batch.columns.difference(['flow_id'])\n",
    "                    combined_batch[semantic_cols] = combined_batch[semantic_cols].fillna(0)\n",
    "                else:\n",
    "                    combined_batch = flow_batch\n",
    "            else:\n",
    "                combined_batch = flow_batch\n",
    "            \n",
    "            # Apply feature engineering to create derived features\n",
    "            combined_batch = self.engineer_features(combined_batch)\n",
    "            \n",
    "            # Optimize data types\n",
    "            combined_batch = self.optimize_dtypes(combined_batch)\n",
    "            \n",
    "            # Save merged batch\n",
    "            output_file = os.path.join(\n",
    "                self.output_dir,\n",
    "                f'batch_{batch_counter:04d}.parquet'\n",
    "            )\n",
    "            combined_batch.to_parquet(\n",
    "                output_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            batch_counter += 1\n",
    "            \n",
    "            # Clean up memory\n",
    "            del combined_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"✓ Features merged and saved to: {self.output_dir}\")\n",
    "        print(f\"  Total batches processed: {batch_counter}\")\n",
    "        return self.output_dir\n",
    "    \n",
    "    def load_matching_semantic_features(self, semantic_dir, flow_ids):\n",
    "        \"\"\"\n",
    "        Load semantic features that match given flow IDs.\n",
    "        Efficient loading - only reads matching records.\n",
    "        \n",
    "        Args:\n",
    "            semantic_dir: Directory containing semantic feature Parquet files\n",
    "            flow_ids: Set of flow IDs to match\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Semantic features for matching flows\n",
    "        \"\"\"\n",
    "        matching_features = []\n",
    "        \n",
    "        import glob\n",
    "        semantic_files = sorted(glob.glob(os.path.join(semantic_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in semantic_files:\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            # Filter for matching flow IDs\n",
    "            matches = batch[batch['flow_id'].isin(flow_ids)]\n",
    "            \n",
    "            if not matches.empty:\n",
    "                matching_features.append(matches)\n",
    "        \n",
    "        # Combine all matching features\n",
    "        if matching_features:\n",
    "            return pd.concat(matching_features, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Create derived features that better capture attack patterns.\n",
    "        Feature engineering is crucial for ML model performance.\n",
    "        \n",
    "        ENHANCED: Added CICIDS-specific engineered features.\n",
    "        \n",
    "        Engineered features include:\n",
    "        1. Rate features: packets/second, bytes/second\n",
    "        2. Ratio features: flag counts / total packets\n",
    "        3. Port categories: well-known (<1024), registered (1024-49151)\n",
    "        4. Suspicious indicators: binary flags for quick filtering\n",
    "        5. CICIDS ratios: forward/backward packet ratios\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with raw features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With additional engineered features\n",
    "        \"\"\"\n",
    "        # ===== RATE FEATURES =====\n",
    "        # Packet rate (packets per second)\n",
    "        if 'total_packets' in df.columns and 'flow_duration' in df.columns:\n",
    "            df['packet_rate'] = df['total_packets'] / (df['flow_duration'] + 0.001)  # Avoid division by zero\n",
    "            df['packet_rate'] = df['packet_rate'].astype(np.float32)\n",
    "        \n",
    "        # Average packet size\n",
    "        if 'total_bytes' in df.columns and 'total_packets' in df.columns:\n",
    "            df['avg_packet_size'] = df['total_bytes'] / (df['total_packets'] + 1)\n",
    "            df['avg_packet_size'] = df['avg_packet_size'].astype(np.float32)\n",
    "        \n",
    "        # ===== CICIDS-SPECIFIC RATIOS =====\n",
    "        # Forward/Backward packet ratio\n",
    "        if 'fwd_packets' in df.columns and 'bwd_packets' in df.columns:\n",
    "            df['fwd_bwd_packets_ratio'] = df['fwd_packets'] / (df['bwd_packets'] + 1)\n",
    "            df['fwd_bwd_packets_ratio'] = df['fwd_bwd_packets_ratio'].astype(np.float32)\n",
    "        \n",
    "        # Forward/Backward bytes ratio\n",
    "        if 'fwd_bytes' in df.columns and 'bwd_bytes' in df.columns:\n",
    "            df['fwd_bwd_bytes_ratio'] = df['fwd_bytes'] / (df['bwd_bytes'] + 1)\n",
    "            df['fwd_bwd_bytes_ratio'] = df['fwd_bwd_bytes_ratio'].astype(np.float32)\n",
    "        \n",
    "        # ===== FLAG RATIO FEATURES =====\n",
    "        # Calculate flag ratios (important for detecting SYN floods, etc.)\n",
    "        flag_cols = ['syn_count', 'ack_count', 'fin_count', 'rst_count', 'psh_count', 'urg_count']\n",
    "        if all(col in df.columns for col in flag_cols) and 'total_packets' in df.columns:\n",
    "            for flag in flag_cols:\n",
    "                ratio_name = f'{flag}_ratio'\n",
    "                df[ratio_name] = df[flag] / (df['total_packets'] + 1)\n",
    "                df[ratio_name] = df[ratio_name].astype(np.float32)\n",
    "        \n",
    "        # ===== PORT CATEGORY FEATURES =====\n",
    "        # Categorize ports for better pattern recognition\n",
    "        if 'dst_port' in df.columns:\n",
    "            # Well-known ports (0-1023) - usually system services\n",
    "            df['is_well_known_port'] = (df['dst_port'] < 1024).astype(np.int8)\n",
    "            \n",
    "            # Registered ports (1024-49151) - usually applications\n",
    "            df['is_registered_port'] = ((df['dst_port'] >= 1024) & \n",
    "                                        (df['dst_port'] < 49152)).astype(np.int8)\n",
    "            \n",
    "            # Dynamic/private ports (49152-65535) - usually client connections\n",
    "            df['is_dynamic_port'] = (df['dst_port'] >= 49152).astype(np.int8)\n",
    "            \n",
    "            # Common attack target ports\n",
    "            attack_ports = {80, 443, 22, 23, 3389, 445, 139, 21, 25, 3306}\n",
    "            df['is_common_target_port'] = df['dst_port'].isin(attack_ports).astype(np.int8)\n",
    "        \n",
    "        # ===== SUSPICIOUS INDICATORS =====\n",
    "        # Binary flags for quick filtering\n",
    "        if 'suspicious_score' in df.columns:\n",
    "            df['is_suspicious'] = (df['suspicious_score'] > 0).astype(np.int8)\n",
    "            df['highly_suspicious'] = (df['suspicious_score'] > 5).astype(np.int8)\n",
    "        \n",
    "        # ===== ATTACK COMBINATION FEATURES =====\n",
    "        # Some attacks use specific combinations\n",
    "        if 'has_sql' in df.columns and 'has_script' in df.columns:\n",
    "            # SQL + Script often indicates complex web attack\n",
    "            df['sql_and_script'] = ((df.get('has_sql', 0) > 0) & \n",
    "                                    (df.get('has_script', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        if 'has_cmd' in df.columns and 'nlp_encoding_detected' in df.columns:\n",
    "            # Command injection + encoding often indicates obfuscated attack\n",
    "            df['encoded_cmd'] = ((df.get('has_cmd', 0) > 0) & \n",
    "                                 (df.get('nlp_encoding_detected', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        # ===== PAYLOAD RATIO FEATURES =====\n",
    "        if 'total_payload_bytes' in df.columns and 'total_bytes' in df.columns:\n",
    "            # Ratio of payload to total traffic\n",
    "            df['payload_ratio'] = df['total_payload_bytes'] / (df['total_bytes'] + 1)\n",
    "            df['payload_ratio'] = df['payload_ratio'].astype(np.float32)\n",
    "        \n",
    "        # ===== ACTIVE/IDLE FEATURES (CICIDS) =====\n",
    "        if 'active_mean' in df.columns and 'idle_mean' in df.columns:\n",
    "            # Active to idle ratio\n",
    "            df['active_idle_ratio'] = df['active_mean'] / (df['idle_mean'] + 0.001)\n",
    "            df['active_idle_ratio'] = df['active_idle_ratio'].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def optimize_dtypes(self, df):\n",
    "        \"\"\"\n",
    "        Optimize DataFrame dtypes for memory efficiency.\n",
    "        Reduces memory usage significantly.\n",
    "        \"\"\"\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            \n",
    "            if col_type != 'object':\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                \n",
    "                # Integer optimization\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                \n",
    "                # Float optimization\n",
    "                elif str(col_type)[:5] == 'float':\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539f45e",
   "metadata": {},
   "source": [
    "# ### Step 6: CICIDS2017 Label Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: CICIDS Label Matcher - Ground Truth Integration\n",
    "\"\"\"\n",
    "PURPOSE: Match extracted flows with CICIDS2017 ground truth labels\n",
    "This class integrates the official attack labels from CICIDS2017 dataset\n",
    "to enable supervised learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads CICIDS2017 CSV label files incrementally\n",
    "2. Creates a mapping between network flows and attack types\n",
    "3. Handles multiple CSV files from different days\n",
    "4. Assigns numeric labels for ML training\n",
    "5. Maintains attack type distribution statistics\n",
    "\n",
    "CICIDS2017 ATTACK TYPES:\n",
    "The dataset contains 15 different attack categories:\n",
    "0. BENIGN - Normal, non-malicious traffic\n",
    "1. Bot - Botnet traffic\n",
    "2. DDoS - Distributed Denial of Service\n",
    "3. DoS GoldenEye - Application layer DoS\n",
    "4. DoS Hulk - Volume-based DoS\n",
    "5. DoS Slowhttptest - Slow HTTP attack\n",
    "6. DoS slowloris - Connection exhaustion\n",
    "7. FTP-Patator - FTP brute force\n",
    "8. Heartbleed - SSL vulnerability exploit\n",
    "9. Infiltration - Network infiltration\n",
    "10. PortScan - Port scanning activity\n",
    "11. SSH-Patator - SSH brute force\n",
    "12. Web Attack - Brute Force\n",
    "13. Web Attack - SQL Injection\n",
    "14. Web Attack - XSS\n",
    "\n",
    "MATCHING STRATEGY:\n",
    "- Primary: Match by 5-tuple (IPs, ports, protocol)\n",
    "- Fallback: Match by destination port (majority vote)\n",
    "- Default: Label as BENIGN if no match found\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Loads CSV files in chunks (100K rows at a time)\n",
    "- Builds port-label cache instead of full flow mapping\n",
    "- Processes labels incrementally without loading all\n",
    "- Uses Parquet for output\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet instead of HDF5 for storage\n",
    "- Improved chunked CSV reading\n",
    "- Better handling of CICIDS CSV column variations\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedLabelMatcher:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize label matcher with attack type mappings.\n",
    "        \"\"\"\n",
    "        # Mapping from text labels to numeric codes for ML\n",
    "        self.attack_mapping = {\n",
    "            'BENIGN': 0,\n",
    "            'Bot': 1,\n",
    "            'DDoS': 2,\n",
    "            'DoS GoldenEye': 3,\n",
    "            'DoS Hulk': 4,\n",
    "            'DoS Slowhttptest': 5,\n",
    "            'DoS slowloris': 6,\n",
    "            'FTP-Patator': 7,\n",
    "            'Heartbleed': 8,\n",
    "            'Infiltration': 9,\n",
    "            'PortScan': 10,\n",
    "            'SSH-Patator': 11,\n",
    "            'Web Attack - Brute Force': 12,\n",
    "            'Web Attack - SQL Injection': 13,\n",
    "            'Web Attack - XSS': 14,\n",
    "            'Web Attack – Brute Force': 12,  # Alternative spelling\n",
    "            'Web Attack – XSS': 14,  # Alternative spelling\n",
    "            'Web Attack – Sql Injection': 13  # Alternative spelling\n",
    "        }\n",
    "        \n",
    "        # Cache for port-to-label mapping (memory efficient)\n",
    "        self.port_label_cache = {}\n",
    "        \n",
    "        # Cache for IP+port combinations (more accurate)\n",
    "        self.ip_port_label_cache = {}\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.label_statistics = Counter()\n",
    "    \n",
    "    def load_labels_incrementally(self, label_files):\n",
    "        \"\"\"\n",
    "        Load CICIDS label files incrementally to avoid memory overflow.\n",
    "        Builds port-based and IP+port mappings for efficient matching.\n",
    "        \n",
    "        Args:\n",
    "            label_files: List of CSV file paths\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING CICIDS LABELS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        label_counts = Counter()\n",
    "        total_rows_processed = 0\n",
    "        \n",
    "        # Process each label file\n",
    "        for file_idx, label_file in enumerate(label_files):\n",
    "            print(f\"\\nProcessing label file {file_idx + 1}/{len(label_files)}: {os.path.basename(label_file)}\")\n",
    "            \n",
    "            # Read CSV in chunks to manage memory\n",
    "            chunk_size = 100000\n",
    "            chunks_processed = 0\n",
    "            \n",
    "            try:\n",
    "                # Process file in chunks\n",
    "                for chunk in pd.read_csv(label_file, encoding='latin-1', chunksize=chunk_size):\n",
    "                    # Clean column names (remove spaces)\n",
    "                    chunk.columns = chunk.columns.str.strip()\n",
    "                    \n",
    "                    # Find label column (handles different naming conventions)\n",
    "                    label_col = None\n",
    "                    for col in ['Label', 'label', 'LABEL', ' Label']:\n",
    "                        if col in chunk.columns:\n",
    "                            label_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if not label_col:\n",
    "                        print(f\"  Warning: No label column found in chunk {chunks_processed}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Find required columns for matching\n",
    "                    dst_port_col = None\n",
    "                    dst_ip_col = None\n",
    "                    src_ip_col = None\n",
    "                    \n",
    "                    # Port column variations\n",
    "                    for col in ['Destination Port', 'Dst Port', 'dst_port', ' Destination Port', 'dst Port']:\n",
    "                        if col in chunk.columns:\n",
    "                            dst_port_col = col\n",
    "                            break\n",
    "                    \n",
    "                    # IP column variations\n",
    "                    for col in ['Destination IP', 'Dst IP', 'dst_ip', ' Destination IP']:\n",
    "                        if col in chunk.columns:\n",
    "                            dst_ip_col = col\n",
    "                            break\n",
    "                    \n",
    "                    for col in ['Source IP', 'Src IP', 'src_ip', ' Source IP']:\n",
    "                        if col in chunk.columns:\n",
    "                            src_ip_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if label_col and dst_port_col:\n",
    "                        # Build port-label mapping\n",
    "                        for port, group in chunk.groupby(dst_port_col):\n",
    "                            # Get most frequent label for this port\n",
    "                            most_common_label = group[label_col].mode()\n",
    "                            if len(most_common_label) > 0:\n",
    "                                label = most_common_label.iloc[0]\n",
    "                                \n",
    "                                # Update port cache with majority vote\n",
    "                                if port not in self.port_label_cache:\n",
    "                                    self.port_label_cache[port] = Counter()\n",
    "                                self.port_label_cache[port][label] += len(group)\n",
    "                                \n",
    "                                # Update statistics\n",
    "                                label_counts[label] += len(group)\n",
    "                        \n",
    "                        # Build IP+port mapping for better accuracy\n",
    "                        if dst_ip_col and src_ip_col:\n",
    "                            for _, row in chunk.iterrows():\n",
    "                                key = f\"{row[src_ip_col]}:{row[dst_ip_col]}:{row[dst_port_col]}\"\n",
    "                                label = row[label_col]\n",
    "                                \n",
    "                                if key not in self.ip_port_label_cache:\n",
    "                                    self.ip_port_label_cache[key] = Counter()\n",
    "                                self.ip_port_label_cache[key][label] += 1\n",
    "                    \n",
    "                    chunks_processed += 1\n",
    "                    total_rows_processed += len(chunk)\n",
    "                    \n",
    "                    # Periodic memory cleanup\n",
    "                    if chunks_processed % 10 == 0:\n",
    "                        gc.collect()\n",
    "                        print(f\"  Processed {chunks_processed * chunk_size:,} rows...\")\n",
    "                \n",
    "                print(f\"  Completed: {chunks_processed} chunks\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing file: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Finalize port-label mapping (keep only most common label per port)\n",
    "        for port in self.port_label_cache:\n",
    "            if isinstance(self.port_label_cache[port], Counter):\n",
    "                # Get most common label for this port\n",
    "                most_common = self.port_label_cache[port].most_common(1)[0][0]\n",
    "                self.port_label_cache[port] = most_common\n",
    "        \n",
    "        # Finalize IP+port mapping\n",
    "        for key in self.ip_port_label_cache:\n",
    "            if isinstance(self.ip_port_label_cache[key], Counter):\n",
    "                most_common = self.ip_port_label_cache[key].most_common(1)[0][0]\n",
    "                self.ip_port_label_cache[key] = most_common\n",
    "        \n",
    "        # Display label distribution\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABEL DISTRIBUTION:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(label_counts.values())\n",
    "        for label, count in label_counts.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTotal rows processed: {total_rows_processed:,}\")\n",
    "        print(f\"Port-label mappings created: {len(self.port_label_cache):,}\")\n",
    "        print(f\"IP+Port mappings created: {len(self.ip_port_label_cache):,}\")\n",
    "        \n",
    "        # Store statistics\n",
    "        self.label_statistics = label_counts\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def apply_labels_to_batch(self, df):\n",
    "        \"\"\"\n",
    "        Apply labels to a batch of extracted features.\n",
    "        Uses IP+port mapping first, then port-based mapping as fallback.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with extracted features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With added label columns\n",
    "        \"\"\"\n",
    "        # Initialize with BENIGN\n",
    "        df['label'] = 0\n",
    "        df['attack_type'] = 'BENIGN'\n",
    "        df['label_confidence'] = 0.0\n",
    "        \n",
    "        # Try IP+port matching first (more accurate)\n",
    "        if 'src_ip' in df.columns and 'dst_ip' in df.columns and 'dst_port' in df.columns:\n",
    "            for idx, row in df.iterrows():\n",
    "                key = f\"{row.get('src_ip', '')}:{row.get('dst_ip', '')}:{row.get('dst_port', 0)}\"\n",
    "                if key in self.ip_port_label_cache:\n",
    "                    attack_type = self.ip_port_label_cache[key]\n",
    "                    df.at[idx, 'attack_type'] = attack_type\n",
    "                    df.at[idx, 'label'] = self.attack_mapping.get(attack_type, 0)\n",
    "                    df.at[idx, 'label_confidence'] = 0.9  # High confidence\n",
    "        \n",
    "        # Fallback to port-only matching for unlabeled flows\n",
    "        if 'dst_port' in df.columns:\n",
    "            unlabeled = df[df['label'] == 0]\n",
    "            if not unlabeled.empty:\n",
    "                # Map ports to attack types using cache\n",
    "                port_labels = unlabeled['dst_port'].map(self.port_label_cache).fillna('BENIGN')\n",
    "                df.loc[unlabeled.index, 'attack_type'] = port_labels\n",
    "                \n",
    "                # Convert text labels to numeric\n",
    "                df.loc[unlabeled.index, 'label'] = port_labels.map(self.attack_mapping).fillna(0)\n",
    "                \n",
    "                # Lower confidence for port-only matching\n",
    "                df.loc[unlabeled.index, 'label_confidence'] = 0.5\n",
    "        \n",
    "        # Ensure correct dtypes\n",
    "        df['label'] = df['label'].astype(np.int8)\n",
    "        df['label_confidence'] = df['label_confidence'].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_features_with_labels(self, features_dir, label_files):\n",
    "        \"\"\"\n",
    "        Process feature files and add labels in batches.\n",
    "        Creates new Parquet files with labeled features.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            label_files: List of CICIDS CSV files\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to labeled feature directory\n",
    "        \"\"\"\n",
    "        if not label_files:\n",
    "            print(\"No label files provided, skipping labeling\")\n",
    "            return features_dir\n",
    "        \n",
    "        # Load label mappings into cache\n",
    "        print(\"\\nBuilding label cache from CSV files...\")\n",
    "        self.load_labels_incrementally(label_files)\n",
    "        \n",
    "        # Create output directory for labeled features\n",
    "        labeled_dir = os.path.join(Config.TEMP_DIR, 'labeled_features')\n",
    "        os.makedirs(labeled_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"APPLYING LABELS TO FEATURES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        labeled_count = Counter()\n",
    "        \n",
    "        # Process each feature batch\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for idx, feature_file in enumerate(tqdm(feature_files, desc=\"Labeling batches\")):\n",
    "            # Load feature batch\n",
    "            batch = pd.read_parquet(feature_file)\n",
    "            \n",
    "            # Apply labels\n",
    "            batch = self.apply_labels_to_batch(batch)\n",
    "            \n",
    "            # Track label distribution\n",
    "            labeled_count.update(batch['attack_type'].value_counts().to_dict())\n",
    "            \n",
    "            # Save labeled batch\n",
    "            output_file = os.path.join(labeled_dir, f'batch_{idx:04d}.parquet')\n",
    "            batch.to_parquet(\n",
    "                output_file,\n",
    "                engine='pyarrow',\n",
    "                compression='snappy',\n",
    "                index=False\n",
    "            )\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Display labeling results\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABELING RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(labeled_count.values())\n",
    "        for attack_type, count in labeled_count.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {attack_type:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nLabeled features saved to: {labeled_dir}\")\n",
    "        \n",
    "        # Validate labeling quality\n",
    "        benign_pct = labeled_count.get('BENIGN', 0) / total * 100 if total > 0 else 0\n",
    "        if benign_pct > 90:\n",
    "            print(\"\\n Warning: >90% flows labeled as BENIGN\")\n",
    "            print(\"   This might indicate labeling issues or imbalanced dataset\")\n",
    "            print(\"   Consider using stratified sampling for ML training\")\n",
    "        \n",
    "        return labeled_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3307df",
   "metadata": {},
   "source": [
    "# ### Step 7: Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Analysis - Selection & Importance Ranking\n",
    "\"\"\"\n",
    "PURPOSE: Analyze and select the most important features for machine learning\n",
    "This class determines which features are most useful for detecting attacks.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Collects statistical information about all features\n",
    "2. Calculates feature importance using mutual information\n",
    "3. Selects the top N most informative features\n",
    "4. Removes redundant or uninformative features\n",
    "5. Provides feature ranking for interpretability\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- MUTUAL INFORMATION: Measures how much knowing a feature reduces uncertainty about the label\n",
    "- FEATURE SELECTION: Choosing subset of features that maximize predictive power\n",
    "- CURSE OF DIMENSIONALITY: Too many features can hurt ML performance\n",
    "- FEATURE IMPORTANCE: Understanding which features drive predictions\n",
    "\n",
    "WHY FEATURE SELECTION MATTERS:\n",
    "- Reduces training time (fewer features to process)\n",
    "- Improves model performance (removes noise)\n",
    "- Prevents overfitting (simpler models generalize better)\n",
    "- Enhances interpretability (understand what drives detection)\n",
    "\n",
    "SELECTION STRATEGY:\n",
    "1. Statistical Analysis: Mean, variance, range for each feature\n",
    "2. Importance Scoring: Mutual information with target labels\n",
    "3. Redundancy Removal: Correlation analysis\n",
    "4. Top-K Selection: Keep only the best features\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet for data loading\n",
    "- Parallel processing for importance calculation\n",
    "- Better handling of CICIDS-specific features\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeatureAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize feature analyzer with storage for statistics and importance scores.\n",
    "        \"\"\"\n",
    "        self.feature_importance = {}  # Feature name -> importance score\n",
    "        self.selected_features = []   # Final list of selected features\n",
    "        self.feature_stats = {}        # Statistical summaries per feature\n",
    "    \n",
    "    def analyze_features_incrementally(self, features_dir, target_col='label'):\n",
    "        \"\"\"\n",
    "        Analyze features using two-pass incremental processing.\n",
    "        Pass 1: Collect statistics\n",
    "        Pass 2: Calculate importance on representative sample\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            target_col: Name of label column\n",
    "            \n",
    "        Returns:\n",
    "            list: Selected feature names\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE ANALYSIS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ===== PASS 1: STATISTICS COLLECTION =====\n",
    "        print(\"Pass 1: Collecting feature statistics...\")\n",
    "        print(\"  This helps understand feature distributions\")\n",
    "        self.collect_feature_stats(features_dir, target_col)\n",
    "        \n",
    "        # ===== PASS 2: IMPORTANCE CALCULATION =====\n",
    "        print(\"\\nPass 2: Calculating feature importance on sample...\")\n",
    "        print(\"  This identifies most informative features\")\n",
    "        self.calculate_importance_sample(features_dir, target_col)\n",
    "        \n",
    "        # ===== FEATURE SELECTION =====\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(\n",
    "            self.feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Select top features\n",
    "        self.selected_features = [f[0] for f in sorted_features[:Config.TOP_FEATURES]]\n",
    "        \n",
    "        # ===== DISPLAY RESULTS =====\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\"TOP {min(15, len(sorted_features))} FEATURES:\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"{'Rank':<5} {'Feature':<35} {'Importance':<10} {'Type'}\")\n",
    "        print(\"-\"*65)\n",
    "        \n",
    "        for i, (feature, score) in enumerate(sorted_features[:15], 1):\n",
    "            # Determine feature type for interpretation\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            print(f\"{i:<5} {feature:<35} {score:>10.4f} {feature_type}\")\n",
    "        \n",
    "        print(f\"\\nTotal features analyzed: {len(self.feature_importance)}\")\n",
    "        print(f\"Features selected: {len(self.selected_features)}\")\n",
    "        \n",
    "        # ===== CICIDS-SPECIFIC FEATURE INSIGHTS =====\n",
    "        cicids_features = [f for f in self.selected_features if any(x in f.lower() for x in ['fwd', 'bwd', 'active', 'idle', 'iat'])]\n",
    "        if cicids_features:\n",
    "            print(f\"\\nCICIDS-specific features selected: {len(cicids_features)}\")\n",
    "            print(f\"  Examples: {cicids_features[:3]}\")\n",
    "        \n",
    "        return self.selected_features\n",
    "    \n",
    "    def collect_feature_stats(self, features_dir, target_col):\n",
    "        \"\"\"\n",
    "        First pass: Collect statistical summaries for each feature.\n",
    "        This helps understand data distribution and identify issues.\n",
    "        \n",
    "        Statistics collected:\n",
    "        - Sum, sum of squares (for mean/variance calculation)\n",
    "        - Min, max (for range)\n",
    "        - Count (for missing value detection)\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Get feature columns from first batch\n",
    "        first_batch = pd.read_parquet(feature_files[0], columns=None, engine='pyarrow')\n",
    "        \n",
    "        # Identify feature columns (exclude metadata)\n",
    "        exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "        feature_cols = [col for col in first_batch.columns \n",
    "                      if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"  Analyzing {len(feature_cols)} features...\")\n",
    "        \n",
    "        # Initialize statistics collectors\n",
    "        for col in feature_cols:\n",
    "            self.feature_stats[col] = {\n",
    "                'sum': 0,      # For mean calculation\n",
    "                'sum_sq': 0,   # For variance calculation\n",
    "                'count': 0,    # Total non-null values\n",
    "                'min': float('inf'),\n",
    "                'max': float('-inf'),\n",
    "                'zeros': 0,    # Count of zero values\n",
    "                'unique': set() # Track unique values (sampled)\n",
    "            }\n",
    "        \n",
    "        # Process all batches\n",
    "        for file in tqdm(feature_files, desc=\"Collecting stats\"):\n",
    "            batch = pd.read_parquet(file, columns=feature_cols, engine='pyarrow')\n",
    "            \n",
    "            for col in feature_cols:\n",
    "                if col in batch.columns:\n",
    "                    # Get non-null values\n",
    "                    values = batch[col].fillna(0)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    stats = self.feature_stats[col]\n",
    "                    stats['sum'] += values.sum()\n",
    "                    stats['sum_sq'] += (values ** 2).sum()\n",
    "                    stats['count'] += len(values)\n",
    "                    stats['min'] = min(stats['min'], values.min())\n",
    "                    stats['max'] = max(stats['max'], values.max())\n",
    "                    stats['zeros'] += (values == 0).sum()\n",
    "                    \n",
    "                    # Sample unique values (limit to 100 for memory)\n",
    "                    if len(stats['unique']) < 100:\n",
    "                        sample_size = min(10, len(values))\n",
    "                        stats['unique'].update(values.sample(sample_size).tolist())\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate derived statistics\n",
    "        for col, stats in self.feature_stats.items():\n",
    "            if stats['count'] > 0:\n",
    "                stats['mean'] = stats['sum'] / stats['count']\n",
    "                stats['variance'] = (stats['sum_sq'] / stats['count']) - (stats['mean'] ** 2)\n",
    "                stats['std'] = np.sqrt(max(0, stats['variance']))  # Avoid negative variance due to rounding\n",
    "                stats['range'] = stats['max'] - stats['min']\n",
    "                stats['zero_ratio'] = stats['zeros'] / stats['count']\n",
    "            else:\n",
    "                stats['mean'] = stats['variance'] = stats['std'] = stats['range'] = 0\n",
    "                stats['zero_ratio'] = 1\n",
    "    \n",
    "    def calculate_importance_sample(self, features_dir, target_col, sample_size=100000):\n",
    "        \"\"\"\n",
    "        Second pass: Calculate feature importance using mutual information.\n",
    "        Uses a representative sample for efficiency.\n",
    "        \n",
    "        OPTIMIZED: Can use parallel processing if available.\n",
    "        \n",
    "        Mutual Information measures:\n",
    "        - How much information a feature provides about the target\n",
    "        - Non-linear relationships (unlike correlation)\n",
    "        - Works with any feature type\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory with feature files\n",
    "            target_col: Target label column\n",
    "            sample_size: Number of samples for importance calculation\n",
    "        \"\"\"\n",
    "        # Load stratified sample for importance calculation\n",
    "        print(f\"  Loading sample of {sample_size:,} flows...\")\n",
    "        sample_dfs = []\n",
    "        remaining_samples = sample_size\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Sample from different parts of the dataset\n",
    "        sample_interval = max(1, len(feature_files) // 10)  # Sample from 10 points\n",
    "        \n",
    "        for i in range(0, len(feature_files), sample_interval):\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "            \n",
    "            file = feature_files[min(i, len(feature_files)-1)]\n",
    "            batch_sample_size = min(remaining_samples, 10000)\n",
    "            \n",
    "            # Load batch sample\n",
    "            batch = pd.read_parquet(file)\n",
    "            if len(batch) > batch_sample_size:\n",
    "                batch = batch.sample(batch_sample_size, random_state=Config.RANDOM_STATE)\n",
    "            sample_dfs.append(batch)\n",
    "            remaining_samples -= len(batch)\n",
    "        \n",
    "        # Combine samples\n",
    "        sample_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "        print(f\"  Sample loaded: {len(sample_df):,} flows\")\n",
    "        \n",
    "        # Get feature columns\n",
    "        exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "        feature_cols = [col for col in sample_df.columns \n",
    "                       if col not in exclude_cols]\n",
    "        \n",
    "        # Prepare feature matrix and labels\n",
    "        X = sample_df[feature_cols].fillna(0)\n",
    "        y = sample_df[target_col]\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        print(\"  Calculating mutual information scores...\")\n",
    "        print(f\"  Using {Config.N_JOBS if Config.N_JOBS > 0 else 'all'} CPU cores\")\n",
    "        \n",
    "        mi_scores = mutual_info_classif(\n",
    "            X, y, \n",
    "            random_state=Config.RANDOM_STATE,\n",
    "            n_neighbors=3  # Faster calculation\n",
    "        )\n",
    "        \n",
    "        # Store importance scores\n",
    "        self.feature_importance = dict(zip(feature_cols, mi_scores))\n",
    "        \n",
    "        # Identify uninformative features (near-zero importance)\n",
    "        uninformative = [f for f, score in self.feature_importance.items() if score < 0.001]\n",
    "        if uninformative:\n",
    "            print(f\"\\n   Found {len(uninformative)} uninformative features (MI < 0.001)\")\n",
    "            print(f\"     Examples: {uninformative[:5]}\")\n",
    "        \n",
    "        # Identify highly important CICIDS features\n",
    "        cicids_important = [(f, s) for f, s in self.feature_importance.items() \n",
    "                           if s > 0.1 and any(x in f.lower() for x in ['fwd', 'bwd', 'iat', 'active', 'idle'])]\n",
    "        if cicids_important:\n",
    "            print(f\"\\n  ✓ Important CICIDS features found: {len(cicids_important)}\")\n",
    "            for feat, score in cicids_important[:3]:\n",
    "                print(f\"     {feat}: {score:.3f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del sample_df, sample_dfs, X, y\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_feature_type(self, feature_name):\n",
    "        \"\"\"\n",
    "        Categorize feature by its name for better interpretation.\n",
    "        Enhanced to recognize CICIDS-specific features.\n",
    "        \n",
    "        Categories:\n",
    "        - Flow: Timing and size statistics\n",
    "        - Forward: Forward direction features (CICIDS)\n",
    "        - Backward: Backward direction features (CICIDS)\n",
    "        - Active/Idle: Activity patterns (CICIDS)\n",
    "        - Flag: TCP flag related\n",
    "        - Port: Port number features\n",
    "        - Payload: Content-based features\n",
    "        - NLP: Natural language processing scores\n",
    "        - Engineered: Derived features\n",
    "        \"\"\"\n",
    "        feature_lower = feature_name.lower()\n",
    "        \n",
    "        # CICIDS-specific categories\n",
    "        if 'fwd' in feature_lower or 'forward' in feature_lower:\n",
    "            return \"Forward\"\n",
    "        elif 'bwd' in feature_lower or 'backward' in feature_lower:\n",
    "            return \"Backward\"\n",
    "        elif 'active' in feature_lower or 'idle' in feature_lower:\n",
    "            return \"Active/Idle\"\n",
    "        elif 'iat' in feature_lower:\n",
    "            return \"IAT\"\n",
    "        elif 'flow' in feature_lower or 'duration' in feature_lower:\n",
    "            return \"Flow\"\n",
    "        elif 'flag' in feature_lower or 'syn' in feature_lower or 'ack' in feature_lower:\n",
    "            return \"Flag\"\n",
    "        elif 'port' in feature_lower:\n",
    "            return \"Port\"\n",
    "        elif 'payload' in feature_lower or 'entropy' in feature_lower:\n",
    "            return \"Payload\"\n",
    "        elif 'nlp' in feature_lower:\n",
    "            return \"NLP\"\n",
    "        elif 'ratio' in feature_lower or 'rate' in feature_lower:\n",
    "            return \"Engineered\"\n",
    "        elif 'has_' in feature_lower or 'is_' in feature_lower:\n",
    "            return \"Binary\"\n",
    "        elif 'suspicious' in feature_lower:\n",
    "            return \"Detection\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    def get_feature_insights(self):\n",
    "        \"\"\"\n",
    "        Provide insights about selected features for interpretability.\n",
    "        Helps understand what the model will focus on.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Insights about feature selection\n",
    "        \"\"\"\n",
    "        insights = {\n",
    "            'total_features': len(self.feature_importance),\n",
    "            'selected_features': len(self.selected_features),\n",
    "            'feature_types': Counter(),\n",
    "            'top_5_features': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Analyze feature types\n",
    "        for feature in self.selected_features:\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            insights['feature_types'][feature_type] += 1\n",
    "        \n",
    "        # Get top 5 features with scores\n",
    "        for feature in self.selected_features[:5]:\n",
    "            insights['top_5_features'].append({\n",
    "                'name': feature,\n",
    "                'importance': self.feature_importance[feature],\n",
    "                'type': self.get_feature_type(feature)\n",
    "            })\n",
    "        \n",
    "        # Provide recommendations based on feature distribution\n",
    "        if insights['feature_types'].get('Forward', 0) + insights['feature_types'].get('Backward', 0) > 10:\n",
    "            insights['recommendations'].append(\n",
    "                \"Strong directional features - good for detecting asymmetric attacks\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('NLP', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High NLP feature count - model focuses on payload content\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('Active/Idle', 0) > 3:\n",
    "            insights['recommendations'].append(\n",
    "                \"Active/Idle features selected - good for detecting slow attacks\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('Flow', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High flow feature count - model focuses on traffic patterns\"\n",
    "            )\n",
    "        \n",
    "        if len(self.selected_features) < 30:\n",
    "            insights['recommendations'].append(\n",
    "                \"Consider increasing TOP_FEATURES if accuracy is low\"\n",
    "            )\n",
    "        \n",
    "        return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e192c",
   "metadata": {},
   "source": [
    "# ### Step 8: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19848809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Machine Learning Pipeline - Enhanced Model Selection Support\n",
    "\n",
    "\"\"\"\n",
    "PURPOSE: Train and evaluate machine learning models with class balancing, comprehensive metrics, and visualizations\n",
    "This class implements memory-efficient ML training with enhanced evaluation for imbalanced datasets.\n",
    "WHAT THIS CELL DOES:\n",
    "1. Implements incremental/online learning algorithms with class balancing\n",
    "2. Trains models in batches without loading all data\n",
    "3. Performs stratified train/test split to maintain class distribution\n",
    "4. Evaluates models using comprehensive metrics beyond accuracy\n",
    "5. Provides detailed per-class performance analysis\n",
    "6. Supports multiple ML algorithms optimized for imbalanced data\n",
    "7. ENHANCED: Generates all performance visualizations automatically\n",
    "8. FIXED: SGD class balancing issue with partial_fit method\n",
    "9. FIXED: Deprecated parameters and XGBoost multiclass configuration\n",
    "10. FIXED: PyArrow Parquet nrows compatibility issue\n",
    "11. FIXED: LightGBM prediction format issue (returns probabilities vs class predictions)\n",
    " \n",
    "ALGORITHMS USED:\n",
    "1. SGD Classifier with Manual Class Balancing \n",
    "2. Random Forest with Balanced Classes\n",
    "3. LightGBM (FIXED: Proper prediction handling)\n",
    "4. XGBoost with External Memory \n",
    " \n",
    "VISUALIZATION OUTPUTS:\n",
    "- Confusion matrix heatmap\n",
    "- Per-class F1 score bar chart\n",
    "- Feature importance plot\n",
    "- Performance metrics radar chart\n",
    " \n",
    "ENHANCED FEATURES:\n",
    "- Class balancing to handle BENIGN vs attack imbalance\n",
    "- Stratified sampling to ensure all attack types in train/test\n",
    "- Multiple evaluation metrics (F1, precision, recall)\n",
    "- Per-class performance analysis for each attack type\n",
    "- Attack detection rate calculation\n",
    "- Enhanced Random Forest implementation\n",
    "- Checkpoint saving for recovery\n",
    "- FIXED: SGD class balancing compatibility with partial_fit\n",
    "- FIXED: XGBoost multiclass probability output\n",
    "- FIXED: LightGBM prediction format conversion\n",
    "- ADDED: Automatic visualization generation\n",
    " \n",
    " \n",
    "WHY ENHANCED EVALUATION:\n",
    "- Accuracy alone misleading with imbalanced data\n",
    "- F1-score balances precision and recall\n",
    "- Per-class metrics identify which attacks are detected\n",
    "- Attack detection rate measures security effectiveness\n",
    "- Visualizations provide intuitive understanding of performance\n",
    " \n",
    "TECHNICAL FIXES APPLIED:\n",
    "- SGD: Fixed deprecated 'log_loss' parameter and removed conflicting warm_start\n",
    "- XGBoost: Fixed multiclass objective to return probabilities and removed invalid scale_pos_weight\n",
    "- PyArrow: Fixed nrows parameter compatibility issue in stratified data splitting\n",
    "- LightGBM: Fixed prediction format (probabilities -> class predictions)\n",
    "- Both fixes maintain backward compatibility while improving performance\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from math import pi\n",
    "\n",
    "class MemoryOptimizedMLPipeline:\n",
    "    def __init__(self, selected_models=None):\n",
    "        \"\"\"\n",
    "        Initialize ML pipeline with enhanced metrics storage\n",
    "        \n",
    "        MINIMAL FIX: Added selected_models parameter\n",
    "        \"\"\"\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = None\n",
    "        self.train_history = defaultdict(list)\n",
    "        self.feature_importance = {}\n",
    "        self.checkpoints_dir = os.path.join(Config.OUTPUT_DIR, 'checkpoints')\n",
    "        os.makedirs(self.checkpoints_dir, exist_ok=True)\n",
    "        \n",
    "        # MINIMAL FIX: Store selected models, fallback to Config if not provided\n",
    "        self.selected_models = selected_models or getattr(Config, 'SELECTED_MODELS', ['sgd', 'lightgbm'])\n",
    "        \n",
    "        # Enhanced metrics tracking\n",
    "        self.training_metadata = {\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_duration_seconds': 0,\n",
    "            'batches_processed': 0,\n",
    "            'total_training_samples': 0,\n",
    "            'total_test_samples': 0,\n",
    "            'checkpoints_saved': []\n",
    "        }\n",
    "        \n",
    "        # SGD class balancing flag\n",
    "        self.use_manual_class_weights = Config.USE_CLASS_BALANCING\n",
    "    \n",
    "    def save_checkpoint(self, model_name, batch_num):\n",
    "        \"\"\"\n",
    "        Save model checkpoint for recovery\n",
    "        \"\"\"\n",
    "        if model_name in self.models and self.models[model_name] is not None:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.checkpoints_dir, \n",
    "                f'{model_name}_batch_{batch_num}.pkl'\n",
    "            )\n",
    "            try:\n",
    "                with open(checkpoint_path, 'wb') as f:\n",
    "                    pickle.dump({\n",
    "                        'model': self.models[model_name],\n",
    "                        'scaler': self.scaler,\n",
    "                        'batch_num': batch_num,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }, f)\n",
    "                self.training_metadata['checkpoints_saved'].append(checkpoint_path)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not save checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def train_models_incrementally(self, features_dir, selected_features):\n",
    "        \"\"\"\n",
    "        Main training function with comprehensive metrics capture and visualizations\n",
    "        \"\"\"\n",
    "        # Record training start\n",
    "        self.training_metadata['start_time'] = datetime.now()\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED INCREMENTAL MACHINE LEARNING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training models: {self.selected_models}\")  # MINIMAL FIX: Use self.selected_models\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "        print(f\"Class Balancing: {'ENABLED' if Config.USE_CLASS_BALANCING else 'DISABLED'}\")\n",
    "        print(f\"Stratified Split: {'ENABLED' if Config.USE_STRATIFIED_SPLIT else 'DISABLED'}\")\n",
    "        print(f\"Evaluation Metrics: {', '.join(Config.EVALUATION_METRICS)}\")\n",
    "        print(f\"Checkpoints will be saved to: {self.checkpoints_dir}\")\n",
    "        \n",
    "        # Initialize models with FIXED parameters\n",
    "        self.initialize_enhanced_models()\n",
    "        \n",
    "        # Training setup\n",
    "        n_classes = 15\n",
    "        classes = np.arange(n_classes)\n",
    "        batch_count = 0\n",
    "        train_scores = defaultdict(list)\n",
    "        all_train_labels = []\n",
    "        \n",
    "        # Get feature files\n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        n_batches = len(feature_files)\n",
    "        \n",
    "        if n_batches == 0:\n",
    "            print(\"Error: No feature files found!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Create train/test split with FIXED method\n",
    "        train_files, test_files = self.create_stratified_split_fixed(feature_files, selected_features)\n",
    "        \n",
    "        print(f\"\\nEnhanced Data Split:\")\n",
    "        print(f\"  Training batches: {len(train_files)}\")\n",
    "        print(f\"  Testing batches: {len(test_files)}\")\n",
    "        \n",
    "        if len(train_files) == 0 or len(test_files) == 0:\n",
    "            print(\"Error: Invalid train/test split!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Analyze class distribution\n",
    "        train_class_dist = self.analyze_class_distribution(train_files, selected_features)\n",
    "        print(f\"\\nTraining Class Distribution Analysis:\")\n",
    "        self.display_class_distribution(train_class_dist)\n",
    "        \n",
    "        # Store distribution in metadata\n",
    "        self.training_metadata['class_distribution'] = dict(train_class_dist)\n",
    "        \n",
    "        # Training Phase\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"ENHANCED TRAINING PHASE\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for file in tqdm(train_files, desc=\"Training with class balancing\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is None or len(X_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Feature Scaling\n",
    "            if self.scaler is None:\n",
    "                print(f\"  Fitting scaler on first batch ({len(X_batch)} samples)\")\n",
    "                from sklearn.preprocessing import StandardScaler\n",
    "                self.scaler = StandardScaler()\n",
    "                X_batch = self.scaler.fit_transform(X_batch)\n",
    "            else:\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "            \n",
    "            # Track training labels\n",
    "            all_train_labels.extend(y_batch)\n",
    "            self.training_metadata['total_training_samples'] += len(y_batch)\n",
    "            \n",
    "            # Train incremental models with enhanced SGD - Check selected models\n",
    "            if 'sgd' in self.models and 'sgd' in self.selected_models:\n",
    "                try:\n",
    "                    # Manual class weight calculation for SGD partial_fit\n",
    "                    if self.use_manual_class_weights and len(np.unique(y_batch)) > 1:\n",
    "                        from sklearn.utils.class_weight import compute_class_weight\n",
    "                        \n",
    "                        # Calculate class weights for this batch\n",
    "                        unique_classes, class_counts = np.unique(y_batch, return_counts=True)\n",
    "                        if len(unique_classes) > 1:  # Only if multiple classes present\n",
    "                            class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_batch)\n",
    "                            class_weight_dict = dict(zip(unique_classes, class_weights))\n",
    "                            \n",
    "                            # Apply sample weights (approximate class balancing)\n",
    "                            sample_weights = np.array([class_weight_dict.get(y, 1.0) for y in y_batch])\n",
    "                            \n",
    "                            self.models['sgd'].partial_fit(X_batch, y_batch, classes=classes, sample_weight=sample_weights)\n",
    "                        else:\n",
    "                            # Single class in batch - no weighting needed\n",
    "                            self.models['sgd'].partial_fit(X_batch, y_batch, classes=classes)\n",
    "                    else:\n",
    "                        # No class balancing requested\n",
    "                        self.models['sgd'].partial_fit(X_batch, y_batch, classes=classes)\n",
    "                    \n",
    "                    if batch_count % 5 == 0:\n",
    "                        score = self.models['sgd'].score(X_batch, y_batch)\n",
    "                        train_scores['sgd'].append(score)\n",
    "                        self.train_history['sgd'].append({\n",
    "                            'batch': batch_count,\n",
    "                            'score': score,\n",
    "                            'samples': len(X_batch)\n",
    "                        })\n",
    "                        \n",
    "                        # Save checkpoint every 10 batches\n",
    "                        if batch_count % 10 == 0:\n",
    "                            self.save_checkpoint('sgd', batch_count)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"  Warning: SGD training error on batch {batch_count}: {e}\")\n",
    "            \n",
    "            batch_count += 1\n",
    "            self.training_metadata['batches_processed'] = batch_count\n",
    "            \n",
    "            # Memory management\n",
    "            del batch, X_batch, y_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"\\nIncremental training complete: {batch_count} batches processed\")\n",
    "        print(f\"Checkpoints saved: {len(self.training_metadata['checkpoints_saved'])}\")\n",
    "        \n",
    "        # Train Random Forest - Use selected models\n",
    "        if 'random_forest' in self.selected_models:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"ENHANCED RANDOM FOREST TRAINING\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_enhanced_random_forest(train_files, selected_features)\n",
    "        \n",
    "        # Train LightGBM - Use selected models\n",
    "        if 'lightgbm' in self.selected_models:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"LIGHTGBM TRAINING\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_lightgbm(train_files, selected_features, classes)\n",
    "        \n",
    "        # Train XGBoost - Use selected models\n",
    "        if 'xgboost_incremental' in self.selected_models:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"XGBOOST TRAINING\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_xgboost_external_memory(train_files, selected_features)\n",
    "        \n",
    "        # Comprehensive Testing Phase\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"COMPREHENSIVE TESTING PHASE\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        test_results = self.comprehensive_evaluation(test_files, selected_features, classes)\n",
    "        \n",
    "        # Generate visualizations automatically\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"GENERATING PERFORMANCE VISUALIZATIONS\")\n",
    "        print(\"-\"*40)\n",
    "        self.generate_evaluation_visualizations(test_results)\n",
    "        \n",
    "        # Store all results with metadata\n",
    "        self.results.update(test_results)\n",
    "        \n",
    "        # Record training end\n",
    "        self.training_metadata['end_time'] = datetime.now()\n",
    "        self.training_metadata['total_duration_seconds'] = time.time() - training_start_time\n",
    "        \n",
    "        # Add metadata to all model results\n",
    "        for model_name in self.results:\n",
    "            self.results[model_name]['training_metadata'] = self.training_metadata.copy()\n",
    "            self.results[model_name]['selected_features'] = selected_features\n",
    "            \n",
    "            # Add feature importance if available\n",
    "            if model_name in self.feature_importance:\n",
    "                self.results[model_name]['feature_importance'] = self.feature_importance[model_name]\n",
    "            \n",
    "            # Add training history if available\n",
    "            if model_name in self.train_history:\n",
    "                self.results[model_name]['training_history'] = self.train_history[model_name]\n",
    "        \n",
    "        # Final summary\n",
    "        all_train_labels = np.array(all_train_labels)\n",
    "        final_train_dist = Counter(all_train_labels)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total training samples: {len(all_train_labels):,}\")\n",
    "        print(f\"Training duration: {self.training_metadata['total_duration_seconds']:.1f} seconds\")\n",
    "        print(f\"Final training distribution:\")\n",
    "        total_train = len(all_train_labels)\n",
    "        benign_count = final_train_dist.get(0, 0)\n",
    "        attack_count = total_train - benign_count\n",
    "        print(f\"  BENIGN: {benign_count:,} ({benign_count/total_train:.1%})\")\n",
    "        print(f\"  ATTACKS: {attack_count:,} ({attack_count/total_train:.1%})\")\n",
    "        \n",
    "        return self.models, self.results\n",
    "    \n",
    "    def initialize_enhanced_models(self):\n",
    "        \"\"\"\n",
    "        Initialize models with enhanced configurations for comprehensive metrics\n",
    "        \"\"\"\n",
    "        if 'sgd' in self.selected_models:\n",
    "            print(\"\\nInitializing Enhanced SGD Classifier...\")\n",
    "            from sklearn.linear_model import SGDClassifier\n",
    "            \n",
    "            # Updated parameters for SGD with partial_fit\n",
    "            self.models['sgd'] = SGDClassifier(\n",
    "                loss='log_loss',  # Correct parameter for newer scikit-learn versions\n",
    "                penalty='l2',\n",
    "                alpha=0.001,\n",
    "                max_iter=1000,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                n_jobs=Config.N_JOBS\n",
    "                # Removed warm_start (conflicts with partial_fit)\n",
    "                # Removed class_weight (handled manually)\n",
    "            )\n",
    "            print(\"  Updated deprecated parameters for compatibility\")\n",
    "        \n",
    "        if 'random_forest' in self.selected_models:\n",
    "            print(\"Random Forest will be trained in batch mode...\")\n",
    "            self.models['random_forest'] = None\n",
    "        \n",
    "        if 'lightgbm' in self.selected_models:\n",
    "            self.models['lightgbm'] = None\n",
    "        \n",
    "        if 'xgboost_incremental' in self.selected_models:\n",
    "            self.models['xgboost_incremental'] = None\n",
    "    \n",
    "    def train_xgboost_external_memory(self, train_files, selected_features):\n",
    "        \"\"\"\n",
    "        Train XGBoost with in-memory approach for improved compatibility\n",
    "        \"\"\"\n",
    "        print(\"Training XGBoost with in-memory multiclass configuration...\")\n",
    "        \n",
    "        import xgboost as xgb\n",
    "        \n",
    "        xgb_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load training data in memory (same approach as LightGBM)\n",
    "            X_train_list = []\n",
    "            y_train_list = []\n",
    "            \n",
    "            # Use subset of files for memory efficiency\n",
    "            sample_files = train_files[::max(1, len(train_files) // 20)]\n",
    "            \n",
    "            for file in tqdm(sample_files, desc=\"Loading XGBoost data\"):\n",
    "                batch = pd.read_parquet(file)\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                \n",
    "                if X_batch is not None:\n",
    "                    # Limit samples per file for memory management\n",
    "                    if len(X_batch) > 10000:\n",
    "                        indices = np.random.choice(len(X_batch), 10000, replace=False)\n",
    "                        X_batch = X_batch[indices]\n",
    "                        y_batch = y_batch[indices]\n",
    "                    \n",
    "                    X_batch = self.scaler.transform(X_batch)\n",
    "                    X_train_list.append(X_batch)\n",
    "                    y_train_list.append(y_batch)\n",
    "                \n",
    "                del batch\n",
    "                gc.collect()\n",
    "            \n",
    "            if not X_train_list:\n",
    "                print(\"  No training data available for XGBoost\")\n",
    "                return\n",
    "            \n",
    "            X_train = np.vstack(X_train_list)\n",
    "            y_train = np.hstack(y_train_list)\n",
    "            \n",
    "            print(f\"  Training on {len(X_train):,} samples\")\n",
    "            \n",
    "            # Create DMatrix directly from numpy arrays\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            \n",
    "            # Enhanced parameters for multiclass classification\n",
    "            params = {\n",
    "                'max_depth': 8,\n",
    "                'eta': 0.05,\n",
    "                'objective': 'multi:softprob',  # Returns probabilities\n",
    "                'num_class': 15,\n",
    "                'tree_method': 'hist',  # More memory efficient than 'approx'\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'nthread': Config.N_JOBS if Config.N_JOBS > 0 else -1,\n",
    "                'seed': Config.RANDOM_STATE,\n",
    "                'verbosity': 1\n",
    "            }\n",
    "            \n",
    "            num_rounds = 100\n",
    "            self.models['xgboost_incremental'] = xgb.train(\n",
    "                params, dtrain, num_rounds,\n",
    "                verbose_eval=20\n",
    "            )\n",
    "            \n",
    "            # Capture feature importance for XGBoost\n",
    "            if hasattr(self.models['xgboost_incremental'], 'get_score'):\n",
    "                importance_dict = self.models['xgboost_incremental'].get_score(importance_type='gain')\n",
    "                # Map feature names directly\n",
    "                self.feature_importance['xgboost_incremental'] = {\n",
    "                    selected_features[i]: importance_dict.get(f'f{i}', 0.0) \n",
    "                    for i in range(len(selected_features))\n",
    "                }\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint('xgboost_incremental', -1)\n",
    "            \n",
    "            xgb_training_time = time.time() - xgb_start_time\n",
    "            self.train_history['xgboost_incremental'] = [{\n",
    "                'training_time_seconds': xgb_training_time,\n",
    "                'training_samples': len(X_train)\n",
    "            }]\n",
    "            \n",
    "            print(f\"  XGBoost training complete ({xgb_training_time:.1f} seconds)\")\n",
    "            print(\"  Using in-memory training instead of LibSVM\")\n",
    "            \n",
    "            del X_train, y_train, X_train_list, y_train_list\n",
    "            gc.collect()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  XGBoost training failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def create_stratified_split_fixed(self, feature_files, selected_features):\n",
    "        \"\"\"\n",
    "        Create stratified train/test split with PyArrow compatibility\n",
    "        Resolved PyArrow/pandas nrows parameter incompatibility issue that was\n",
    "        causing silent data loading failures and empty training datasets.\n",
    "        \"\"\"\n",
    "        if not Config.USE_STRATIFIED_SPLIT:\n",
    "            train_size = max(1, int(len(feature_files) * (1 - Config.TEST_SIZE)))\n",
    "            return feature_files[:train_size], feature_files[train_size:]\n",
    "        \n",
    "        print(\"Creating stratified split based on class distribution...\")\n",
    "        print(\"Using PyArrow-compatible data loading without nrows parameter\")\n",
    "        \n",
    "        file_class_info = []\n",
    "        \n",
    "        for file in feature_files:\n",
    "            try:\n",
    "                # PyArrow Parquet compatibility - removed problematic nrows parameter\n",
    "                # Previous code used nrows=1 which is not supported by PyArrow Parquet reader\n",
    "                # This was causing silent failures and empty training datasets\n",
    "                sample_batch = pd.read_parquet(file)\n",
    "                \n",
    "                # Check if label column exists\n",
    "                if 'label' not in sample_batch.columns:\n",
    "                    print(f\"  Warning: No labels found in {os.path.basename(file)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Sample if file is large for memory efficiency \n",
    "                # This replaces the failed nrows=1 optimization with a working approach\n",
    "                if len(sample_batch) > 10000:\n",
    "                    sample_batch = sample_batch.sample(10000, random_state=Config.RANDOM_STATE)\n",
    "                \n",
    "                # Process only label column for stratification analysis\n",
    "                sample_batch = sample_batch[['label']]\n",
    "                class_dist = sample_batch['label'].value_counts().to_dict()\n",
    "                file_class_info.append((file, class_dist))\n",
    "                    \n",
    "                del sample_batch\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not analyze {os.path.basename(file)}: {e}\")\n",
    "                print(f\"    This may indicate PyArrow/Parquet compatibility issues\")\n",
    "                continue\n",
    "        \n",
    "        if not file_class_info:\n",
    "            print(\"  No labels found, using simple split\")\n",
    "            train_size = max(1, int(len(feature_files) * (1 - Config.TEST_SIZE)))\n",
    "            return feature_files[:train_size], feature_files[train_size:]\n",
    "        \n",
    "        # Stratified split ensuring all classes are represented\n",
    "        train_files = []\n",
    "        test_files = []\n",
    "        \n",
    "        # Group files by dominant class\n",
    "        files_by_class = defaultdict(list)\n",
    "        for file, class_dist in file_class_info:\n",
    "            if class_dist:\n",
    "                dominant_class = max(class_dist.items(), key=lambda x: x[1])[0]\n",
    "                files_by_class[dominant_class].append(file)\n",
    "        \n",
    "        # Split each class's files\n",
    "        for class_label, class_files in files_by_class.items():\n",
    "            n_test = max(1, int(len(class_files) * Config.TEST_SIZE))\n",
    "            test_files.extend(class_files[:n_test])\n",
    "            train_files.extend(class_files[n_test:])\n",
    "        \n",
    "        # Ensure we have both train and test files\n",
    "        if not train_files and test_files:\n",
    "            train_files.append(test_files.pop())\n",
    "        elif not test_files and train_files:\n",
    "            test_files.append(train_files.pop())\n",
    "        \n",
    "        print(f\"  Stratified split created: {len(train_files)} train, {len(test_files)} test files\")\n",
    "        return train_files, test_files\n",
    "    \n",
    "    def comprehensive_evaluation(self, test_files, selected_features, classes):\n",
    "        \"\"\"\n",
    "        Enhanced evaluation with complete metrics capture and improved model prediction handling\n",
    "        \"\"\"\n",
    "        print(\"Performing comprehensive evaluation...\")\n",
    "        \n",
    "        eval_start_time = time.time()\n",
    "        \n",
    "        # Collect predictions and labels\n",
    "        all_test_labels = []\n",
    "        all_predictions = defaultdict(list)\n",
    "        all_probabilities = defaultdict(list)\n",
    "        test_batches = 0\n",
    "        \n",
    "        for file in tqdm(test_files, desc=\"Comprehensive testing\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is None or len(X_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            X_batch = self.scaler.transform(X_batch)\n",
    "            all_test_labels.extend(y_batch)\n",
    "            self.training_metadata['total_test_samples'] += len(y_batch)\n",
    "            \n",
    "            # Get predictions from each model with improved handling\n",
    "            for model_name, model in self.models.items():\n",
    "                if model is not None:\n",
    "                    try:\n",
    "                        # Handle different prediction formats properly\n",
    "                        if model_name == 'lightgbm':\n",
    "                            # LightGBM returns probabilities by default for multiclass\n",
    "                            probs = model.predict(X_batch, num_iteration=model.best_iteration)\n",
    "                            \n",
    "                            # Convert probabilities to class predictions\n",
    "                            if len(probs.shape) > 1 and probs.shape[1] > 1:  # Multi-class probabilities\n",
    "                                predictions = np.argmax(probs, axis=1)\n",
    "                                all_probabilities[model_name].append(probs)\n",
    "                            else:  # Single output or binary, threshold at 0.5\n",
    "                                predictions = (probs > 0.5).astype(int)\n",
    "                                \n",
    "                        elif model_name == 'xgboost_incremental':\n",
    "                            # XGBoost now returns probabilities, convert to class predictions\n",
    "                            import xgboost as xgb\n",
    "                            probs = model.predict(xgb.DMatrix(X_batch))\n",
    "                            if len(probs.shape) > 1:  # Multi-class probabilities\n",
    "                                predictions = np.argmax(probs, axis=1)\n",
    "                                all_probabilities[model_name].append(probs)\n",
    "                            else:  # Single prediction\n",
    "                                predictions = model.predict(xgb.DMatrix(X_batch))\n",
    "                        else:\n",
    "                            # Standard sklearn models\n",
    "                            predictions = model.predict(X_batch)\n",
    "                        \n",
    "                        all_predictions[model_name].extend(predictions)\n",
    "                        \n",
    "                        # Get probabilities if available (for ROC-AUC) - but not for already processed ones\n",
    "                        if hasattr(model, 'predict_proba') and model_name not in ['xgboost_incremental', 'lightgbm']:\n",
    "                            probs = model.predict_proba(X_batch)\n",
    "                            all_probabilities[model_name].append(probs)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  Error predicting with {model_name}: {e}\")\n",
    "            \n",
    "            test_batches += 1\n",
    "            del batch, X_batch, y_batch\n",
    "            gc.collect()\n",
    "        \n",
    "        all_test_labels = np.array(all_test_labels)\n",
    "        print(f\"Testing complete: {test_batches} batches, {len(all_test_labels):,} samples\")\n",
    "        \n",
    "        # Calculate comprehensive metrics for each model\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, predictions in all_predictions.items():\n",
    "            if not predictions:\n",
    "                continue\n",
    "                \n",
    "            predictions = np.array(predictions)\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            print(f\"COMPREHENSIVE EVALUATION: {model_name.upper()}\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Calculate all metrics\n",
    "            model_results = self.calculate_comprehensive_metrics(\n",
    "                all_test_labels, \n",
    "                predictions, \n",
    "                model_name,\n",
    "                all_probabilities.get(model_name, None)\n",
    "            )\n",
    "            \n",
    "            # Add evaluation time\n",
    "            model_results['evaluation_time_seconds'] = time.time() - eval_start_time\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "            \n",
    "            # Display results\n",
    "            self.display_comprehensive_results(model_results, model_name)\n",
    "        \n",
    "        # Overall summary\n",
    "        self.display_evaluation_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_comprehensive_metrics(self, y_true, y_pred, model_name, y_proba=None):\n",
    "        \"\"\"\n",
    "        Calculate and store ALL evaluation metrics comprehensively\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'test_samples': len(y_true),\n",
    "            'unique_predictions': len(np.unique(y_pred)),\n",
    "            'unique_true_labels': len(np.unique(y_true)),\n",
    "            'evaluation_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Basic metrics\n",
    "        results['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "        results['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        results['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        results['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        results['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        results['confusion_matrix'] = cm.tolist()\n",
    "        \n",
    "        # Attack detection metrics\n",
    "        benign_mask = (y_true == 0)\n",
    "        attack_mask = (y_true > 0)\n",
    "        \n",
    "        if attack_mask.sum() > 0:\n",
    "            attack_detected = (y_pred[attack_mask] > 0).sum()\n",
    "            results['attack_detection_rate'] = attack_detected / attack_mask.sum()\n",
    "            results['attack_true_positives'] = int(attack_detected)\n",
    "            results['attack_false_negatives'] = int(attack_mask.sum() - attack_detected)\n",
    "        else:\n",
    "            results['attack_detection_rate'] = 0\n",
    "            results['attack_true_positives'] = 0\n",
    "            results['attack_false_negatives'] = 0\n",
    "        \n",
    "        if benign_mask.sum() > 0:\n",
    "            benign_misclassified = (y_pred[benign_mask] > 0).sum()\n",
    "            results['false_positive_rate'] = benign_misclassified / benign_mask.sum()\n",
    "            results['benign_true_negatives'] = int(benign_mask.sum() - benign_misclassified)\n",
    "            results['benign_false_positives'] = int(benign_misclassified)\n",
    "        else:\n",
    "            results['false_positive_rate'] = 0\n",
    "            results['benign_true_negatives'] = 0\n",
    "            results['benign_false_positives'] = 0\n",
    "        \n",
    "        # Per-class metrics (detailed classification report)\n",
    "        unique_classes = sorted(np.unique(np.concatenate([y_true, y_pred])))\n",
    "        \n",
    "        # Map to attack names if using CICIDS\n",
    "        attack_mapping = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS_GoldenEye',\n",
    "            4: 'DoS_Hulk',\n",
    "            5: 'DoS_Slowhttptest',\n",
    "            6: 'DoS_slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web_Attack_BruteForce',\n",
    "            13: 'Web_Attack_SqlInjection',\n",
    "            14: 'Web_Attack_XSS'\n",
    "        }\n",
    "        \n",
    "        target_names = [attack_mapping.get(i, f'Class_{i}') for i in unique_classes]\n",
    "        \n",
    "        results['classification_report'] = classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=unique_classes,\n",
    "            target_names=target_names,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Per-class F1 scores for easy access\n",
    "        results['per_class_f1'] = {}\n",
    "        for i, class_name in enumerate(target_names):\n",
    "            if class_name in results['classification_report']:\n",
    "                results['per_class_f1'][class_name] = results['classification_report'][class_name]['f1-score']\n",
    "        \n",
    "        # ROC-AUC if probabilities available (for binary or OvR multiclass)\n",
    "        if y_proba is not None and len(y_proba) > 0:\n",
    "            try:\n",
    "                # Concatenate probabilities if in batches\n",
    "                if isinstance(y_proba[0], np.ndarray):\n",
    "                    y_proba_concat = np.vstack(y_proba)\n",
    "                    # For multiclass, use OvR macro average\n",
    "                    if y_proba_concat.shape[1] > 2:\n",
    "                        results['roc_auc_macro'] = roc_auc_score(\n",
    "                            y_true, y_proba_concat,\n",
    "                            multi_class='ovr',\n",
    "                            average='macro'\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not calculate ROC-AUC: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_comprehensive_results(self, results, model_name):\n",
    "        \"\"\"\n",
    "        Display comprehensive evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"\\nCore Metrics:\")\n",
    "        print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score (Macro): {results['f1_macro']:.4f}\")\n",
    "        print(f\"  F1-Score (Weighted): {results['f1_weighted']:.4f}\")\n",
    "        print(f\"  Precision (Macro): {results['precision_macro']:.4f}\")\n",
    "        print(f\"  Recall (Macro): {results['recall_macro']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nSecurity-Focused Metrics:\")\n",
    "        print(f\"  Attack Detection Rate: {results['attack_detection_rate']:.4f}\")\n",
    "        print(f\"  False Positive Rate: {results['false_positive_rate']:.4f}\")\n",
    "        \n",
    "        if 'roc_auc_macro' in results:\n",
    "            print(f\"  ROC-AUC (Macro): {results['roc_auc_macro']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nPrediction Analysis:\")\n",
    "        print(f\"  Test Samples: {results['test_samples']:,}\")\n",
    "        print(f\"  Unique Predictions: {results['unique_predictions']}\")\n",
    "        print(f\"  Unique True Labels: {results['unique_true_labels']}\")\n",
    "        \n",
    "        # Display top per-class F1 scores\n",
    "        if 'per_class_f1' in results and results['per_class_f1']:\n",
    "            print(f\"\\nTop Per-Class F1 Scores:\")\n",
    "            sorted_f1 = sorted(results['per_class_f1'].items(), key=lambda x: x[1], reverse=True)\n",
    "            for class_name, f1 in sorted_f1[:5]:\n",
    "                if f1 > 0:\n",
    "                    print(f\"  {class_name}: {f1:.4f}\")\n",
    "        \n",
    "        # Warnings\n",
    "        if results['unique_predictions'] == 1:\n",
    "            print(f\"\\n⚠️ WARNING: Model only predicts one class!\")\n",
    "        elif results['attack_detection_rate'] < Config.MINIMUM_ATTACK_DETECTION_RATE:\n",
    "            print(f\"\\n⚠️ WARNING: Low attack detection rate\")\n",
    "    \n",
    "    def display_evaluation_summary(self, results):\n",
    "        \"\"\"\n",
    "        Display overall evaluation summary\n",
    "        \"\"\"\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(\"OVERALL EVALUATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No models were successfully evaluated.\")\n",
    "            return\n",
    "        \n",
    "        # Find best model by F1-score\n",
    "        best_model_f1 = None\n",
    "        best_f1_score = 0\n",
    "        \n",
    "        print(f\"{'Model':<20} {'Accuracy':<10} {'F1-Macro':<10} {'Attack Det.':<12} {'FPR':<10} {'Status':<15}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for model_name, model_results in results.items():\n",
    "            accuracy = model_results.get('accuracy', 0)\n",
    "            f1_macro = model_results.get('f1_macro', 0)\n",
    "            attack_rate = model_results.get('attack_detection_rate', 0)\n",
    "            fpr = model_results.get('false_positive_rate', 0)\n",
    "            unique_preds = model_results.get('unique_predictions', 0)\n",
    "            \n",
    "            # Determine status\n",
    "            if unique_preds == 1:\n",
    "                status = \"❌ BROKEN\"\n",
    "            elif attack_rate < 0.3:\n",
    "                status = \"⚠️ POOR\"\n",
    "            elif attack_rate < 0.7:\n",
    "                status = \"🔶 FAIR\"\n",
    "            elif attack_rate < 0.9:\n",
    "                status = \"✅ GOOD\"\n",
    "            else:\n",
    "                status = \"🌟 EXCELLENT\"\n",
    "            \n",
    "            print(f\"{model_name:<20} {accuracy:<10.4f} {f1_macro:<10.4f} {attack_rate:<12.4f} {fpr:<10.4f} {status:<15}\")\n",
    "            \n",
    "            # Track best F1 score\n",
    "            if f1_macro > best_f1_score:\n",
    "                best_f1_score = f1_macro\n",
    "                best_model_f1 = model_name\n",
    "        \n",
    "        if best_model_f1:\n",
    "            print(f\"\\n🏆 Best Model (by F1-Score): {best_model_f1} ({best_f1_score:.4f})\")\n",
    "    \n",
    "    def generate_evaluation_visualizations(self, results):\n",
    "        \"\"\"\n",
    "        Generate all performance visualizations after evaluation\n",
    "        \"\"\"\n",
    "        viz_dir = os.path.join(Config.OUTPUT_DIR, 'model_visualizations')\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        \n",
    "        for model_name, model_results in results.items():\n",
    "            print(f\"\\nGenerating visualizations for {model_name}...\")\n",
    "            \n",
    "            # 1. Confusion Matrix Heatmap\n",
    "            if 'confusion_matrix' in model_results:\n",
    "                self.plot_confusion_matrix(model_results, model_name, viz_dir)\n",
    "            \n",
    "            # 2. Per-Class Performance Bar Chart\n",
    "            if 'per_class_f1' in model_results:\n",
    "                self.plot_per_class_performance(model_results, model_name, viz_dir)\n",
    "            \n",
    "            # 3. Feature Importance Plot\n",
    "            if 'feature_importance' in model_results:\n",
    "                self.plot_feature_importance(model_results, model_name, viz_dir)\n",
    "            \n",
    "            # 4. Metrics Summary Radar Chart\n",
    "            self.plot_metrics_radar(model_results, model_name, viz_dir)\n",
    "        \n",
    "        print(f\"\\n✅ Model performance visualizations saved to: {viz_dir}\")\n",
    "    \n",
    "    def plot_confusion_matrix(self, results, model_name, viz_dir):\n",
    "        \"\"\"Create confusion matrix heatmap\"\"\"\n",
    "        cm = np.array(results['confusion_matrix'])\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {model_name.title()}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        plt.savefig(os.path.join(viz_dir, f'confusion_matrix_{model_name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_per_class_performance(self, results, model_name, viz_dir):\n",
    "        \"\"\"Create bar chart of per-class F1 scores\"\"\"\n",
    "        if 'per_class_f1' in results and results['per_class_f1']:\n",
    "            per_class_f1 = results['per_class_f1']\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            classes = list(per_class_f1.keys())\n",
    "            f1_scores = list(per_class_f1.values())\n",
    "            \n",
    "            colors = ['green' if cls == 'BENIGN' else 'red' for cls in classes]\n",
    "            plt.bar(classes, f1_scores, color=colors, alpha=0.7)\n",
    "            plt.title(f'Per-Class F1 Scores - {model_name.title()}')\n",
    "            plt.xlabel('Attack Type')\n",
    "            plt.ylabel('F1-Score')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            plt.savefig(os.path.join(viz_dir, f'per_class_f1_{model_name}.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    def plot_feature_importance(self, results, model_name, viz_dir):\n",
    "        \"\"\"Create feature importance plot for top features\"\"\"\n",
    "        if model_name in self.feature_importance and self.feature_importance[model_name]:\n",
    "            importance = self.feature_importance[model_name]\n",
    "            \n",
    "            # Get top 15 features\n",
    "            sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "            \n",
    "            features, importances = zip(*sorted_features)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.barh(range(len(features)), importances)\n",
    "            plt.yticks(range(len(features)), features)\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'Top Feature Importance - {model_name.title()}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            \n",
    "            plt.savefig(os.path.join(viz_dir, f'feature_importance_{model_name}.png'), \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    def plot_metrics_radar(self, results, model_name, viz_dir):\n",
    "        \"\"\"Create radar chart of key metrics\"\"\"\n",
    "        metrics = {\n",
    "            'Accuracy': results.get('accuracy', 0),\n",
    "            'F1-Macro': results.get('f1_macro', 0),\n",
    "            'Precision': results.get('precision_macro', 0),\n",
    "            'Recall': results.get('recall_macro', 0),\n",
    "            'Attack Detection': results.get('attack_detection_rate', 0)\n",
    "        }\n",
    "        \n",
    "        # Setup radar chart\n",
    "        angles = [n / len(metrics) * 2 * pi for n in range(len(metrics))]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        values = list(metrics.values())\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
    "        ax.plot(angles, values, 'o-', linewidth=2)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics.keys())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title(f'Performance Metrics - {model_name.title()}', pad=20)\n",
    "        \n",
    "        plt.savefig(os.path.join(viz_dir, f'metrics_radar_{model_name}.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # [Include remaining helper methods - they were working correctly]\n",
    "    def analyze_class_distribution(self, files, selected_features):\n",
    "        \"\"\"Analyze class distribution across files\"\"\"\n",
    "        total_dist = Counter()\n",
    "        \n",
    "        for file in files:\n",
    "            try:\n",
    "                batch = pd.read_parquet(file)\n",
    "                if 'label' in batch.columns:\n",
    "                    batch_dist = Counter(batch['label'])\n",
    "                    total_dist.update(batch_dist)\n",
    "                del batch\n",
    "                gc.collect()\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return total_dist\n",
    "    \n",
    "    def display_class_distribution(self, class_dist):\n",
    "        \"\"\"Display class distribution\"\"\"\n",
    "        total = sum(class_dist.values())\n",
    "        \n",
    "        if total == 0:\n",
    "            print(\"  No labeled data found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"  Total samples: {total:,}\")\n",
    "        print(f\"  BENIGN (0): {class_dist.get(0, 0):,} ({class_dist.get(0, 0)/total:.1%})\")\n",
    "        \n",
    "        attack_total = sum(count for label, count in class_dist.items() if label > 0)\n",
    "        if attack_total > 0:\n",
    "            print(f\"  ATTACKS (1-14): {attack_total:,} ({attack_total/total:.1%})\")\n",
    "            \n",
    "            attack_types = {label: count for label, count in class_dist.items() if label > 0}\n",
    "            if attack_types:\n",
    "                top_attacks = sorted(attack_types.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                print(f\"    Top attack types:\")\n",
    "                for label, count in top_attacks:\n",
    "                    print(f\"      Label {label}: {count:,}\")\n",
    "    \n",
    "    def prepare_batch(self, batch, selected_features):\n",
    "        \"\"\"Prepare batch for training with better error handling\"\"\"\n",
    "        if 'label' not in batch.columns:\n",
    "            return None, None\n",
    "        \n",
    "        available_features = [f for f in selected_features if f in batch.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            return None, None\n",
    "        \n",
    "        if len(available_features) < len(selected_features) * 0.8:\n",
    "            print(f\"    Warning: Only {len(available_features)}/{len(selected_features)} features available\")\n",
    "        \n",
    "        X = batch[available_features].fillna(0).values\n",
    "        y = batch['label'].values\n",
    "        \n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_enhanced_random_forest(self, train_files, selected_features):\n",
    "        \"\"\"Train Random Forest with feature importance capture and early stopping\"\"\"\n",
    "        print(\"Training Random Forest with balanced classes...\")\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        \n",
    "        rf_start_time = time.time()\n",
    "        \n",
    "        # Load training data with memory management\n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        max_samples_per_file = 10000\n",
    "        total_samples = 0\n",
    "        \n",
    "        for file in tqdm(train_files, desc=\"Loading RF training data\"):\n",
    "            if total_samples >= Config.SAMPLE_SIZE:\n",
    "                break\n",
    "                \n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is not None:\n",
    "                if len(X_batch) > max_samples_per_file:\n",
    "                    indices = np.random.choice(len(X_batch), max_samples_per_file, replace=False)\n",
    "                    X_batch = X_batch[indices]\n",
    "                    y_batch = y_batch[indices]\n",
    "                \n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                X_train_list.append(X_batch)\n",
    "                y_train_list.append(y_batch)\n",
    "                total_samples += len(X_batch)\n",
    "            \n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        if not X_train_list:\n",
    "            print(\"  No training data available for Random Forest\")\n",
    "            return\n",
    "        \n",
    "        X_train = np.vstack(X_train_list)\n",
    "        y_train = np.hstack(y_train_list)\n",
    "        \n",
    "        print(f\"  Training on {len(X_train):,} samples\")\n",
    "        \n",
    "        # Calculate class weights for better balancing\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        classes = np.unique(y_train)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        class_weight_dict = dict(zip(classes, class_weights))\n",
    "        \n",
    "        # Train Random Forest with enhanced parameters\n",
    "        self.models['random_forest'] = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=class_weight_dict if Config.USE_CLASS_BALANCING else None,\n",
    "            random_state=Config.RANDOM_STATE,\n",
    "            n_jobs=Config.N_JOBS,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        self.models['random_forest'].fit(X_train, y_train)\n",
    "        \n",
    "        # Quick validation with cross-validation\n",
    "        print(\"  Performing quick validation...\")\n",
    "        cv_scores = cross_val_score(\n",
    "            self.models['random_forest'], \n",
    "            X_train[:10000] if len(X_train) > 10000 else X_train,\n",
    "            y_train[:10000] if len(y_train) > 10000 else y_train,\n",
    "            cv=3,\n",
    "            scoring='f1_macro'\n",
    "        )\n",
    "        print(f\"  Cross-validation F1-Macro: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "        \n",
    "        # Capture feature importance\n",
    "        self.feature_importance['random_forest'] = dict(\n",
    "            zip(selected_features, self.models['random_forest'].feature_importances_)\n",
    "        )\n",
    "        \n",
    "        # Store top features\n",
    "        top_features = sorted(\n",
    "            self.feature_importance['random_forest'].items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        print(\"  Top 10 Most Important Features:\")\n",
    "        for i, (feature, importance) in enumerate(top_features, 1):\n",
    "            print(f\"    {i:2d}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self.save_checkpoint('random_forest', -1)\n",
    "        \n",
    "        # Record training time\n",
    "        rf_training_time = time.time() - rf_start_time\n",
    "        self.train_history['random_forest'] = [{\n",
    "            'training_time_seconds': rf_training_time,\n",
    "            'training_samples': len(X_train),\n",
    "            'cv_f1_macro': cv_scores.mean()\n",
    "        }]\n",
    "        \n",
    "        print(f\"  Random Forest training complete ({rf_training_time:.1f} seconds)\")\n",
    "        \n",
    "        del X_train, y_train, X_train_list, y_train_list\n",
    "        gc.collect()\n",
    "    \n",
    "    def train_lightgbm(self, train_files, selected_features, classes):\n",
    "        \"\"\"Train LightGBM with metrics capture\"\"\"\n",
    "        print(\"Training LightGBM with imbalanced data optimization...\")\n",
    "        \n",
    "        import lightgbm as lgb\n",
    "        \n",
    "        lgb_start_time = time.time()\n",
    "        \n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        sample_files = train_files[::max(1, len(train_files) // 20)]\n",
    "        \n",
    "        for file in tqdm(sample_files, desc=\"Loading LightGBM data\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "            \n",
    "            if X_batch is not None:\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                X_train_list.append(X_batch)\n",
    "                y_train_list.append(y_batch)\n",
    "            \n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        if not X_train_list:\n",
    "            print(\"  No training data available for LightGBM\")\n",
    "            return\n",
    "        \n",
    "        X_train = np.vstack(X_train_list)\n",
    "        y_train = np.hstack(y_train_list)\n",
    "        \n",
    "        print(f\"  Training on {len(X_train):,} samples\")\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(classes),\n",
    "            'metric': 'multi_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'device': 'cpu',\n",
    "            'num_threads': Config.N_JOBS if Config.N_JOBS > 0 else -1,\n",
    "            'verbose': -1,\n",
    "            'max_bin': 255,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'seed': Config.RANDOM_STATE,\n",
    "            'is_unbalance': Config.USE_CLASS_BALANCING,\n",
    "            'boost_from_average': False\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        \n",
    "        self.models['lightgbm'] = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=150,\n",
    "            valid_sets=[train_data],\n",
    "            callbacks=[lgb.early_stopping(15), lgb.log_evaluation(25)]\n",
    "        )\n",
    "        \n",
    "        # Capture feature importance for LightGBM\n",
    "        if hasattr(self.models['lightgbm'], 'feature_importance'):\n",
    "            importance = self.models['lightgbm'].feature_importance(importance_type='gain')\n",
    "            self.feature_importance['lightgbm'] = dict(zip(selected_features, importance))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        self.save_checkpoint('lightgbm', -1)\n",
    "        \n",
    "        lgb_training_time = time.time() - lgb_start_time\n",
    "        self.train_history['lightgbm'] = [{\n",
    "            'training_time_seconds': lgb_training_time,\n",
    "            'training_samples': len(X_train)\n",
    "        }]\n",
    "        \n",
    "        print(f\"  LightGBM training complete ({lgb_training_time:.1f} seconds)\")\n",
    "        \n",
    "        del X_train, y_train, X_train_list, y_train_list\n",
    "        gc.collect()\n",
    "    \n",
    "    def create_libsvm_file(self, train_files, selected_features, max_rows=500000):\n",
    "        \"\"\"Convert Parquet features to LibSVM format\"\"\"\n",
    "        libsvm_file = os.path.join(Config.TEMP_DIR, 'train.libsvm')\n",
    "        \n",
    "        with open(libsvm_file, 'w') as f:\n",
    "            rows_written = 0\n",
    "            \n",
    "            for file in train_files:\n",
    "                if rows_written >= max_rows:\n",
    "                    break\n",
    "                \n",
    "                batch = pd.read_parquet(file)\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                \n",
    "                if X_batch is None:\n",
    "                    continue\n",
    "                \n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                \n",
    "                for i in range(len(X_batch)):\n",
    "                    label = int(y_batch[i])\n",
    "                    features = ' '.join([f'{j+1}:{v}' for j, v in enumerate(X_batch[i]) if v != 0])\n",
    "                    f.write(f'{label} {features}\\n')\n",
    "                \n",
    "                rows_written += len(X_batch)\n",
    "                del batch, X_batch, y_batch\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"    Created LibSVM file with {rows_written:,} samples\")\n",
    "        return libsvm_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05e1fc",
   "metadata": {},
   "source": [
    "# ### Step 9: extended Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization Suite - Data Analysis Dashboards\n",
    "\"\"\"\n",
    "PURPOSE: Create interactive visualizations to understand data and results\n",
    "This class generates comprehensive visualizations for analysis insights.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates interactive charts using Plotly\n",
    "2. Visualizes attack distributions and patterns\n",
    "3. Shows model performance comparisons\n",
    "4. Generates network communication graphs\n",
    "5. Builds an integrated HTML dashboard\n",
    "\n",
    "VISUALIZATIONS CREATED:\n",
    "1. Protocol Distribution - Pie chart of TCP/UDP/ICMP traffic\n",
    "2. Attack Distribution - Bar chart of attack type frequencies  \n",
    "3. Flow Timeline - Scatter plot of traffic over time\n",
    "4. Port Heatmap - Communication patterns between ports\n",
    "5. Feature Correlation - Correlation matrix of top features\n",
    "6. Model Comparison - Bar chart of model accuracies\n",
    "7. CICIDS Features - Special visualizations for forward/backward features\n",
    "8. Dashboard - Combined HTML view of all visualizations\n",
    "\n",
    "WHY VISUALIZATION MATTERS:\n",
    "- Helps understand data patterns before modeling\n",
    "- Identifies class imbalance issues\n",
    "- Reveals temporal attack patterns\n",
    "- Shows which features are correlated\n",
    "- Makes results interpretable for stakeholders\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Uses data sampling (max 10K points per viz)\n",
    "- Loads only required columns\n",
    "- Generates static HTML files\n",
    "- Cleans up data after each visualization\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet for data loading\n",
    "- Added CICIDS-specific visualizations\n",
    "- Optimized for large datasets\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedVisualizer:\n",
    "    def __init__(self, features_dir, ml_results, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize visualizer with data source and output location.\n",
    "        \n",
    "        Args:\n",
    "            features_dir: Directory containing feature Parquet files\n",
    "            ml_results: Dictionary of ML model results\n",
    "            output_dir: Where to save visualization files\n",
    "        \"\"\"\n",
    "        self.features_dir = features_dir\n",
    "        self.ml_results = ml_results\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = min(Config.SAMPLE_SIZE, 10000)  # Max points for visualization\n",
    "    \n",
    "    def create_all_visualizations(self):\n",
    "        \"\"\"\n",
    "        Generate all visualizations and create dashboard.\n",
    "        Each visualization is saved as a separate HTML file.\n",
    "        \"\"\"\n",
    "        if not Config.GENERATE_VISUALS:\n",
    "            print(\"Visualization disabled in config\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERATING VISUALIZATIONS (SAMPLED DATA)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Creating visualizations with {self.sample_size:,} sample points\")\n",
    "        print(\"This provides insights without loading full dataset\")\n",
    "        \n",
    "        # Load sample data for visualization\n",
    "        sample_df = self.load_visualization_sample()\n",
    "        \n",
    "        if sample_df is None or sample_df.empty:\n",
    "            print(\"No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # List of visualizations to create\n",
    "        viz_functions = [\n",
    "            (\"Protocol Distribution\", lambda: self.create_protocol_distribution(sample_df)),\n",
    "            (\"Attack Distribution\", lambda: self.create_attack_distribution(sample_df)),\n",
    "            (\"Flow Timeline\", lambda: self.create_flow_timeline(sample_df)),\n",
    "            (\"Port Heatmap\", lambda: self.create_port_heatmap(sample_df)),\n",
    "            (\"Feature Correlation\", lambda: self.create_feature_correlation(sample_df)),\n",
    "            (\"CICIDS Features\", lambda: self.create_cicids_feature_analysis(sample_df)),\n",
    "            (\"Model Performance\", lambda: self.create_model_comparison())\n",
    "        ]\n",
    "        \n",
    "        # Create each visualization\n",
    "        for name, func in tqdm(viz_functions, desc=\"Creating visualizations\"):\n",
    "            try:\n",
    "                func()\n",
    "                print(f\"  Created: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed {name}: {e}\")\n",
    "        \n",
    "        # Create integrated dashboard\n",
    "        self.create_dashboard(sample_df)\n",
    "        \n",
    "        # Clean up sample data\n",
    "        del sample_df\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\nVisualizations saved to: {self.output_dir}\")\n",
    "        print(f\"  Open dashboard.html in browser to view all charts\")\n",
    "    \n",
    "    def load_visualization_sample(self):\n",
    "        \"\"\"\n",
    "        Load a representative sample of data for visualization.\n",
    "        Samples from different parts of dataset for better representation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: Sample of features for visualization\n",
    "        \"\"\"\n",
    "        print(f\"\\nLoading sample of {self.sample_size:,} flows for visualization...\")\n",
    "        \n",
    "        sample_dfs = []\n",
    "        remaining = self.sample_size\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(self.features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        # Sample from different parts of dataset for diversity\n",
    "        sample_interval = max(1, len(feature_files) // 10)  # Sample from 10 points\n",
    "        \n",
    "        for i in range(0, len(feature_files), sample_interval):\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "            \n",
    "            file = feature_files[min(i, len(feature_files)-1)]\n",
    "            batch_sample_size = min(remaining, 1000)\n",
    "            \n",
    "            # Load sample from this batch\n",
    "            batch = pd.read_parquet(file)\n",
    "            if len(batch) > batch_sample_size:\n",
    "                batch = batch.sample(batch_sample_size, random_state=Config.RANDOM_STATE)\n",
    "            sample_dfs.append(batch)\n",
    "            remaining -= len(batch)\n",
    "        \n",
    "        if sample_dfs:\n",
    "            sample = pd.concat(sample_dfs, ignore_index=True)\n",
    "            print(f\"  Loaded {len(sample):,} flows from {len(sample_dfs)} batches\")\n",
    "            return sample\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def create_protocol_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create pie chart showing distribution of network protocols.\n",
    "        Helps understand traffic composition (TCP vs UDP vs other).\n",
    "        \"\"\"\n",
    "        if 'protocol' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Map protocol numbers to names\n",
    "        protocol_map = {\n",
    "            6: 'TCP',\n",
    "            17: 'UDP', \n",
    "            1: 'ICMP',\n",
    "            41: 'IPv6',\n",
    "            47: 'GRE'\n",
    "        }\n",
    "        \n",
    "        df['protocol_name'] = df['protocol'].map(protocol_map).fillna('Other')\n",
    "        protocol_counts = df['protocol_name'].value_counts()\n",
    "        \n",
    "        # Create pie chart\n",
    "        fig = go.Figure(data=[go.Pie(\n",
    "            labels=protocol_counts.index,\n",
    "            values=protocol_counts.values,\n",
    "            hole=0.3,  # Donut chart\n",
    "            marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96A6A6', '#FFA07A']),\n",
    "            textposition='auto',\n",
    "            textinfo='label+percent'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Protocol Distribution (Sample)\",\n",
    "            height=400,\n",
    "            width=600,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'protocol_distribution.html'))\n",
    "    \n",
    "    def create_attack_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create bar chart showing distribution of attack types.\n",
    "        Critical for understanding class balance in dataset.\n",
    "        \"\"\"\n",
    "        if 'attack_type' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_counts = df['attack_type'].value_counts().head(15)\n",
    "        \n",
    "        # Create bar chart with color coding\n",
    "        colors = ['green' if x == 'BENIGN' else 'red' for x in attack_counts.index]\n",
    "        \n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=attack_counts.index,\n",
    "            y=attack_counts.values,\n",
    "            text=attack_counts.values,\n",
    "            textposition='auto',\n",
    "            marker_color=colors,\n",
    "            hovertemplate='%{x}<br>Count: %{y}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Attack Type Distribution (Sample)\",\n",
    "            xaxis_title=\"Attack Type\",\n",
    "            yaxis_title=\"Number of Flows\",\n",
    "            xaxis_tickangle=-45,\n",
    "            height=500,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'attack_distribution.html'))\n",
    "    \n",
    "    def create_flow_timeline(self, df):\n",
    "        \"\"\"\n",
    "        Create scatter plot showing traffic patterns over time.\n",
    "        Reveals temporal patterns in attacks and normal traffic.\n",
    "        \"\"\"\n",
    "        if 'flow_duration' not in df.columns or 'total_bytes' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Further sample if still too large for smooth visualization\n",
    "        if len(df) > 1000:\n",
    "            df = df.sample(1000)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Color by attack type if available\n",
    "        if 'attack_type' in df.columns:\n",
    "            # Create trace for each attack type\n",
    "            for attack_type in df['attack_type'].unique():\n",
    "                mask = df['attack_type'] == attack_type\n",
    "                \n",
    "                # Determine color\n",
    "                color = 'green' if attack_type == 'BENIGN' else 'red'\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df[mask].index,\n",
    "                    y=df[mask]['total_bytes'],\n",
    "                    mode='markers',\n",
    "                    name=str(attack_type),\n",
    "                    marker=dict(\n",
    "                        size=np.log1p(df[mask]['total_packets']) * 2,  # Size by packet count\n",
    "                        color=color,\n",
    "                        opacity=0.6,\n",
    "                        line=dict(width=0)\n",
    "                    ),\n",
    "                    hovertemplate='Flow %{x}<br>Bytes: %{y}<br>Type: ' + str(attack_type) + '<extra></extra>'\n",
    "                ))\n",
    "        else:\n",
    "            # Single trace if no attack labels\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df['total_bytes'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='blue', opacity=0.6)\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Flow Timeline (Sample)\",\n",
    "            xaxis_title=\"Flow Index\",\n",
    "            yaxis_title=\"Total Bytes\",\n",
    "            yaxis_type=\"log\",  # Log scale for better visibility\n",
    "            height=500,\n",
    "            width=1200,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'flow_timeline.html'))\n",
    "    \n",
    "    def create_port_heatmap(self, df):\n",
    "        \"\"\"\n",
    "        Create heatmap showing communication patterns between ports.\n",
    "        Helps identify service interactions and potential scanning.\n",
    "        \"\"\"\n",
    "        if 'src_port' not in df.columns or 'dst_port' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Get top ports by frequency\n",
    "        top_src = df['src_port'].value_counts().head(15).index\n",
    "        top_dst = df['dst_port'].value_counts().head(15).index\n",
    "        \n",
    "        # Filter for top ports\n",
    "        df_filtered = df[df['src_port'].isin(top_src) & df['dst_port'].isin(top_dst)]\n",
    "        \n",
    "        # Create communication matrix\n",
    "        matrix = pd.crosstab(df_filtered['src_port'], df_filtered['dst_port'])\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=matrix.values,\n",
    "            x=[str(int(p)) for p in matrix.columns],\n",
    "            y=[str(int(p)) for p in matrix.index],\n",
    "            colorscale='Viridis',\n",
    "            hoverongaps=False,\n",
    "            hovertemplate='Src Port: %{y}<br>Dst Port: %{x}<br>Count: %{z}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Port Communication Heatmap (Top Ports)\",\n",
    "            xaxis_title=\"Destination Port\",\n",
    "            yaxis_title=\"Source Port\",\n",
    "            height=600,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'port_heatmap.html'))\n",
    "    \n",
    "    def create_feature_correlation(self, df):\n",
    "        \"\"\"\n",
    "        Create correlation matrix of top features.\n",
    "        Identifies redundant features and relationships.\n",
    "        \"\"\"\n",
    "        # Select numeric columns (excluding metadata)\n",
    "        exclude_cols = {'flow_id', 'label', 'attack_type', 'label_confidence'}\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols][:15]\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            return\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,  # Center colorscale at 0\n",
    "            colorbar=dict(title=\"Correlation\"),\n",
    "            hovertemplate='%{x}<br>%{y}<br>Correlation: %{z:.2f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Feature Correlation Matrix (Top Features)\",\n",
    "            height=700,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'feature_correlation.html'))\n",
    "    \n",
    "    def create_cicids_feature_analysis(self, df):\n",
    "        \"\"\"\n",
    "        Create CICIDS-specific visualizations for forward/backward features.\n",
    "        Shows directional traffic patterns important for CICIDS.\n",
    "        \"\"\"\n",
    "        # Check for CICIDS features\n",
    "        fwd_cols = [col for col in df.columns if 'fwd' in col.lower()]\n",
    "        bwd_cols = [col for col in df.columns if 'bwd' in col.lower()]\n",
    "        \n",
    "        if not fwd_cols or not bwd_cols:\n",
    "            return\n",
    "        \n",
    "        # Create subplot figure\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Forward vs Backward Packets', 'Forward vs Backward Bytes',\n",
    "                          'Active vs Idle Time', 'Flag Distribution'),\n",
    "            specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "                   [{'type': 'bar'}, {'type': 'bar'}]]\n",
    "        )\n",
    "        \n",
    "        # Forward vs Backward packets\n",
    "        if 'fwd_packets' in df.columns and 'bwd_packets' in df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['fwd_packets'], y=df['bwd_packets'],\n",
    "                          mode='markers', marker=dict(size=3),\n",
    "                          name='Fwd vs Bwd Packets'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Forward vs Backward bytes\n",
    "        if 'fwd_bytes' in df.columns and 'bwd_bytes' in df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=df['fwd_bytes'], y=df['bwd_bytes'],\n",
    "                          mode='markers', marker=dict(size=3),\n",
    "                          name='Fwd vs Bwd Bytes'),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Active vs Idle statistics\n",
    "        if 'active_mean' in df.columns and 'idle_mean' in df.columns:\n",
    "            active_idle_data = pd.DataFrame({\n",
    "                'Active': df['active_mean'].mean(),\n",
    "                'Idle': df['idle_mean'].mean()\n",
    "            }, index=[0])\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(x=['Active', 'Idle'],\n",
    "                      y=[active_idle_data['Active'].values[0], active_idle_data['Idle'].values[0]],\n",
    "                      name='Active/Idle'),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Flag distribution\n",
    "        flag_cols = ['syn_count', 'ack_count', 'fin_count', 'rst_count', 'psh_count']\n",
    "        flag_means = []\n",
    "        flag_names = []\n",
    "        for col in flag_cols:\n",
    "            if col in df.columns:\n",
    "                flag_means.append(df[col].mean())\n",
    "                flag_names.append(col.replace('_count', '').upper())\n",
    "        \n",
    "        if flag_means:\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=flag_names, y=flag_means, name='Flags'),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=800, width=1200, title=\"CICIDS Feature Analysis\")\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'cicids_features.html'))\n",
    "    \n",
    "    def create_model_comparison(self):\n",
    "        \"\"\"\n",
    "        Create bar chart comparing ML model performance.\n",
    "        Shows which algorithms work best for this data.\n",
    "        \"\"\"\n",
    "        if not self.ml_results:\n",
    "            return\n",
    "        \n",
    "        # Extract model names and accuracies\n",
    "        models = list(self.ml_results.keys())\n",
    "        accuracies = [self.ml_results[m]['accuracy'] for m in models]\n",
    "        \n",
    "        # Create bar chart\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=models,\n",
    "            y=accuracies,\n",
    "            text=[f\"{acc:.4f}\" for acc in accuracies],\n",
    "            textposition='auto',\n",
    "            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96A6A6'][:len(models)],\n",
    "            hovertemplate='%{x}<br>Accuracy: %{y:.4f}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Model Performance Comparison\",\n",
    "            xaxis_title=\"Model\",\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            yaxis_range=[0, 1],\n",
    "            height=400,\n",
    "            width=600\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'model_comparison.html'))\n",
    "    \n",
    "    def create_dashboard(self, sample_df):\n",
    "        \"\"\"\n",
    "        Create integrated HTML dashboard combining all visualizations.\n",
    "        Provides single-page overview of analysis results.\n",
    "        \"\"\"\n",
    "        # Calculate summary statistics\n",
    "        total_flows = self.get_total_flow_count()\n",
    "        attack_flows = sample_df[sample_df.get('label', 0) > 0].shape[0] if 'label' in sample_df.columns else 0\n",
    "        attack_pct = (attack_flows / len(sample_df) * 100) if len(sample_df) > 0 else 0\n",
    "        best_accuracy = max(r['accuracy'] for r in self.ml_results.values()) if self.ml_results else 0\n",
    "        \n",
    "        # Create dashboard HTML\n",
    "        dashboard_html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Network Analysis Dashboard</title>\n",
    "            <style>\n",
    "                body {{ \n",
    "                    font-family: 'Segoe UI', Arial, sans-serif; \n",
    "                    margin: 20px; \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    min-height: 100vh;\n",
    "                }}\n",
    "                .container {{\n",
    "                    max-width: 1400px;\n",
    "                    margin: 0 auto;\n",
    "                    background: rgba(255,255,255,0.95);\n",
    "                    border-radius: 20px;\n",
    "                    padding: 30px;\n",
    "                    box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "                }}\n",
    "                h1 {{ \n",
    "                    color: #2d3748; \n",
    "                    border-bottom: 3px solid #667eea; \n",
    "                    padding-bottom: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    font-size: 28px;\n",
    "                }}\n",
    "                .stats {{ \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    padding: 25px;\n",
    "                    border-radius: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    color: white;\n",
    "                    box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
    "                }}\n",
    "                .stats h2 {{\n",
    "                    margin-top: 0;\n",
    "                    font-size: 20px;\n",
    "                    opacity: 0.9;\n",
    "                }}\n",
    "                .stat-grid {{\n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "                    gap: 20px;\n",
    "                    margin-top: 15px;\n",
    "                }}\n",
    "                .stat-item {{\n",
    "                    background: rgba(255,255,255,0.1);\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    backdrop-filter: blur(10px);\n",
    "                }}\n",
    "                .stat-value {{\n",
    "                    font-size: 24px;\n",
    "                    font-weight: bold;\n",
    "                    margin-bottom: 5px;\n",
    "                }}\n",
    "                .stat-label {{\n",
    "                    font-size: 12px;\n",
    "                    opacity: 0.8;\n",
    "                    text-transform: uppercase;\n",
    "                }}\n",
    "                .warning {{ \n",
    "                    background: #fed7d7;\n",
    "                    color: #742a2a;\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    margin-bottom: 20px;\n",
    "                    border-left: 4px solid #fc8181;\n",
    "                }}\n",
    "                .grid {{ \n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(2, 1fr);\n",
    "                    gap: 25px;\n",
    "                    margin-top: 20px;\n",
    "                }}\n",
    "                .viz-frame {{ \n",
    "                    background: white;\n",
    "                    border-radius: 12px;\n",
    "                    padding: 15px;\n",
    "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                    transition: transform 0.3s ease;\n",
    "                }}\n",
    "                .viz-frame:hover {{\n",
    "                    transform: translateY(-5px);\n",
    "                    box-shadow: 0 8px 25px rgba(0,0,0,0.15);\n",
    "                }}\n",
    "                iframe {{ \n",
    "                    width: 100%;\n",
    "                    height: 400px;\n",
    "                    border: none;\n",
    "                    border-radius: 8px;\n",
    "                }}\n",
    "                .full-width {{ \n",
    "                    grid-column: span 2;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>CICIDS2017 Network Analysis Dashboard</h1>\n",
    "                \n",
    "                <div class=\"warning\">\n",
    "                    <strong>Note:</strong> Visualizations based on sampled data ({len(sample_df):,} flows).\n",
    "                    Full dataset contains {total_flows:,} flows.\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"stats\">\n",
    "                    <h2>Summary Statistics</h2>\n",
    "                    <div class=\"stat-grid\">\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{total_flows:,}</div>\n",
    "                            <div class=\"stat-label\">Total Flows Processed</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{attack_pct:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Attack Traffic (Sample)</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{best_accuracy:.2%}</div>\n",
    "                            <div class=\"stat-label\">Best Model Accuracy</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{psutil.virtual_memory().percent:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Memory Usage</div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"grid\">\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Protocol Distribution</h3>\n",
    "                        <iframe src=\"protocol_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Model Performance</h3>\n",
    "                        <iframe src=\"model_comparison.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Attack Type Distribution</h3>\n",
    "                        <iframe src=\"attack_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Flow Timeline Analysis</h3>\n",
    "                        <iframe src=\"flow_timeline.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Port Communication Patterns</h3>\n",
    "                        <iframe src=\"port_heatmap.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Feature Correlations</h3>\n",
    "                        <iframe src=\"feature_correlation.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>CICIDS Feature Analysis</h3>\n",
    "                        <iframe src=\"cicids_features.html\"></iframe>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <p style=\"text-align: center; color: #718096; margin-top: 30px; font-size: 14px;\">\n",
    "                    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                    Pipeline: Memory-Optimized Network Analysis V2.0\n",
    "                </p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save dashboard\n",
    "        with open(os.path.join(self.output_dir, 'dashboard.html'), 'w') as f:\n",
    "            f.write(dashboard_html)\n",
    "        \n",
    "        print(\"\\nDashboard created: dashboard.html\")\n",
    "        print(\"  Open in browser to view all visualizations\")\n",
    "    \n",
    "    def get_total_flow_count(self):\n",
    "        \"\"\"\n",
    "        Get total number of flows from Parquet files.\n",
    "        Counts rows without loading data into memory.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total flow count\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        import glob\n",
    "        feature_files = glob.glob(os.path.join(self.features_dir, 'batch_*.parquet'))\n",
    "        \n",
    "        for file in feature_files:\n",
    "            # Get row count from Parquet metadata\n",
    "            df = pd.read_parquet(file, columns=['flow_id'])\n",
    "            total += len(df)\n",
    "            del df\n",
    "            gc.collect()\n",
    "        \n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd9c7b",
   "metadata": {},
   "source": [
    "# ### Step 10: Export and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Result Exporter - Complete Metrics and Report Generation\n",
    "\"\"\"\n",
    "PURPOSE: Export all analysis results in various formats for use and sharing\n",
    "This class handles saving features, models, and reports to disk.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Exports extracted features to CSV format (in chunks)\n",
    "2. Saves trained ML models for reuse\n",
    "3. Generates comprehensive text report with multi-PCAP support\n",
    "4. Creates metadata manifest for tracking\n",
    "5. Handles cleanup of temporary files\n",
    "\n",
    "OUTPUT FILES CREATED:\n",
    "- features_chunk_*.csv: Feature data split into manageable files\n",
    "- *.pkl: Serialized ML models\n",
    "- scaler.pkl: Feature scaler for inference\n",
    "- analysis_report.txt: Detailed text report\n",
    "- manifest.json: Metadata about the analysis\n",
    "- dashboard.html: Interactive visualization dashboard\n",
    "\n",
    "WHY CHUNKED EXPORT:\n",
    "- CSV files have size limits\n",
    "- Easier to transfer and share\n",
    "- Can be processed separately\n",
    "- Prevents memory overflow during export\n",
    "\n",
    "REPORT CONTENTS:\n",
    "- Processing statistics\n",
    "- Label distribution  \n",
    "- Feature importance\n",
    "- Model performance\n",
    "- Memory usage\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Reading from Parquet instead of HDF5\n",
    "- Exports feature scaler for inference\n",
    "- Better handling of large exports\n",
    "- Includes CICIDS-specific statistics\n",
    "- Fixed multi-PCAP reporting in analysis report\n",
    "\n",
    "ENHANCEMENTS:\n",
    "- Exports metrics to JSON for easy loading\n",
    "- Exports confusion matrices as CSV\n",
    "- Generates per-class performance CSV\n",
    "- Creates comprehensive text report with all metrics\n",
    "- Exports feature importance rankings\n",
    "- Creates thesis-ready summary CSV\n",
    "- Includes training history and metadata\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class MemoryOptimizedResultExporter:\n",
    "    def __init__(self, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize exporter with output directory\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for organized export\n",
    "        self.metrics_dir = os.path.join(output_dir, 'metrics')\n",
    "        self.models_dir = os.path.join(output_dir, 'models')\n",
    "        self.reports_dir = os.path.join(output_dir, 'reports')\n",
    "        \n",
    "        os.makedirs(self.metrics_dir, exist_ok=True)\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "        os.makedirs(self.reports_dir, exist_ok=True)\n",
    "    \n",
    "    def export_all(self, features_dir, models, ml_results, selected_features, scaler=None):\n",
    "        \"\"\"\n",
    "        Main export function - coordinates all export operations\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPORTING ALL RESULTS WITH COMPLETE METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Export features if requested\n",
    "        if Config.ML_EXPORT:\n",
    "            self.export_features_chunked(features_dir)\n",
    "        \n",
    "        # Export models and scaler\n",
    "        if models:\n",
    "            self.export_models(models, scaler, selected_features)\n",
    "        \n",
    "        # Export comprehensive metrics\n",
    "        if ml_results:\n",
    "            self.export_metrics_json(ml_results)\n",
    "            self.export_confusion_matrices(ml_results)\n",
    "            self.export_per_class_metrics(ml_results)\n",
    "            self.export_feature_importance(ml_results)\n",
    "            self.export_thesis_summary(ml_results)\n",
    "            self.export_training_history(ml_results)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        self.generate_comprehensive_report(features_dir, ml_results, selected_features)\n",
    "        \n",
    "        # Create export manifest\n",
    "        self.create_export_manifest(ml_results)\n",
    "        \n",
    "        print(f\"\\n✓ All results exported to: {self.output_dir}\")\n",
    "        self.print_export_summary()\n",
    "    \n",
    "    def export_features_chunked(self, features_dir, chunk_size=100000):\n",
    "        \"\"\"\n",
    "        Export features from Parquet to CSV in chunks\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING FEATURES IN CHUNKS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Exporting to CSV with max {chunk_size:,} rows per file\")\n",
    "        \n",
    "        chunk_counter = 0\n",
    "        total_rows = 0\n",
    "        file_list = []\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        features_export_dir = os.path.join(self.output_dir, 'features')\n",
    "        os.makedirs(features_export_dir, exist_ok=True)\n",
    "        \n",
    "        for file in tqdm(feature_files, desc=\"Exporting chunks\"):\n",
    "            batch = pd.read_parquet(file)\n",
    "            \n",
    "            for start_idx in range(0, len(batch), chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(batch))\n",
    "                chunk = batch.iloc[start_idx:end_idx]\n",
    "                \n",
    "                chunk_file = os.path.join(\n",
    "                    features_export_dir,\n",
    "                    f'features_chunk_{chunk_counter:03d}.csv'\n",
    "                )\n",
    "                \n",
    "                chunk.to_csv(chunk_file + '.gz', index=False, compression='gzip')\n",
    "                \n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(chunk_file) + '.gz',\n",
    "                    'rows': len(chunk),\n",
    "                    'size_mb': os.path.getsize(chunk_file + '.gz') / (1024*1024)\n",
    "                }\n",
    "                file_list.append(file_info)\n",
    "                \n",
    "                chunk_counter += 1\n",
    "                total_rows += len(chunk)\n",
    "            \n",
    "            del batch\n",
    "            gc.collect()\n",
    "        \n",
    "        print(f\"✓ Exported {total_rows:,} flows in {chunk_counter} chunks\")\n",
    "        \n",
    "        # Create features manifest\n",
    "        manifest = {\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'total_rows': total_rows,\n",
    "            'total_chunks': chunk_counter,\n",
    "            'files': file_list\n",
    "        }\n",
    "        \n",
    "        manifest_file = os.path.join(features_export_dir, 'features_manifest.json')\n",
    "        with open(manifest_file, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    def export_models(self, models, scaler=None, selected_features=None):\n",
    "        \"\"\"\n",
    "        Save trained ML models and associated objects\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING ML MODELS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Export each model\n",
    "        for model_name, model in models.items():\n",
    "            if model is not None:\n",
    "                model_path = os.path.join(self.models_dir, f'{model_name}.pkl')\n",
    "                \n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                \n",
    "                size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "                print(f\"✓ Saved {model_name}: {size_mb:.2f} MB\")\n",
    "        \n",
    "        # Export scaler\n",
    "        if scaler is not None:\n",
    "            scaler_path = os.path.join(self.models_dir, 'scaler.pkl')\n",
    "            with open(scaler_path, 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "            print(f\"✓ Saved feature scaler\")\n",
    "        \n",
    "        # Export selected features\n",
    "        if selected_features is not None:\n",
    "            features_path = os.path.join(self.models_dir, 'selected_features.pkl')\n",
    "            with open(features_path, 'wb') as f:\n",
    "                pickle.dump(selected_features, f)\n",
    "            print(f\"✓ Saved {len(selected_features)} selected feature names\")\n",
    "            \n",
    "            # Also save as readable text file\n",
    "            features_txt_path = os.path.join(self.models_dir, 'selected_features.txt')\n",
    "            with open(features_txt_path, 'w') as f:\n",
    "                for feature in selected_features:\n",
    "                    f.write(f\"{feature}\\n\")\n",
    "    \n",
    "    def export_metrics_json(self, ml_results):\n",
    "        \"\"\"\n",
    "        Export all metrics to JSON format for easy loading\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"Exporting metrics to JSON...\")\n",
    "        \n",
    "        metrics_file = os.path.join(self.metrics_dir, 'training_metrics.json')\n",
    "        \n",
    "        # Clean metrics for JSON serialization\n",
    "        metrics_clean = {}\n",
    "        for model_name, results in ml_results.items():\n",
    "            metrics_clean[model_name] = {}\n",
    "            \n",
    "            for key, value in results.items():\n",
    "                # Skip complex objects\n",
    "                if key in ['training_metadata', 'classification_report', 'confusion_matrix', \n",
    "                          'per_class_f1', 'feature_importance', 'training_history']:\n",
    "                    # Handle these separately\n",
    "                    if key == 'classification_report' and isinstance(value, dict):\n",
    "                        metrics_clean[model_name][key] = value\n",
    "                    elif key == 'confusion_matrix' and isinstance(value, (list, np.ndarray)):\n",
    "                        metrics_clean[model_name][key] = np.array(value).tolist()\n",
    "                    elif key == 'per_class_f1' and isinstance(value, dict):\n",
    "                        metrics_clean[model_name][key] = value\n",
    "                    elif key == 'feature_importance' and isinstance(value, dict):\n",
    "                        # Save top 20 features\n",
    "                        top_features = sorted(value.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "                        metrics_clean[model_name]['top_20_features'] = dict(top_features)\n",
    "                    elif key == 'training_metadata' and isinstance(value, dict):\n",
    "                        metrics_clean[model_name][key] = value\n",
    "                    continue\n",
    "                \n",
    "                # Convert numpy types to Python types\n",
    "                if isinstance(value, (np.integer, np.floating)):\n",
    "                    metrics_clean[model_name][key] = float(value)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    metrics_clean[model_name][key] = value.tolist()\n",
    "                elif isinstance(value, (list, dict, str, int, float, bool)):\n",
    "                    metrics_clean[model_name][key] = value\n",
    "        \n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(metrics_clean, f, indent=2 , default=str)\n",
    "        \n",
    "        print(f\"✓ Exported metrics to: {os.path.basename(metrics_file)}\")\n",
    "    \n",
    "    def export_confusion_matrices(self, ml_results):\n",
    "        \"\"\"\n",
    "        Export confusion matrices as CSV files\n",
    "        \"\"\"\n",
    "        print(\"Exporting confusion matrices...\")\n",
    "        \n",
    "        for model_name, results in ml_results.items():\n",
    "            if 'confusion_matrix' in results:\n",
    "                cm = np.array(results['confusion_matrix'])\n",
    "                \n",
    "                # Create DataFrame with labels\n",
    "                n_classes = cm.shape[0]\n",
    "                labels = [f'Class_{i}' for i in range(n_classes)]\n",
    "                \n",
    "                # Map to attack names if CICIDS\n",
    "                attack_mapping = {\n",
    "                    0: 'BENIGN',\n",
    "                    1: 'Bot',\n",
    "                    2: 'DDoS',\n",
    "                    3: 'DoS_GoldenEye',\n",
    "                    4: 'DoS_Hulk',\n",
    "                    5: 'DoS_Slowhttptest',\n",
    "                    6: 'DoS_slowloris',\n",
    "                    7: 'FTP-Patator',\n",
    "                    8: 'Heartbleed',\n",
    "                    9: 'Infiltration',\n",
    "                    10: 'PortScan',\n",
    "                    11: 'SSH-Patator',\n",
    "                    12: 'Web_BruteForce',\n",
    "                    13: 'Web_SqlInjection',\n",
    "                    14: 'Web_XSS'\n",
    "                }\n",
    "                \n",
    "                labels = [attack_mapping.get(i, f'Class_{i}') for i in range(n_classes)]\n",
    "                \n",
    "                cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "                cm_df.index.name = 'True Label'\n",
    "                cm_df.columns.name = 'Predicted Label'\n",
    "                \n",
    "                # Save confusion matrix\n",
    "                cm_file = os.path.join(self.metrics_dir, f'confusion_matrix_{model_name}.csv')\n",
    "                cm_df.to_csv(cm_file)\n",
    "                \n",
    "                print(f\"✓ Exported confusion matrix for {model_name}\")\n",
    "    \n",
    "    def export_per_class_metrics(self, ml_results):\n",
    "        \"\"\"\n",
    "        Export detailed per-class metrics to CSV\n",
    "        \"\"\"\n",
    "        print(\"Exporting per-class metrics...\")\n",
    "        \n",
    "        for model_name, results in ml_results.items():\n",
    "            if 'classification_report' in results and isinstance(results['classification_report'], dict):\n",
    "                report = results['classification_report']\n",
    "                \n",
    "                # Extract per-class metrics\n",
    "                metrics_data = []\n",
    "                \n",
    "                for class_name, metrics in report.items():\n",
    "                    if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "                        metrics_data.append({\n",
    "                            'Class': class_name,\n",
    "                            'Precision': metrics.get('precision', 0),\n",
    "                            'Recall': metrics.get('recall', 0),\n",
    "                            'F1-Score': metrics.get('f1-score', 0),\n",
    "                            'Support': metrics.get('support', 0)\n",
    "                        })\n",
    "                \n",
    "                if metrics_data:\n",
    "                    df = pd.DataFrame(metrics_data)\n",
    "                    \n",
    "                    # Sort by F1-Score\n",
    "                    df = df.sort_values('F1-Score', ascending=False)\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    metrics_file = os.path.join(self.metrics_dir, f'per_class_metrics_{model_name}.csv')\n",
    "                    df.to_csv(metrics_file, index=False)\n",
    "                    \n",
    "                    print(f\"✓ Exported per-class metrics for {model_name}\")\n",
    "    \n",
    "    def export_feature_importance(self, ml_results):\n",
    "        \"\"\"\n",
    "        Export feature importance rankings\n",
    "        \"\"\"\n",
    "        print(\"Exporting feature importance...\")\n",
    "        \n",
    "        for model_name, results in ml_results.items():\n",
    "            if 'feature_importance' in results and results['feature_importance']:\n",
    "                importance = results['feature_importance']\n",
    "                \n",
    "                # Sort by importance\n",
    "                sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Create DataFrame\n",
    "                df = pd.DataFrame(sorted_features, columns=['Feature', 'Importance'])\n",
    "                df['Rank'] = range(1, len(df) + 1)\n",
    "                \n",
    "                # Reorder columns\n",
    "                df = df[['Rank', 'Feature', 'Importance']]\n",
    "                \n",
    "                # Save to CSV\n",
    "                importance_file = os.path.join(self.metrics_dir, f'feature_importance_{model_name}.csv')\n",
    "                df.to_csv(importance_file, index=False)\n",
    "                \n",
    "                print(f\"✓ Exported feature importance for {model_name}\")\n",
    "    \n",
    "    def export_thesis_summary(self, ml_results):\n",
    "        \"\"\"\n",
    "        Create a single summary CSV with key metrics for thesis tables\n",
    "        \"\"\"\n",
    "        print(\"Creating thesis summary...\")\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for model_name, results in ml_results.items():\n",
    "            row = {\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Accuracy': results.get('accuracy', 0),\n",
    "                'F1_Macro': results.get('f1_macro', 0),\n",
    "                'F1_Weighted': results.get('f1_weighted', 0),\n",
    "                'Precision_Macro': results.get('precision_macro', 0),\n",
    "                'Recall_Macro': results.get('recall_macro', 0),\n",
    "                'Attack_Detection_Rate': results.get('attack_detection_rate', 0),\n",
    "                'False_Positive_Rate': results.get('false_positive_rate', 0),\n",
    "                'Test_Samples': results.get('test_samples', 0),\n",
    "                'Training_Samples': results.get('training_metadata', {}).get('total_training_samples', 0),\n",
    "                'Unique_Predictions': results.get('unique_predictions', 0),\n",
    "                'Training_Time_Seconds': results.get('training_metadata', {}).get('total_duration_seconds', 0)\n",
    "            }\n",
    "            \n",
    "            # Add ROC-AUC if available\n",
    "            if 'roc_auc_macro' in results:\n",
    "                row['ROC_AUC_Macro'] = results['roc_auc_macro']\n",
    "            \n",
    "            summary_data.append(row)\n",
    "        \n",
    "        if summary_data:\n",
    "            df = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Sort by F1_Macro score\n",
    "            df = df.sort_values('F1_Macro', ascending=False)\n",
    "            \n",
    "            # Format numeric columns\n",
    "            numeric_cols = ['Accuracy', 'F1_Macro', 'F1_Weighted', 'Precision_Macro', \n",
    "                          'Recall_Macro', 'Attack_Detection_Rate', 'False_Positive_Rate']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].round(4)\n",
    "            \n",
    "            # Save to CSV\n",
    "            summary_file = os.path.join(self.metrics_dir, 'thesis_summary.csv')\n",
    "            df.to_csv(summary_file, index=False)\n",
    "            \n",
    "            # Also save as LaTeX table for thesis\n",
    "            latex_file = os.path.join(self.metrics_dir, 'thesis_summary.tex')\n",
    "            with open(latex_file, 'w') as f:\n",
    "                f.write(df.to_latex(index=False, float_format='%.4f'))\n",
    "            \n",
    "            print(f\"✓ Exported thesis summary\")\n",
    "    \n",
    "    def export_training_history(self, ml_results):\n",
    "        \"\"\"\n",
    "        Export training history and progression\n",
    "        \"\"\"\n",
    "        print(\"Exporting training history...\")\n",
    "        \n",
    "        for model_name, results in ml_results.items():\n",
    "            if 'training_history' in results and results['training_history']:\n",
    "                history = results['training_history']\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame(history)\n",
    "                \n",
    "                # Save to CSV\n",
    "                history_file = os.path.join(self.metrics_dir, f'training_history_{model_name}.csv')\n",
    "                df.to_csv(history_file, index=False)\n",
    "                \n",
    "                print(f\"✓ Exported training history for {model_name}\")\n",
    "    \n",
    "    def generate_comprehensive_report(self, features_dir, ml_results, selected_features):\n",
    "        \"\"\"\n",
    "        Generate comprehensive text report with all metrics\n",
    "        \"\"\"\n",
    "        report_path = os.path.join(self.reports_dir, 'analysis_report.txt')\n",
    "        \n",
    "        # Collect statistics\n",
    "        stats = self.collect_statistics(features_dir)\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            # Header\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"COMPREHENSIVE NETWORK PACKET ANALYSIS REPORT\\n\")\n",
    "            f.write(\"Memory-Optimized Pipeline V2.0 for CICIDS2017\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            # System Information\n",
    "            f.write(\"SYSTEM INFORMATION\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\\n\")\n",
    "            f.write(f\"CPU: {psutil.cpu_count()} cores\\n\")\n",
    "            f.write(f\"Storage Format: {Config.STORAGE_FORMAT}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Input Configuration\n",
    "            f.write(\"INPUT CONFIGURATION\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            \n",
    "            if Config.PCAP_FILES and len(Config.PCAP_FILES) > 1:\n",
    "                f.write(f\"PCAP Files: {len(Config.PCAP_FILES)} files processed\\n\")\n",
    "                total_size_gb = 0\n",
    "                for i, pcap in enumerate(Config.PCAP_FILES, 1):\n",
    "                    if os.path.exists(pcap):\n",
    "                        size_gb = os.path.getsize(pcap) / (1024**3)\n",
    "                        total_size_gb += size_gb\n",
    "                        f.write(f\"  {i}. {os.path.basename(pcap)} ({size_gb:.2f} GB)\\n\")\n",
    "                f.write(f\"Total PCAP Size: {total_size_gb:.2f} GB\\n\")\n",
    "            elif Config.PCAP_FILE:\n",
    "                f.write(f\"PCAP File: {os.path.basename(Config.PCAP_FILE)}\\n\")\n",
    "                if os.path.exists(Config.PCAP_FILE):\n",
    "                    f.write(f\"File Size: {os.path.getsize(Config.PCAP_FILE) / (1024**3):.2f} GB\\n\")\n",
    "            \n",
    "            if Config.LABEL_FILES:\n",
    "                f.write(f\"Label Files: {len(Config.LABEL_FILES)} CSV files\\n\")\n",
    "            \n",
    "            f.write(f\"Analysis Mode: {Config.ANALYSIS_MODE}\\n\")\n",
    "            f.write(f\"Deep Inspection: {Config.DEEP_INSPECTION}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Processing Statistics\n",
    "            f.write(\"PROCESSING STATISTICS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total Flows: {stats['total_flows']:,}\\n\")\n",
    "            f.write(f\"Processing Strategy: {'Disk-based' if Config.USE_DISK_CACHE else 'Memory-based'}\\n\")\n",
    "            f.write(f\"Max Memory Setting: {Config.MAX_MEMORY_GB:.1f} GB\\n\")\n",
    "            f.write(f\"Class Balancing: {'ENABLED' if Config.USE_CLASS_BALANCING else 'DISABLED'}\\n\")\n",
    "            f.write(f\"Stratified Split: {'ENABLED' if Config.USE_STRATIFIED_SPLIT else 'DISABLED'}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Label Distribution\n",
    "            if stats.get('label_distribution'):\n",
    "                f.write(\"ATTACK TYPE DISTRIBUTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                total = sum(stats['label_distribution'].values())\n",
    "                for label, count in sorted(stats['label_distribution'].items(), \n",
    "                                          key=lambda x: x[1], reverse=True):\n",
    "                    pct = count / total * 100 if total > 0 else 0\n",
    "                    f.write(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\\n\")\n",
    "                f.write(f\"\\nTotal Labeled Flows: {total:,}\\n\")\n",
    "                \n",
    "                benign_ratio = stats['label_distribution'].get('BENIGN', 0) / total\n",
    "                attack_ratio = 1 - benign_ratio\n",
    "                f.write(f\"Attack Traffic Ratio: {attack_ratio:.2%}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Feature Selection\n",
    "            if selected_features:\n",
    "                f.write(\"FEATURE SELECTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                f.write(f\"Features Analyzed: {stats.get('total_features', 'Unknown')}\\n\")\n",
    "                f.write(f\"Features Selected: {len(selected_features)}\\n\")\n",
    "                \n",
    "                cicids_features = [f for f in selected_features if any(x in f.lower() \n",
    "                                  for x in ['fwd', 'bwd', 'active', 'idle', 'iat'])]\n",
    "                f.write(f\"CICIDS-specific Features: {len(cicids_features)}\\n\")\n",
    "                \n",
    "                f.write(f\"\\nTop 10 Selected Features:\\n\")\n",
    "                for i, feature in enumerate(selected_features[:10], 1):\n",
    "                    f.write(f\"  {i:2d}. {feature}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Machine Learning Results (Enhanced)\n",
    "            if ml_results:\n",
    "                f.write(\"=\"*70 + \"\\n\")\n",
    "                f.write(\"MACHINE LEARNING RESULTS (COMPREHENSIVE)\\n\")\n",
    "                f.write(\"=\"*70 + \"\\n\\n\")\n",
    "                \n",
    "                # Find best model\n",
    "                best_model = max(ml_results.items(), \n",
    "                               key=lambda x: x[1].get('f1_macro', 0))\n",
    "                \n",
    "                for model_name, results in ml_results.items():\n",
    "                    is_best = model_name == best_model[0]\n",
    "                    marker = \" BEST MODEL\" if is_best else \"\"\n",
    "                    \n",
    "                    f.write(f\"{model_name.upper()}{marker}\\n\")\n",
    "                    f.write(\"-\"*50 + \"\\n\")\n",
    "                    \n",
    "                    # Core Metrics\n",
    "                    f.write(\"Core Performance Metrics:\\n\")\n",
    "                    f.write(f\"  Test Accuracy: {results.get('accuracy', 0):.4f}\\n\")\n",
    "                    f.write(f\"  F1-Score (Macro): {results.get('f1_macro', 0):.4f}\\n\")\n",
    "                    f.write(f\"  F1-Score (Weighted): {results.get('f1_weighted', 0):.4f}\\n\")\n",
    "                    f.write(f\"  Precision (Macro): {results.get('precision_macro', 0):.4f}\\n\")\n",
    "                    f.write(f\"  Recall (Macro): {results.get('recall_macro', 0):.4f}\\n\")\n",
    "                    \n",
    "                    if 'roc_auc_macro' in results:\n",
    "                        f.write(f\"  ROC-AUC (Macro): {results['roc_auc_macro']:.4f}\\n\")\n",
    "                    \n",
    "                    # Security Metrics\n",
    "                    f.write(\"\\nSecurity-Focused Metrics:\\n\")\n",
    "                    f.write(f\"  Attack Detection Rate: {results.get('attack_detection_rate', 0):.4f}\\n\")\n",
    "                    f.write(f\"  False Positive Rate: {results.get('false_positive_rate', 0):.4f}\\n\")\n",
    "                    f.write(f\"  True Positives (Attacks): {results.get('attack_true_positives', 0)}\\n\")\n",
    "                    f.write(f\"  False Negatives (Missed): {results.get('attack_false_negatives', 0)}\\n\")\n",
    "                    f.write(f\"  True Negatives (Benign): {results.get('benign_true_negatives', 0)}\\n\")\n",
    "                    f.write(f\"  False Positives (Benign as Attack): {results.get('benign_false_positives', 0)}\\n\")\n",
    "                    \n",
    "                    # Data Statistics\n",
    "                    f.write(\"\\nData Statistics:\\n\")\n",
    "                    f.write(f\"  Test Samples: {results.get('test_samples', 0):,}\\n\")\n",
    "                    f.write(f\"  Unique Predictions: {results.get('unique_predictions', 0)}\\n\")\n",
    "                    f.write(f\"  Unique True Labels: {results.get('unique_true_labels', 0)}\\n\")\n",
    "                    \n",
    "                    # Training Metadata\n",
    "                    if 'training_metadata' in results:\n",
    "                        metadata = results['training_metadata']\n",
    "                        f.write(\"\\nTraining Information:\\n\")\n",
    "                        f.write(f\"  Training Samples: {metadata.get('total_training_samples', 0):,}\\n\")\n",
    "                        f.write(f\"  Training Duration: {metadata.get('total_duration_seconds', 0):.1f} seconds\\n\")\n",
    "                        f.write(f\"  Batches Processed: {metadata.get('batches_processed', 0)}\\n\")\n",
    "                    \n",
    "                    # Top Per-Class Performance\n",
    "                    if 'per_class_f1' in results and results['per_class_f1']:\n",
    "                        f.write(\"\\nTop Per-Class F1 Scores:\\n\")\n",
    "                        sorted_f1 = sorted(results['per_class_f1'].items(), \n",
    "                                         key=lambda x: x[1], reverse=True)\n",
    "                        for class_name, f1 in sorted_f1[:5]:\n",
    "                            if f1 > 0:\n",
    "                                f.write(f\"  {class_name}: {f1:.4f}\\n\")\n",
    "                    \n",
    "                    # Top Feature Importance\n",
    "                    if 'feature_importance' in results and results['feature_importance']:\n",
    "                        f.write(\"\\nTop 10 Important Features:\\n\")\n",
    "                        sorted_features = sorted(results['feature_importance'].items(),\n",
    "                                               key=lambda x: x[1], reverse=True)[:10]\n",
    "                        for i, (feature, importance) in enumerate(sorted_features, 1):\n",
    "                            f.write(f\"  {i:2d}. {feature}: {importance:.4f}\\n\")\n",
    "                    \n",
    "                    # Model Status Assessment\n",
    "                    unique_preds = results.get('unique_predictions', 0)\n",
    "                    attack_rate = results.get('attack_detection_rate', 0)\n",
    "                    \n",
    "                    f.write(\"\\nModel Assessment:\\n\")\n",
    "                    if unique_preds == 1:\n",
    "                        f.write(\"  STATUS:  BROKEN (only predicts one class)\\n\")\n",
    "                    elif attack_rate > 0.9:\n",
    "                        f.write(\"  STATUS:  EXCELLENT (>90% attack detection)\\n\")\n",
    "                    elif attack_rate > 0.7:\n",
    "                        f.write(\"  STATUS:   GOOD (>70% attack detection)\\n\")\n",
    "                    elif attack_rate > 0.3:\n",
    "                        f.write(\"  STATUS:   FAIR (>30% attack detection)\\n\")\n",
    "                    else:\n",
    "                        f.write(\"  STATUS:  POOR (<30% attack detection)\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                f.write(f\"\\nBest Model: {best_model[0]} (F1-Macro: {best_model[1].get('f1_macro', 0):.4f})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Memory Usage\n",
    "            mem = psutil.virtual_memory()\n",
    "            f.write(\"MEMORY USAGE\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Current Memory: {mem.percent:.1f}%\\n\")\n",
    "            f.write(f\"Available: {mem.available / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Used: {mem.used / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Total System: {mem.total / (1024**3):.2f} GB\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Analysis Insights & Recommendations\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"ANALYSIS INSIGHTS & RECOMMENDATIONS\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            \n",
    "            if Config.PCAP_FILES and len(Config.PCAP_FILES) > 1:\n",
    "                f.write(\" Multi-Day Training: ENABLED\\n\")\n",
    "                f.write(\"   Provides balanced attack examples for better detection\\n\\n\")\n",
    "            \n",
    "            if stats.get('label_distribution'):\n",
    "                benign_ratio = stats['label_distribution'].get('BENIGN', 0) / stats['total_flows']\n",
    "                if benign_ratio > 0.95:\n",
    "                    f.write(\"  Very high class imbalance (>95% BENIGN)\\n\")\n",
    "                    f.write(\"   Consider: Using more attack-rich CICIDS days\\n\\n\")\n",
    "                elif benign_ratio > 0.9:\n",
    "                    f.write(\"  High class imbalance (90-95% BENIGN)\\n\")\n",
    "                    f.write(\"   Class balancing is helping mitigate this\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\" Good class balance for training\\n\\n\")\n",
    "            \n",
    "            if ml_results:\n",
    "                avg_f1 = np.mean([r.get('f1_macro', 0) for r in ml_results.values()])\n",
    "                avg_attack_detection = np.mean([r.get('attack_detection_rate', 0) for r in ml_results.values()])\n",
    "                \n",
    "                if avg_attack_detection > 0.9:\n",
    "                    f.write(\"  Excellent average attack detection (>90%)\\n\")\n",
    "                    f.write(\"   Ready for deployment consideration\\n\\n\")\n",
    "                elif avg_attack_detection > 0.7:\n",
    "                    f.write(\" Good average attack detection (70-90%)\\n\")\n",
    "                    f.write(\"   Consider fine-tuning for production\\n\\n\")\n",
    "                else:\n",
    "                    f.write(\" Moderate attack detection (<70%)\\n\")\n",
    "                    f.write(\"   Consider different algorithms or more data\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Storage Format: {' Parquet (optimal)' if Config.STORAGE_FORMAT == 'parquet' else 'Consider Parquet'}\\n\")\n",
    "            f.write(f\"Class Balancing: {' ENABLED' if Config.USE_CLASS_BALANCING else '⚠️  DISABLED'}\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            f.write(\"END OF COMPREHENSIVE REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"✓ Comprehensive report saved to: {report_path}\")\n",
    "    \n",
    "    def collect_statistics(self, features_dir):\n",
    "        \"\"\"\n",
    "        Collect statistics from Parquet files\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_flows': 0,\n",
    "            'total_features': 0,\n",
    "            'label_distribution': Counter()\n",
    "        }\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in feature_files:\n",
    "            try:\n",
    "                df_sample = pd.read_parquet(file, columns=['flow_id'] if 'flow_id' in pd.read_parquet(file, nrows=0).columns else [])\n",
    "                stats['total_flows'] += len(df_sample)\n",
    "                \n",
    "                if stats['total_features'] == 0:\n",
    "                    df_first = pd.read_parquet(file, nrows=0)\n",
    "                    stats['total_features'] = len(df_first.columns)\n",
    "                \n",
    "                df_full = pd.read_parquet(file)\n",
    "                if 'attack_type' in df_full.columns:\n",
    "                    stats['label_distribution'].update(\n",
    "                        df_full['attack_type'].value_counts().to_dict()\n",
    "                    )\n",
    "                \n",
    "                del df_sample, df_full\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def create_export_manifest(self, ml_results):\n",
    "        \"\"\"\n",
    "        Create a manifest file listing all exported files\n",
    "        \"\"\"\n",
    "        manifest = {\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'pipeline_version': '2.0',\n",
    "            'export_directory': self.output_dir,\n",
    "            'exported_files': {\n",
    "                'models': [],\n",
    "                'metrics': [],\n",
    "                'reports': [],\n",
    "                'features': []\n",
    "            },\n",
    "            'model_performance': {}\n",
    "        }\n",
    "        \n",
    "        # List model files\n",
    "        for file in os.listdir(self.models_dir):\n",
    "            manifest['exported_files']['models'].append(file)\n",
    "        \n",
    "        # List metric files\n",
    "        for file in os.listdir(self.metrics_dir):\n",
    "            manifest['exported_files']['metrics'].append(file)\n",
    "        \n",
    "        # List report files\n",
    "        for file in os.listdir(self.reports_dir):\n",
    "            manifest['exported_files']['reports'].append(file)\n",
    "        \n",
    "        # Add model performance summary\n",
    "        for model_name, results in ml_results.items():\n",
    "            manifest['model_performance'][model_name] = {\n",
    "                'accuracy': results.get('accuracy', 0),\n",
    "                'f1_macro': results.get('f1_macro', 0),\n",
    "                'attack_detection_rate': results.get('attack_detection_rate', 0)\n",
    "            }\n",
    "        \n",
    "        # Save manifest\n",
    "        manifest_file = os.path.join(self.output_dir, 'export_manifest.json')\n",
    "        with open(manifest_file, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Export manifest created: export_manifest.json\")\n",
    "    \n",
    "    def print_export_summary(self):\n",
    "        \"\"\"\n",
    "        Print summary of all exported files\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPORT SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Files created:\")\n",
    "        \n",
    "        print(\"\\n Models Directory:\")\n",
    "        for file in os.listdir(self.models_dir):\n",
    "            size_mb = os.path.getsize(os.path.join(self.models_dir, file)) / (1024*1024)\n",
    "            print(f\"   - {file} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        print(\"\\n Metrics Directory:\")\n",
    "        for file in os.listdir(self.metrics_dir):\n",
    "            print(f\"   - {file}\")\n",
    "        \n",
    "        print(\"\\n Reports Directory:\")\n",
    "        for file in os.listdir(self.reports_dir):\n",
    "            print(f\"   - {file}\")\n",
    "        \n",
    "        print(\"\\n Key Outputs for Thesis:\")\n",
    "        print(\"   - thesis_summary.csv: All model metrics in one table\")\n",
    "        print(\"   - confusion_matrix_*.csv: Confusion matrices for each model\")\n",
    "        print(\"   - per_class_metrics_*.csv: Detailed per-attack performance\")\n",
    "        print(\"   - feature_importance_*.csv: Feature rankings\")\n",
    "        print(\"   - training_metrics.json: Complete metrics in JSON format\")\n",
    "        print(\"   - analysis_report.txt: Comprehensive human-readable report\")\n",
    "        \n",
    "        print(\"\\n All results successfully exported!\")\n",
    "        print(f\"   Location: {self.output_dir}\")\n",
    "        print(\"   Ready for thesis inclusion and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a469471",
   "metadata": {},
   "source": [
    "# ### Step 11: Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main Pipeline - Complete Analysis Orchestration\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate the entire analysis pipeline from start to finish\n",
    "This is the main execution function that coordinates all components.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Validates configuration and checks system resources\n",
    "2. Extracts features from PCAP file\n",
    "3. Matches with CICIDS labels if available\n",
    "4. Analyzes and selects best features\n",
    "5. Trains ML models incrementally\n",
    "6. Generates visualizations\n",
    "7. Exports all results\n",
    "8. Cleans up temporary files\n",
    "\n",
    "PIPELINE FLOW:\n",
    "Config → Extract → Label → Analyze → Train → Visualize → Export → Cleanup\n",
    "\n",
    "ERROR HANDLING:\n",
    "- Saves partial results on failure\n",
    "- Cleans up temporary files\n",
    "- Provides detailed error messages\n",
    "- Suggests fixes for common issues\n",
    "\n",
    "MEMORY MANAGEMENT:\n",
    "- Monitors memory throughout execution\n",
    "- Switches strategies based on usage\n",
    "- Cleans up after each phase\n",
    "- Reports memory statistics\n",
    "\n",
    "IMPORTANT CHANGES:\n",
    "- Using Parquet throughout pipeline\n",
    "- Better error recovery with checkpoints\n",
    "- Parallel processing where applicable\n",
    "- Optimized for AMD Ryzen CPU\n",
    "\"\"\"\n",
    "\n",
    "def run_memory_optimized_pipeline():\n",
    "    \"\"\"\n",
    "    Main execution function for the complete analysis pipeline.\n",
    "    Orchestrates all components while managing memory efficiently.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results dictionary with paths and metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE V2.0\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Pipeline Version: 2.0 (Parquet-based, Windows-optimized)\")\n",
    "    print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"System: {psutil.virtual_memory().total / (1024**3):.1f}GB RAM, {psutil.cpu_count()} CPU cores\")\n",
    "    \n",
    "    # Check if ML should be skipped\n",
    "    if Config.SKIP_ML:\n",
    "        print(\"\\nML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "    \n",
    "    # Check configuration and memory\n",
    "    strategy = Config.check_memory_requirements()\n",
    "    \n",
    "    if strategy is False:\n",
    "        print(\"Error: Invalid PCAP file or configuration\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "    print(f\"Memory Limit: {Config.MAX_MEMORY_GB:.1f} GB\")\n",
    "    print(f\"Storage Format: {Config.STORAGE_FORMAT}\")\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'start_time': datetime.now(),\n",
    "        'strategy': strategy,\n",
    "        'temp_dirs': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ========== STEP 1: FEATURE EXTRACTION ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 1: STREAMING FEATURE EXTRACTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Extracting features from network packets...\")\n",
    "        print(\"Using Parquet for reliable storage (no file locking)\")\n",
    "        \n",
    "        pipeline = MemoryOptimizedFeaturePipeline()\n",
    "        features_dir = pipeline.extract_all_features(\n",
    "            Config.PCAP_FILE,\n",
    "            mode=Config.ANALYSIS_MODE\n",
    "        )\n",
    "        \n",
    "        if not features_dir:\n",
    "            print(\"Error: No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        results['temp_dirs'].append(features_dir)\n",
    "        results['features_dir'] = features_dir\n",
    "        \n",
    "        # Memory checkpoint\n",
    "        monitor_memory(\"After feature extraction\")\n",
    "        \n",
    "        # ========== STEP 2: LABEL MATCHING ==========\n",
    "        if Config.USE_CICIDS_LABELS and Config.LABEL_FILES:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 2: INCREMENTAL LABEL MATCHING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Matching flows with CICIDS ground truth labels...\")\n",
    "            \n",
    "            label_matcher = MemoryOptimizedLabelMatcher()\n",
    "            labeled_dir = label_matcher.process_features_with_labels(\n",
    "                features_dir, Config.LABEL_FILES\n",
    "            )\n",
    "            \n",
    "            features_dir = labeled_dir\n",
    "            results['temp_dirs'].append(labeled_dir)\n",
    "            results['features_dir'] = labeled_dir\n",
    "            \n",
    "            monitor_memory(\"After label matching\")\n",
    "        else:\n",
    "            print(\"\\nStep 2: Skipping label matching (no labels provided)\")\n",
    "        \n",
    "        # ========== STEP 3: FEATURE ANALYSIS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 3: INCREMENTAL FEATURE ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Analyzing feature importance and selecting best features...\")\n",
    "        \n",
    "        analyzer = MemoryOptimizedFeatureAnalyzer()\n",
    "        selected_features = analyzer.analyze_features_incrementally(features_dir)\n",
    "        results['selected_features'] = selected_features\n",
    "        \n",
    "        # Get feature insights\n",
    "        insights = analyzer.get_feature_insights()\n",
    "        results['feature_insights'] = insights\n",
    "        \n",
    "        monitor_memory(\"After feature analysis\")\n",
    "        \n",
    "        # ========== STEP 4: MACHINE LEARNING ==========\n",
    "        if not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: INCREMENTAL MACHINE LEARNING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Training ML models with incremental learning...\")\n",
    "            print(f\"Models to train: {Config.SELECTED_MODELS}\")\n",
    "            \n",
    "            ml_pipeline = MemoryOptimizedMLPipeline()\n",
    "            models, ml_results = ml_pipeline.train_models_incrementally(\n",
    "                features_dir, selected_features\n",
    "            )\n",
    "            \n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "            results['scaler'] = ml_pipeline.scaler\n",
    "            \n",
    "            monitor_memory(\"After ML training\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: SKIPPING ML TRAINING (Test Mode)\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"ML training skipped for pipeline validation\")\n",
    "            print(\"Feature extraction and labeling completed successfully\")\n",
    "            models = {}\n",
    "            ml_results = {}\n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "            results['scaler'] = None\n",
    "        \n",
    "        # ========== STEP 5: VISUALIZATION ==========\n",
    "        if Config.GENERATE_VISUALS and not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 5: SAMPLED VISUALIZATIONS\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Creating interactive visualizations...\")\n",
    "            \n",
    "            visualizer = MemoryOptimizedVisualizer(\n",
    "                features_dir, ml_results, Config.OUTPUT_DIR\n",
    "            )\n",
    "            visualizer.create_all_visualizations()\n",
    "            \n",
    "            monitor_memory(\"After visualization\")\n",
    "        else:\n",
    "            if Config.SKIP_ML:\n",
    "                print(\"\\nStep 5: Skipping visualizations (ML was skipped)\")\n",
    "            else:\n",
    "                print(\"\\nStep 5: Skipping visualizations (disabled)\")\n",
    "        \n",
    "        # ========== STEP 6: EXPORT RESULTS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 6: CHUNKED EXPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Exporting all results to disk...\")\n",
    "        \n",
    "        exporter = MemoryOptimizedResultExporter(Config.OUTPUT_DIR)\n",
    "        exporter.export_all(\n",
    "            features_dir, \n",
    "            models, \n",
    "            ml_results, \n",
    "            selected_features,\n",
    "            results.get('scaler')\n",
    "        )\n",
    "        \n",
    "        monitor_memory(\"After export\")\n",
    "        \n",
    "        # ========== STEP 7: CLEANUP ==========\n",
    "        if Config.CLEANUP_TEMP:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 7: CLEANUP\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Removing temporary files...\")\n",
    "            \n",
    "            import shutil\n",
    "            for temp_dir in results['temp_dirs']:\n",
    "                if os.path.exists(temp_dir):\n",
    "                    try:\n",
    "                        shutil.rmtree(temp_dir)\n",
    "                        print(f\"  Removed: {os.path.basename(temp_dir)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Failed to remove {temp_dir}: {e}\")\n",
    "        \n",
    "        # ========== FINAL SUMMARY ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PIPELINE COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calculate execution time\n",
    "        results['end_time'] = datetime.now()\n",
    "        duration = (results['end_time'] - results['start_time']).total_seconds() / 60\n",
    "        \n",
    "        print(f\"Analysis completed successfully\")\n",
    "        print(f\"  Duration: {duration:.1f} minutes\")\n",
    "        print(f\"  Output Directory: {Config.OUTPUT_DIR}\")\n",
    "        print(f\"  Storage Format: {Config.STORAGE_FORMAT}\")\n",
    "        \n",
    "        # Memory summary\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\nMemory Summary:\")\n",
    "        print(f\"  Peak Usage: ~{Config.MAX_MEMORY_GB:.1f} GB (estimated)\")\n",
    "        print(f\"  Current Usage: {mem.percent:.1f}%\")\n",
    "        print(f\"  Strategy: {strategy}\")\n",
    "        \n",
    "        # ML summary\n",
    "        if ml_results and not Config.SKIP_ML:\n",
    "            best_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "            print(f\"\\nBest Model: {best_model[0]}\")\n",
    "            print(f\"  Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
    "        elif Config.SKIP_ML:\n",
    "            print(f\"\\nML Training: Skipped (Test Mode)\")\n",
    "            print(f\"  Features extracted: {len(selected_features)}\")\n",
    "        \n",
    "        # Create success summary\n",
    "        results['summary'] = {\n",
    "            'success': True,\n",
    "            'duration_minutes': duration,\n",
    "            'strategy': strategy,\n",
    "            'memory_limit_gb': Config.MAX_MEMORY_GB,\n",
    "            'best_accuracy': max(r['accuracy'] for r in ml_results.values()) if ml_results else 0,\n",
    "            'features_selected': len(selected_features),\n",
    "            'output_dir': Config.OUTPUT_DIR,\n",
    "            'ml_skipped': Config.SKIP_ML,\n",
    "            'storage_format': Config.STORAGE_FORMAT\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✓ Pipeline execution successful!\")\n",
    "        print(f\"   View results in: {Config.OUTPUT_DIR}\")\n",
    "        if not Config.SKIP_ML:\n",
    "            print(f\"   Open dashboard.html for visualizations\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling\n",
    "        print(f\"\\n✗ Pipeline Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Try to save partial results\n",
    "        print(\"\\nAttempting to save partial results...\")\n",
    "        if 'features_dir' in results and results['features_dir']:\n",
    "            try:\n",
    "                emergency_dir = os.path.join(Config.OUTPUT_DIR, 'emergency_features')\n",
    "                import shutil\n",
    "                shutil.copytree(results['features_dir'], emergency_dir)\n",
    "                print(f\"Partial results saved to: {emergency_dir}\")\n",
    "            except:\n",
    "                print(\"Could not save partial results\")\n",
    "        \n",
    "        # Cleanup on error\n",
    "        print(\"\\nCleaning up after error...\")\n",
    "        import shutil\n",
    "        for temp_dir in results.get('temp_dirs', []):\n",
    "            if os.path.exists(temp_dir):\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Error summary\n",
    "        results['summary'] = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'partial_results': 'emergency_features' if 'features_dir' in results else None\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def monitor_memory(checkpoint_name=\"\"):\n",
    "    \"\"\"\n",
    "    Monitor and display current memory usage.\n",
    "    Helps track memory consumption throughout pipeline.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_name: Description of current pipeline stage\n",
    "    \"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nMemory Status {checkpoint_name}:\")\n",
    "    print(f\"   Used: {mem.used / (1024**3):.2f} GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Warning if memory usage is high\n",
    "    if mem.percent > 85:\n",
    "        print(\"   ⚠ WARNING: High memory usage detected!\")\n",
    "        print(\"      Consider reducing chunk/batch sizes\")\n",
    "    elif mem.percent > 95:\n",
    "        print(\"   ⛔ CRITICAL: Very high memory usage!\")\n",
    "        print(\"      Pipeline may fail - reduce settings immediately\")\n",
    "\n",
    "\n",
    "def validate_environment():\n",
    "    \"\"\"\n",
    "    Validate that all required libraries and settings are correct.\n",
    "    Run this before starting the pipeline.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENVIRONMENT VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check required libraries\n",
    "    try:\n",
    "        import pyarrow\n",
    "        print(\"✓ PyArrow installed\")\n",
    "    except:\n",
    "        issues.append(\"PyArrow not installed (required for Parquet)\")\n",
    "    \n",
    "    try:\n",
    "        import lightgbm\n",
    "        print(\"✓ LightGBM installed\")\n",
    "    except:\n",
    "        print(\"  LightGBM not installed (optional but recommended)\")\n",
    "    \n",
    "    # Check configuration\n",
    "    if not Config.PCAP_FILE:\n",
    "        issues.append(\"PCAP file not specified\")\n",
    "    elif not os.path.exists(Config.PCAP_FILE):\n",
    "        issues.append(f\"PCAP file not found: {Config.PCAP_FILE}\")\n",
    "    \n",
    "    # Check directories\n",
    "    if not os.path.exists(Config.OUTPUT_DIR):\n",
    "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "        print(f\"✓ Created output directory: {Config.OUTPUT_DIR}\")\n",
    "    \n",
    "    if not os.path.exists(Config.TEMP_DIR):\n",
    "        os.makedirs(Config.TEMP_DIR, exist_ok=True)\n",
    "        print(f\"✓ Created temp directory: {Config.TEMP_DIR}\")\n",
    "    \n",
    "    # Check memory\n",
    "    if Config.MAX_MEMORY_GB > psutil.virtual_memory().available / (1024**3):\n",
    "        issues.append(f\"MAX_MEMORY_GB ({Config.MAX_MEMORY_GB}) exceeds available memory\")\n",
    "    \n",
    "    # Check disk space\n",
    "    disk_usage = psutil.disk_usage('/')\n",
    "    if disk_usage.free < 10 * (1024**3):  # Less than 10GB free\n",
    "        issues.append(f\"Low disk space: {disk_usage.free / (1024**3):.1f} GB free\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n✗ Validation Failed:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"\\n✓ All checks passed!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# ========== EXECUTION CELL ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY TO RUN MEMORY-OPTIMIZED ANALYSIS V2.0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nThis pipeline is optimized for:\")\n",
    "print(\"• Large PCAP files (10+ GB)\")\n",
    "print(\"• CICIDS2017 dataset\")\n",
    "print(\"• Windows systems (no HDF5 file locking)\")\n",
    "print(\"• AMD Ryzen CPUs (parallel processing)\")\n",
    "print(\"• 32GB RAM systems\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"KEY IMPROVEMENTS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"✓ Parquet storage (no file locking)\")\n",
    "print(\"✓ Fixed bidirectional flow identification\")\n",
    "print(\"✓ CICIDS-specific features (Fwd/Bwd, Active/Idle)\")\n",
    "print(\"✓ LightGBM support (faster on CPU)\")\n",
    "print(\"✓ Better memory management\")\n",
    "print(\"✓ Parallel processing support\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"INSTRUCTIONS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"1. Ensure configuration is complete (Cell 2)\")\n",
    "print(\"2. Verify PCAP file path is correct\")\n",
    "print(\"3. Check available disk space for temp files\")\n",
    "print(\"4. Validate environment: validate_environment()\")\n",
    "print(\"5. Run: results = run_memory_optimized_pipeline()\")\n",
    "print(\"6. Monitor memory with: monitor_memory()\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"QUICK START:\")\n",
    "print(\"-\"*40)\n",
    "print(\"# Validate environment first:\")\n",
    "print(\"validate_environment()\")\n",
    "print(\"\")\n",
    "print(\"# To run the analysis:\")\n",
    "print(\"# results = run_memory_optimized_pipeline()\")\n",
    "print(\"\")\n",
    "print(\"# To check memory anytime:\")\n",
    "print(\"# monitor_memory()\")\n",
    "\n",
    "# Display current system status\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"CURRENT SYSTEM STATUS:\")\n",
    "print(\"-\"*40)\n",
    "monitor_memory()\n",
    "\n",
    "disk_usage = psutil.disk_usage('/')\n",
    "print(f\"\\nDisk Space:\")\n",
    "print(f\"   Available: {disk_usage.free / (1024**3):.2f} GB\")\n",
    "print(f\"   Used: {disk_usage.percent:.1f}%\")\n",
    "\n",
    "if Config.SKIP_ML:\n",
    "    print(\"\\n⚠ ML Training is currently DISABLED\")\n",
    "    print(\"   Uncheck 'Skip ML Training' in config to enable\")\n",
    "\n",
    "print(\"\\n✓ Ready to start analysis!\")\n",
    "print(\"   Run: results = run_memory_optimized_pipeline()\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = run_memory_optimized_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089b568",
   "metadata": {},
   "source": [
    "# ### run the code block below feature extracotr / ML trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_memory_optimized_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d09aa",
   "metadata": {},
   "source": [
    "# ### Step 12: Inference Mode - Classify New PCAP Files\n",
    "# ### Use a new PCAP file to be classified by the trained models. \n",
    "# ### can be used for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Inference Mode with Complete UI - Use Trained Models on New PCAP Files \n",
    "# This can be used for Validating Models or Classifying New Traffic\n",
    "\"\"\"\n",
    "PURPOSE: Use trained models to classify new PCAP files with comprehensive UI and diagnostics\n",
    "This cell loads previously trained models and applies them to analyze new network traffic \n",
    "with an interactive interface similar to Cell 2.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Provides interactive UI for model and file selection\n",
    "2. Loads saved models and scaler from pickle files\n",
    "3. Extracts features from new PCAP file with feature alignment\n",
    "4. Applies trained model to classify flows with confidence scoring\n",
    "5. Provides comprehensive attack distribution and statistics\n",
    "6. Outputs detailed diagnostics for troubleshooting\n",
    "7. Exports predictions with metadata to CSV\n",
    "8. Creates visualizations automatically\n",
    "\n",
    "ENHANCED FEATURES:\n",
    "- Interactive UI with real-time feedback\n",
    "- Feature alignment handling (missing/extra features)\n",
    "- Confidence scoring based on feature overlap\n",
    "- Detailed per-attack-type analysis\n",
    "- Comprehensive troubleshooting output\n",
    "- Model performance diagnostics\n",
    "- Feature importance analysis\n",
    "- Timeline analysis of attacks\n",
    "- Export with detailed metadata\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Must have completed training run with saved models\n",
    "- Models saved as: random_forest.pkl, lightgbm.pkl, sgd.pkl, scaler.pkl\n",
    "- Feature names saved as: selected_features.pkl\n",
    "\n",
    "USAGE:\n",
    "1. First run the main pipeline to train and save models\n",
    "2. Use the interactive UI to configure settings\n",
    "3. Click \"Classify PCAP\" to analyze\n",
    "4. Review comprehensive diagnostics output\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "class EnhancedInferenceEngine:\n",
    "    \"\"\"\n",
    "    Enhanced inference engine with comprehensive export capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir='./analysis_output'):\n",
    "        \"\"\"\n",
    "        Initialize enhanced inference engine\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.scaler = None\n",
    "        self.selected_features = None\n",
    "        self.model_metadata = {}\n",
    "        self.inference_diagnostics = {}\n",
    "        self.inference_results = {}\n",
    "        \n",
    "        # Create inference output directory\n",
    "        self.inference_dir = os.path.join(model_dir, 'inference_results')\n",
    "        os.makedirs(self.inference_dir, exist_ok=True)\n",
    "        \n",
    "        # Attack mapping\n",
    "        self.attack_mapping = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS_GoldenEye',\n",
    "            4: 'DoS_Hulk',\n",
    "            5: 'DoS_Slowhttptest',\n",
    "            6: 'DoS_slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web_Attack_BruteForce',\n",
    "            13: 'Web_Attack_SqlInjection',\n",
    "            14: 'Web_Attack_XSS'\n",
    "        }\n",
    "        \n",
    "        # Attack severity levels\n",
    "        self.attack_severity = {\n",
    "            0: 'INFO',\n",
    "            1: 'HIGH',\n",
    "            2: 'CRITICAL',\n",
    "            3: 'HIGH',\n",
    "            4: 'HIGH',\n",
    "            5: 'HIGH',\n",
    "            6: 'HIGH',\n",
    "            7: 'MEDIUM',\n",
    "            8: 'CRITICAL',\n",
    "            9: 'CRITICAL',\n",
    "            10: 'MEDIUM',\n",
    "            11: 'MEDIUM',\n",
    "            12: 'HIGH',\n",
    "            13: 'CRITICAL',\n",
    "            14: 'HIGH'\n",
    "        }\n",
    "    \n",
    "    def load_models_with_diagnostics(self):\n",
    "        \"\"\"\n",
    "        Load trained models with comprehensive diagnostics\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"LOADING TRAINED MODELS WITH DIAGNOSTICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        diagnostics = {\n",
    "            'models_loaded': [],\n",
    "            'models_failed': [],\n",
    "            'feature_count': 0,\n",
    "            'scaler_status': 'NOT_LOADED',\n",
    "            'model_sizes': {},\n",
    "            'load_errors': [],\n",
    "            'load_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Load scaler\n",
    "        scaler_path = os.path.join(self.model_dir, 'models', 'scaler.pkl')\n",
    "        if not os.path.exists(scaler_path):\n",
    "            scaler_path = os.path.join(self.model_dir, 'scaler.pkl')\n",
    "        \n",
    "        if os.path.exists(scaler_path):\n",
    "            try:\n",
    "                with open(scaler_path, 'rb') as f:\n",
    "                    self.scaler = pickle.load(f)\n",
    "                size_mb = os.path.getsize(scaler_path) / (1024*1024)\n",
    "                diagnostics['scaler_status'] = 'LOADED'\n",
    "                diagnostics['scaler_size_mb'] = size_mb\n",
    "                print(f\"✓ Loaded scaler: {size_mb:.2f} MB\")\n",
    "                print(f\"  Expected features: {self.scaler.n_features_in_}\")\n",
    "            except Exception as e:\n",
    "                diagnostics['scaler_status'] = 'FAILED'\n",
    "                diagnostics['load_errors'].append(f\"Scaler: {e}\")\n",
    "                print(f\"✗ Failed to load scaler: {e}\")\n",
    "                return False, diagnostics\n",
    "        else:\n",
    "            print(f\"✗ Scaler not found\")\n",
    "            return False, diagnostics\n",
    "        \n",
    "        # Load selected features\n",
    "        features_path = os.path.join(self.model_dir, 'models', 'selected_features.pkl')\n",
    "        if not os.path.exists(features_path):\n",
    "            features_path = os.path.join(self.model_dir, 'selected_features.pkl')\n",
    "        \n",
    "        if os.path.exists(features_path):\n",
    "            try:\n",
    "                with open(features_path, 'rb') as f:\n",
    "                    self.selected_features = pickle.load(f)\n",
    "                diagnostics['feature_count'] = len(self.selected_features)\n",
    "                print(f\"✓ Loaded {len(self.selected_features)} selected features\")\n",
    "            except Exception as e:\n",
    "                diagnostics['load_errors'].append(f\"Features: {e}\")\n",
    "                print(f\"✗ Failed to load features: {e}\")\n",
    "                return False, diagnostics\n",
    "        else:\n",
    "            print(f\"✗ Selected features not found\")\n",
    "            return False, diagnostics\n",
    "        \n",
    "        # Load available models\n",
    "        model_files = {\n",
    "            'sgd': 'sgd.pkl',\n",
    "            'random_forest': 'random_forest.pkl',\n",
    "            'lightgbm': 'lightgbm.pkl',\n",
    "            'xgboost_incremental': 'xgboost_incremental.pkl'\n",
    "        }\n",
    "        \n",
    "        # Check both locations\n",
    "        for model_name, filename in model_files.items():\n",
    "            model_path = os.path.join(self.model_dir, 'models', filename)\n",
    "            if not os.path.exists(model_path):\n",
    "                model_path = os.path.join(self.model_dir, filename)\n",
    "            \n",
    "            if os.path.exists(model_path):\n",
    "                try:\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model = pickle.load(f)\n",
    "                    \n",
    "                    size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "                    self.models[model_name] = model\n",
    "                    diagnostics['models_loaded'].append(model_name)\n",
    "                    diagnostics['model_sizes'][model_name] = size_mb\n",
    "                    \n",
    "                    print(f\"✓ Loaded {model_name}: {size_mb:.2f} MB\")\n",
    "                    \n",
    "                    self.model_metadata[model_name] = {\n",
    "                        'size_mb': size_mb,\n",
    "                        'type': str(type(model).__name__),\n",
    "                        'expected_features': getattr(model, 'n_features_in_', 'Unknown')\n",
    "                    }\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    diagnostics['models_failed'].append(model_name)\n",
    "                    diagnostics['load_errors'].append(f\"{model_name}: {e}\")\n",
    "                    print(f\"✗ Failed to load {model_name}: {e}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n\" + \"-\"*50)\n",
    "        print(\"MODEL LOADING SUMMARY\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Models loaded: {len(diagnostics['models_loaded'])}\")\n",
    "        print(f\"Total size: {sum(diagnostics['model_sizes'].values()):.2f} MB\")\n",
    "        \n",
    "        if not self.models:\n",
    "            print(\"ERROR: No models loaded!\")\n",
    "            return False, diagnostics\n",
    "        \n",
    "        print(f\"\\nReady models: {list(self.models.keys())}\")\n",
    "        self.inference_diagnostics['model_loading'] = diagnostics\n",
    "        return True, diagnostics\n",
    "    \n",
    "    def extract_features_with_diagnostics(self, pcap_file):\n",
    "        \"\"\"\n",
    "        Extract features with comprehensive diagnostics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FEATURE EXTRACTION WITH DIAGNOSTICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"PCAP: {os.path.basename(pcap_file)}\")\n",
    "        print(f\"Size: {os.path.getsize(pcap_file) / (1024**3):.2f} GB\")\n",
    "        \n",
    "        extraction_start = pd.Timestamp.now()\n",
    "        \n",
    "        # Use existing pipeline\n",
    "        pipeline = MemoryOptimizedFeaturePipeline()\n",
    "        \n",
    "        # Extract features\n",
    "        features_dir = pipeline.extract_all_features(\n",
    "            pcap_file,\n",
    "            mode='flow'\n",
    "        )\n",
    "        \n",
    "        extraction_time = (pd.Timestamp.now() - extraction_start).total_seconds()\n",
    "        \n",
    "        # Analyze extracted features\n",
    "        extraction_diagnostics = self.analyze_extracted_features(features_dir, extraction_time)\n",
    "        extraction_diagnostics['pcap_file'] = os.path.basename(pcap_file)\n",
    "        extraction_diagnostics['pcap_size_gb'] = os.path.getsize(pcap_file) / (1024**3)\n",
    "        \n",
    "        self.inference_diagnostics['feature_extraction'] = extraction_diagnostics\n",
    "        \n",
    "        return features_dir\n",
    "    \n",
    "    def analyze_extracted_features(self, features_dir, extraction_time):\n",
    "        \"\"\"\n",
    "        Analyze extracted features for diagnostics\n",
    "        \"\"\"\n",
    "        diagnostics = {\n",
    "            'extraction_time_seconds': extraction_time,\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'total_flows': 0,\n",
    "            'total_batches': 0,\n",
    "            'feature_columns': [],\n",
    "            'missing_features': [],\n",
    "            'extra_features': [],\n",
    "            'feature_overlap_ratio': 0.0\n",
    "        }\n",
    "        \n",
    "        if not features_dir or not os.path.exists(features_dir):\n",
    "            return diagnostics\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        diagnostics['total_batches'] = len(feature_files)\n",
    "        \n",
    "        if feature_files:\n",
    "            sample_batch = pd.read_parquet(feature_files[0])\n",
    "            diagnostics['feature_columns'] = list(sample_batch.columns)\n",
    "            \n",
    "            for file in feature_files:\n",
    "                batch = pd.read_parquet(file, columns=['flow_id'])\n",
    "                diagnostics['total_flows'] += len(batch)\n",
    "                del batch\n",
    "            \n",
    "            extracted_features = set(diagnostics['feature_columns'])\n",
    "            expected_features = set(self.selected_features) if self.selected_features else set()\n",
    "            \n",
    "            diagnostics['missing_features'] = list(expected_features - extracted_features)\n",
    "            diagnostics['extra_features'] = list(extracted_features - expected_features)\n",
    "            \n",
    "            if expected_features:\n",
    "                overlap = len(expected_features & extracted_features)\n",
    "                diagnostics['feature_overlap_ratio'] = overlap / len(expected_features)\n",
    "        \n",
    "        print(f\"\\nFeature Extraction Complete:\")\n",
    "        print(f\"  Time: {extraction_time:.1f} seconds\")\n",
    "        print(f\"  Flows: {diagnostics['total_flows']:,}\")\n",
    "        print(f\"  Feature Overlap: {diagnostics['feature_overlap_ratio']:.1%}\")\n",
    "        \n",
    "        return diagnostics\n",
    "    \n",
    "    def classify_flows_with_diagnostics(self, features_dir, model_name='lightgbm', with_ground_truth=False):\n",
    "        \"\"\"\n",
    "        Classify flows with enhanced diagnostics and optional ground truth comparison\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            print(f\"Model {model_name} not available\")\n",
    "            return None, None, None\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(f\"CLASSIFICATION: {model_name.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        classification_start = pd.Timestamp.now()\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        all_predictions = []\n",
    "        all_metadata = []\n",
    "        all_confidences = []\n",
    "        all_probabilities = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        classification_diagnostics = {\n",
    "            'model_name': model_name,\n",
    "            'classification_timestamp': datetime.now().isoformat(),\n",
    "            'processing_time_seconds': 0,\n",
    "            'batches_processed': 0,\n",
    "            'flows_classified': 0,\n",
    "            'feature_alignment_issues': 0,\n",
    "            'prediction_distribution': Counter(),\n",
    "            'confidence_stats': {},\n",
    "            'temporal_analysis': [],\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        extraction_diag = self.inference_diagnostics.get('feature_extraction', {})\n",
    "        feature_overlap_ratio = extraction_diag.get('feature_overlap_ratio', 1.0)\n",
    "        \n",
    "        import glob\n",
    "        feature_files = sorted(glob.glob(os.path.join(features_dir, 'batch_*.parquet')))\n",
    "        \n",
    "        for file in tqdm(feature_files, desc=f\"Classifying with {model_name}\"):\n",
    "            try:\n",
    "                batch = pd.read_parquet(file)\n",
    "                classification_diagnostics['batches_processed'] += 1\n",
    "                \n",
    "                # Feature alignment\n",
    "                feature_df = pd.DataFrame(index=batch.index)\n",
    "                missing_count = 0\n",
    "                \n",
    "                for feature in self.selected_features:\n",
    "                    if feature in batch.columns:\n",
    "                        feature_df[feature] = batch[feature]\n",
    "                    else:\n",
    "                        feature_df[feature] = 0.0\n",
    "                        missing_count += 1\n",
    "                \n",
    "                if missing_count > 0:\n",
    "                    classification_diagnostics['feature_alignment_issues'] += 1\n",
    "                \n",
    "                X = feature_df.values.astype(np.float32)\n",
    "                X_scaled = self.scaler.transform(X)\n",
    "                \n",
    "                # Make predictions\n",
    "                if model_name == 'lightgbm':\n",
    "                    predictions = model.predict(X_scaled, num_iteration=model.best_iteration)\n",
    "                else:\n",
    "                    predictions = model.predict(X_scaled)\n",
    "                \n",
    "                # Get probabilities if available\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    probs = model.predict_proba(X_scaled)\n",
    "                    all_probabilities.extend(probs)\n",
    "                \n",
    "                # Calculate confidence\n",
    "                confidences = self.calculate_confidence_scores(\n",
    "                    predictions, feature_overlap_ratio, missing_count, len(self.selected_features)\n",
    "                )\n",
    "                \n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidences.extend(confidences)\n",
    "                classification_diagnostics['flows_classified'] += len(predictions)\n",
    "                classification_diagnostics['prediction_distribution'].update(predictions)\n",
    "                \n",
    "                # Collect metadata\n",
    "                metadata_cols = ['flow_id', 'src_port', 'dst_port', 'total_bytes', \n",
    "                               'total_packets', 'flow_duration', 'protocol']\n",
    "                \n",
    "                # Add timestamp if available\n",
    "                if 'timestamp' in batch.columns:\n",
    "                    metadata_cols.append('timestamp')\n",
    "                \n",
    "                # Check for ground truth labels\n",
    "                if 'label' in batch.columns and with_ground_truth:\n",
    "                    all_true_labels.extend(batch['label'].values)\n",
    "                    metadata_cols.append('label')\n",
    "                \n",
    "                available_metadata_cols = [col for col in metadata_cols if col in batch.columns]\n",
    "                \n",
    "                metadata = batch[available_metadata_cols].copy()\n",
    "                metadata['prediction'] = predictions\n",
    "                metadata['confidence'] = confidences\n",
    "                metadata['attack_type'] = [self.attack_mapping.get(int(p), 'Unknown') for p in predictions]\n",
    "                metadata['severity'] = [self.attack_severity.get(int(p), 'UNKNOWN') for p in predictions]\n",
    "                metadata['model_used'] = model_name\n",
    "                metadata['confidence_category'] = pd.cut(metadata['confidence'], \n",
    "                                                        bins=[0, 0.5, 0.8, 1.0],\n",
    "                                                        labels=['LOW', 'MEDIUM', 'HIGH'])\n",
    "                \n",
    "                all_metadata.append(metadata)\n",
    "                \n",
    "                del batch, feature_df, X, X_scaled\n",
    "                \n",
    "            except Exception as e:\n",
    "                classification_diagnostics['errors'].append(f\"Batch {os.path.basename(file)}: {e}\")\n",
    "                print(f\"  Error: {e}\")\n",
    "        \n",
    "        classification_time = (pd.Timestamp.now() - classification_start).total_seconds()\n",
    "        classification_diagnostics['processing_time_seconds'] = classification_time\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if all_confidences:\n",
    "            classification_diagnostics['confidence_stats'] = {\n",
    "                'mean': np.mean(all_confidences),\n",
    "                'median': np.median(all_confidences),\n",
    "                'min': np.min(all_confidences),\n",
    "                'max': np.max(all_confidences),\n",
    "                'std': np.std(all_confidences),\n",
    "                'low_confidence_pct': np.mean(np.array(all_confidences) < 0.5),\n",
    "                'high_confidence_pct': np.mean(np.array(all_confidences) > 0.8)\n",
    "            }\n",
    "        \n",
    "        # Combine results\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_confidences = np.array(all_confidences)\n",
    "        all_metadata = pd.concat(all_metadata, ignore_index=True) if all_metadata else pd.DataFrame()\n",
    "        \n",
    "        # If ground truth available, calculate inference metrics\n",
    "        if all_true_labels and with_ground_truth:\n",
    "            inference_metrics = self.calculate_inference_metrics(\n",
    "                np.array(all_true_labels), all_predictions\n",
    "            )\n",
    "            classification_diagnostics['inference_metrics'] = inference_metrics\n",
    "            print(\"\\n✓ Calculated inference metrics against ground truth\")\n",
    "        \n",
    "        self.display_classification_diagnostics(classification_diagnostics)\n",
    "        self.inference_diagnostics['classification'] = classification_diagnostics\n",
    "        \n",
    "        # Store results for export\n",
    "        self.inference_results = {\n",
    "            'predictions': all_predictions,\n",
    "            'metadata': all_metadata,\n",
    "            'confidences': all_confidences,\n",
    "            'probabilities': all_probabilities if all_probabilities else None,\n",
    "            'true_labels': all_true_labels if all_true_labels else None\n",
    "        }\n",
    "        \n",
    "        return all_predictions, all_metadata, all_confidences\n",
    "    \n",
    "    def calculate_confidence_scores(self, predictions, feature_overlap_ratio, missing_features, total_features):\n",
    "        \"\"\"\n",
    "        Calculate confidence scores based on feature alignment\n",
    "        \"\"\"\n",
    "        base_confidence = 0.85\n",
    "        feature_penalty = (1.0 - feature_overlap_ratio) * 0.3\n",
    "        missing_penalty = (missing_features / total_features) * 0.2\n",
    "        confidence = max(0.1, base_confidence - feature_penalty - missing_penalty)\n",
    "        \n",
    "        return [confidence] * len(predictions)\n",
    "    \n",
    "    def calculate_inference_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate metrics when ground truth is available\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()\n",
    "        }\n",
    "        \n",
    "        # Attack detection metrics\n",
    "        benign_mask = (y_true == 0)\n",
    "        attack_mask = (y_true > 0)\n",
    "        \n",
    "        if attack_mask.sum() > 0:\n",
    "            attack_detected = (y_pred[attack_mask] > 0).sum()\n",
    "            metrics['attack_detection_rate'] = attack_detected / attack_mask.sum()\n",
    "        else:\n",
    "            metrics['attack_detection_rate'] = 0\n",
    "        \n",
    "        if benign_mask.sum() > 0:\n",
    "            benign_misclassified = (y_pred[benign_mask] > 0).sum()\n",
    "            metrics['false_positive_rate'] = benign_misclassified / benign_mask.sum()\n",
    "        else:\n",
    "            metrics['false_positive_rate'] = 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def display_classification_diagnostics(self, diagnostics):\n",
    "        \"\"\"\n",
    "        Display classification diagnostics\n",
    "        \"\"\"\n",
    "        print(f\"\\nClassification Complete:\")\n",
    "        print(f\"  Time: {diagnostics['processing_time_seconds']:.1f} seconds\")\n",
    "        print(f\"  Flows: {diagnostics['flows_classified']:,}\")\n",
    "        print(f\"  Unique Predictions: {len(diagnostics['prediction_distribution'])}\")\n",
    "        \n",
    "        if diagnostics['confidence_stats']:\n",
    "            conf = diagnostics['confidence_stats']\n",
    "            print(f\"  Avg Confidence: {conf['mean']:.3f}\")\n",
    "            print(f\"  Low Confidence: {conf['low_confidence_pct']:.1%}\")\n",
    "        \n",
    "        if 'inference_metrics' in diagnostics:\n",
    "            metrics = diagnostics['inference_metrics']\n",
    "            print(f\"\\nInference Performance:\")\n",
    "            print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  F1-Macro: {metrics['f1_macro']:.4f}\")\n",
    "            print(f\"  Attack Detection: {metrics['attack_detection_rate']:.4f}\")\n",
    "    \n",
    "    def analyze_results_comprehensive(self, predictions, metadata, confidences):\n",
    "        \"\"\"\n",
    "        Comprehensive analysis with enhanced export\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(\"No predictions to analyze\")\n",
    "            return {}\n",
    "        \n",
    "        total_flows = len(predictions)\n",
    "        attack_counts = Counter(metadata['attack_type'].values) if 'attack_type' in metadata.columns else Counter()\n",
    "        \n",
    "        analysis_results = {\n",
    "            'total_flows': total_flows,\n",
    "            'attack_distribution': dict(attack_counts),\n",
    "            'unique_predictions': len(np.unique(predictions)),\n",
    "            'confidence_analysis': {},\n",
    "            'severity_analysis': {},\n",
    "            'temporal_analysis': {},\n",
    "            'port_analysis': {},\n",
    "            'size_analysis': {},\n",
    "            'analysis_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Attack distribution\n",
    "        print(f\"Total Flows: {total_flows:,}\")\n",
    "        print(f\"Unique Attack Types: {len(attack_counts)}\")\n",
    "        \n",
    "        print(f\"\\n\" + \"-\"*50)\n",
    "        print(\"ATTACK TYPE DISTRIBUTION\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        benign_count = attack_counts.get('BENIGN', 0)\n",
    "        attack_count = total_flows - benign_count\n",
    "        \n",
    "        for attack_type, count in attack_counts.most_common():\n",
    "            percentage = (count / total_flows) * 100\n",
    "            severity = self.attack_severity.get(\n",
    "                list(self.attack_mapping.keys())[list(self.attack_mapping.values()).index(attack_type)],\n",
    "                'UNKNOWN'\n",
    "            )\n",
    "            print(f\"  {attack_type:30s}: {count:8,} ({percentage:5.1f}%) [{severity}]\")\n",
    "        \n",
    "        # Severity analysis\n",
    "        if 'severity' in metadata.columns:\n",
    "            severity_counts = Counter(metadata['severity'].values)\n",
    "            analysis_results['severity_analysis'] = dict(severity_counts)\n",
    "        \n",
    "        # Confidence analysis\n",
    "        if len(confidences) > 0:\n",
    "            conf_stats = {\n",
    "                'mean': np.mean(confidences),\n",
    "                'median': np.median(confidences),\n",
    "                'std': np.std(confidences),\n",
    "                'low_confidence_count': np.sum(confidences < 0.5),\n",
    "                'high_confidence_count': np.sum(confidences > 0.8),\n",
    "                'distribution': {\n",
    "                    'low': np.sum(confidences < 0.5),\n",
    "                    'medium': np.sum((confidences >= 0.5) & (confidences < 0.8)),\n",
    "                    'high': np.sum(confidences >= 0.8)\n",
    "                }\n",
    "            }\n",
    "            analysis_results['confidence_analysis'] = conf_stats\n",
    "        \n",
    "        # Generate recommendations\n",
    "        self.generate_recommendations(analysis_results)\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def generate_recommendations(self, analysis_results):\n",
    "        \"\"\"\n",
    "        Generate security recommendations\n",
    "        \"\"\"\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(\"SECURITY RECOMMENDATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        attack_dist = analysis_results['attack_distribution']\n",
    "        attack_count = analysis_results['total_flows'] - attack_dist.get('BENIGN', 0)\n",
    "        attack_ratio = attack_count / analysis_results['total_flows'] if analysis_results['total_flows'] > 0 else 0\n",
    "        \n",
    "        if attack_ratio < 0.01:\n",
    "            print(\"✅ Very clean traffic (<1% attacks)\")\n",
    "        elif attack_ratio > 0.3:\n",
    "            print(\"⚠️ High attack activity (>30%)\")\n",
    "            print(\"   Immediate investigation recommended\")\n",
    "        else:\n",
    "            print(f\"⚠️ Moderate attack activity ({attack_ratio:.1%})\")\n",
    "    \n",
    "    def export_inference_results(self):\n",
    "        \"\"\"\n",
    "        Export comprehensive inference results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPORTING INFERENCE RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not self.inference_results or 'metadata' not in self.inference_results:\n",
    "            print(\"No results to export\")\n",
    "            return\n",
    "        \n",
    "        metadata = self.inference_results['metadata']\n",
    "        \n",
    "        # Export enhanced predictions CSV\n",
    "        predictions_file = os.path.join(self.inference_dir, 'predictions_enhanced.csv')\n",
    "        metadata.to_csv(predictions_file, index=False)\n",
    "        print(f\"✓ Predictions: {os.path.basename(predictions_file)}\")\n",
    "        \n",
    "        # Export inference summary JSON\n",
    "        summary = {\n",
    "            'inference_timestamp': datetime.now().isoformat(),\n",
    "            'total_flows': len(metadata),\n",
    "            'model_used': self.inference_diagnostics.get('classification', {}).get('model_name', 'unknown'),\n",
    "            'processing_time_seconds': self.inference_diagnostics.get('classification', {}).get('processing_time_seconds', 0),\n",
    "            'feature_extraction_time': self.inference_diagnostics.get('feature_extraction', {}).get('extraction_time_seconds', 0),\n",
    "            'attack_distribution': dict(Counter(metadata['attack_type'].values)),\n",
    "            'confidence_stats': self.inference_diagnostics.get('classification', {}).get('confidence_stats', {}),\n",
    "            'unique_predictions': len(metadata['attack_type'].unique()),\n",
    "            'pcap_analyzed': self.inference_diagnostics.get('feature_extraction', {}).get('pcap_file', 'unknown')\n",
    "        }\n",
    "        \n",
    "        summary_file = os.path.join(self.inference_dir, 'inference_summary.json')\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        print(f\"✓ Summary: {os.path.basename(summary_file)}\")\n",
    "        \n",
    "        print(f\"\\n✅ All inference results exported to: {self.inference_dir}\")\n",
    "    \n",
    "    def create_summary_visualization(self, metadata, analysis_results):\n",
    "        \"\"\"\n",
    "        Create enhanced visualization with export\n",
    "        \"\"\"\n",
    "        if metadata.empty:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Attack distribution\n",
    "        attack_dist = analysis_results['attack_distribution']\n",
    "        if attack_dist:\n",
    "            labels = list(attack_dist.keys())[:10]\n",
    "            sizes = [attack_dist[label] for label in labels]\n",
    "            colors = ['green' if label == 'BENIGN' else 'red' for label in labels]\n",
    "            \n",
    "            axes[0, 0].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "            axes[0, 0].set_title('Attack Type Distribution')\n",
    "        \n",
    "        # Confidence distribution\n",
    "        if 'confidence' in metadata.columns:\n",
    "            axes[0, 1].hist(metadata['confidence'], bins=20, alpha=0.7, color='blue')\n",
    "            axes[0, 1].set_title('Confidence Distribution')\n",
    "            axes[0, 1].set_xlabel('Confidence')\n",
    "            axes[0, 1].set_ylabel('Count')\n",
    "        \n",
    "        # Severity distribution\n",
    "        if 'severity' in metadata.columns:\n",
    "            severity_counts = metadata['severity'].value_counts()\n",
    "            axes[1, 0].bar(severity_counts.index, severity_counts.values)\n",
    "            axes[1, 0].set_title('Attack Severity Distribution')\n",
    "            axes[1, 0].set_xlabel('Severity')\n",
    "            axes[1, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Flow sizes\n",
    "        if 'total_bytes' in metadata.columns:\n",
    "            log_sizes = np.log10(metadata['total_bytes'].replace(0, 1))\n",
    "            axes[1, 1].hist(log_sizes, bins=30, alpha=0.7, color='orange')\n",
    "            axes[1, 1].set_title('Flow Size Distribution')\n",
    "            axes[1, 1].set_xlabel('Log10(Bytes)')\n",
    "            axes[1, 1].set_ylabel('Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        viz_path = os.path.join(self.inference_dir, 'inference_visualization.png')\n",
    "        plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Visualization: {os.path.basename(viz_path)}\")\n",
    "\n",
    "\n",
    "def classify_new_pcap_enhanced(pcap_file, model_name='lightgbm', export=True, \n",
    "                               model_dir='./analysis_output', output_dir=None, with_ground_truth=False):\n",
    "    \"\"\"\n",
    "    Enhanced classification with comprehensive export\n",
    "    \"\"\"\n",
    "    engine = EnhancedInferenceEngine(model_dir=model_dir)\n",
    "    \n",
    "    # Set custom output directory if provided\n",
    "    if output_dir:\n",
    "        engine.inference_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load models\n",
    "    success, load_diagnostics = engine.load_models_with_diagnostics()\n",
    "    if not success:\n",
    "        print(\"Failed to load models\")\n",
    "        return None\n",
    "    \n",
    "    # Extract features\n",
    "    features_dir = engine.extract_features_with_diagnostics(pcap_file)\n",
    "    \n",
    "    if not features_dir:\n",
    "        print(\"Failed to extract features\")\n",
    "        return None\n",
    "    \n",
    "    # Classify flows\n",
    "    predictions, metadata, confidences = engine.classify_flows_with_diagnostics(\n",
    "        features_dir, model_name, with_ground_truth\n",
    "    )\n",
    "    \n",
    "    if predictions is None:\n",
    "        print(\"Classification failed\")\n",
    "        return None\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis_results = engine.analyze_results_comprehensive(predictions, metadata, confidences)\n",
    "    \n",
    "    # Create visualization\n",
    "    engine.create_summary_visualization(metadata, analysis_results)\n",
    "    \n",
    "    # Export all results\n",
    "    if export:\n",
    "        engine.export_inference_results()\n",
    "    \n",
    "    # Clean up\n",
    "    try:\n",
    "        if Config.CLEANUP_TEMP:\n",
    "            import shutil\n",
    "            shutil.rmtree(features_dir)\n",
    "            print(\"\\n✓ Cleaned up temporary files\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\n",
    "        'total_flows': len(predictions),\n",
    "        'attack_distribution': analysis_results['attack_distribution'],\n",
    "        'analysis_results': analysis_results,\n",
    "        'diagnostics': engine.inference_diagnostics,\n",
    "        'metadata': metadata,\n",
    "        'model_used': model_name,\n",
    "        'export_directory': engine.inference_dir\n",
    "    }\n",
    "\n",
    "\n",
    "# ========== INTERACTIVE UI SECTION ==========\n",
    "def create_comprehensive_inference_ui():\n",
    "    \"\"\"\n",
    "    Create comprehensive interactive UI for inference mode\n",
    "    \"\"\"\n",
    "    display(HTML(\"\"\"\n",
    "    <style>\n",
    "        .inference-header {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            color: white;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .inference-section {\n",
    "            background: #f8f9fa;\n",
    "            padding: 15px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        .status-box {\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "            margin-top: 10px;\n",
    "        }\n",
    "        .status-ready { background: #d4edda; color: #155724; }\n",
    "        .status-processing { background: #fff3cd; color: #856404; }\n",
    "        .status-error { background: #f8d7da; color: #721c24; }\n",
    "        .status-success { background: #d1ecf1; color: #0c5460; }\n",
    "    </style>\n",
    "    \"\"\"))\n",
    "    \n",
    "    display(HTML(\"\"\"\n",
    "    <div class=\"inference-header\">\n",
    "        <h2>🔍 Network Traffic Classification - Inference Mode</h2>\n",
    "        <p>Analyze new PCAP files using your trained models</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # ========== MODEL SELECTION SECTION ==========\n",
    "    display(HTML(\"<h3>📊 Model Configuration</h3>\"))\n",
    "    \n",
    "    model_dir_input = widgets.Text(\n",
    "        value='./analysis_output',\n",
    "        description='Model Directory:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='80%'),\n",
    "        placeholder='Path to directory containing saved models'\n",
    "    )\n",
    "    \n",
    "    # Model availability checker\n",
    "    model_status = widgets.HTML(value='<i>Enter model directory to check available models</i>')\n",
    "    \n",
    "    # Model selector with dynamic options\n",
    "    model_selector = widgets.RadioButtons(\n",
    "        options=[\n",
    "            ('Random Forest (Best for balanced accuracy)', 'random_forest'),\n",
    "            ('LightGBM (Fast, good performance)', 'lightgbm'),\n",
    "            ('SGD Classifier (Linear, memory efficient)', 'sgd'),\n",
    "            ('XGBoost (High accuracy, slower)', 'xgboost_incremental')\n",
    "        ],\n",
    "        value='random_forest',\n",
    "        description='Select Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # ========== PCAP INPUT SECTION ==========\n",
    "    display(HTML(\"<h3>PCAP File Selection</h3>\"))\n",
    "    \n",
    "    pcap_input = widgets.Text(\n",
    "        value='',\n",
    "        description='PCAP File Path:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='80%'),\n",
    "        placeholder='Enter full path to PCAP file (e.g., /path/to/capture.pcap)'\n",
    "    )\n",
    "    \n",
    "    # File size display\n",
    "    file_info = widgets.HTML(value='<i>No file selected</i>')\n",
    "    \n",
    "    # Output directory input\n",
    "    output_dir_input = widgets.Text(\n",
    "        value='./inference_output',\n",
    "        description='Output Directory:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='80%'),\n",
    "        placeholder='Where to save inference results (e.g., ./inference_output)'\n",
    "    )\n",
    "    \n",
    "    # ========== PROCESSING OPTIONS ==========\n",
    "    display(HTML(\"<h3>⚙️ Processing Options</h3>\"))\n",
    "    \n",
    "    # Ground truth option\n",
    "    with_ground_truth = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='PCAP has ground truth labels (for accuracy metrics)',\n",
    "        indent=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Export options\n",
    "    export_results = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Export detailed results to files',\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    generate_viz = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Generate visualization charts',\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    cleanup_temp = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Clean up temporary files after processing',\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    # Advanced options accordion\n",
    "    advanced_options = widgets.Accordion(children=[\n",
    "        widgets.VBox([\n",
    "            widgets.IntSlider(\n",
    "                value=50000,\n",
    "                min=10000,\n",
    "                max=200000,\n",
    "                step=10000,\n",
    "                description='Chunk Size:',\n",
    "                style={'description_width': 'initial'}\n",
    "            ),\n",
    "            widgets.FloatSlider(\n",
    "                value=0.5,\n",
    "                min=0.1,\n",
    "                max=1.0,\n",
    "                step=0.1,\n",
    "                description='Confidence Threshold:',\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "        ])\n",
    "    ])\n",
    "    advanced_options.set_title(0, 'Advanced Settings')\n",
    "    advanced_options.selected_index = None  # Start collapsed\n",
    "    \n",
    "    # ========== PROGRESS AND OUTPUT SECTION ==========\n",
    "    display(HTML(\"<h3>📈 Analysis Progress</h3>\"))\n",
    "    \n",
    "    # Progress bar\n",
    "    progress_bar = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='Progress:',\n",
    "        bar_style='',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    # Status display\n",
    "    status_display = widgets.HTML(value='<div class=\"status-box status-ready\">✅ Ready to classify</div>')\n",
    "    \n",
    "    # Output area for results\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # ========== CONTROL BUTTONS ==========\n",
    "    display(HTML(\"<h3>🎮 Controls</h3>\"))\n",
    "    \n",
    "    # Check models button\n",
    "    check_models_btn = widgets.Button(\n",
    "        description='Check Models',\n",
    "        button_style='info',\n",
    "        tooltip='Check which models are available',\n",
    "        icon='search',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    # Classify button\n",
    "    classify_btn = widgets.Button(\n",
    "        description='Classify PCAP',\n",
    "        button_style='success',\n",
    "        tooltip='Start classification process',\n",
    "        icon='play',\n",
    "        layout=widgets.Layout(width='150px', height='40px')\n",
    "    )\n",
    "    \n",
    "    # Stop button\n",
    "    stop_btn = widgets.Button(\n",
    "        description='Stop',\n",
    "        button_style='danger',\n",
    "        tooltip='Stop current processing',\n",
    "        icon='stop',\n",
    "        layout=widgets.Layout(width='100px'),\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Clear output button\n",
    "    clear_btn = widgets.Button(\n",
    "        description='Clear Output',\n",
    "        button_style='warning',\n",
    "        tooltip='Clear the output area',\n",
    "        icon='trash',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "    \n",
    "    # ========== EVENT HANDLERS ==========\n",
    "    \n",
    "    def check_file_info(change):\n",
    "        \"\"\"Update file information when PCAP path changes\"\"\"\n",
    "        pcap_path = pcap_input.value.strip()\n",
    "        if pcap_path and os.path.exists(pcap_path):\n",
    "            size_gb = os.path.getsize(pcap_path) / (1024**3)\n",
    "            file_info.value = f'<i>✓ File found: {size_gb:.2f} GB</i>'\n",
    "        elif pcap_path:\n",
    "            file_info.value = '<i style=\"color: red;\">✗ File not found</i>'\n",
    "        else:\n",
    "            file_info.value = '<i>No file selected</i>'\n",
    "    \n",
    "    pcap_input.observe(check_file_info, names='value')\n",
    "    \n",
    "    def check_models(b):\n",
    "        \"\"\"Check available models in the specified directory\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            model_dir = model_dir_input.value.strip()\n",
    "            \n",
    "            if not os.path.exists(model_dir):\n",
    "                model_status.value = '<i style=\"color: red;\">✗ Directory not found</i>'\n",
    "                print(f\"Directory not found: {model_dir}\")\n",
    "                return\n",
    "            \n",
    "            # Check for model files\n",
    "            models_found = []\n",
    "            models_path = os.path.join(model_dir, 'models')\n",
    "            \n",
    "            if not os.path.exists(models_path):\n",
    "                models_path = model_dir\n",
    "            \n",
    "            for model_name in ['random_forest.pkl', 'lightgbm.pkl', 'sgd.pkl', 'xgboost_incremental.pkl']:\n",
    "                if os.path.exists(os.path.join(models_path, model_name)):\n",
    "                    models_found.append(model_name.replace('.pkl', ''))\n",
    "            \n",
    "            if models_found:\n",
    "                model_status.value = f'<i style=\"color: green;\">✓ Found models: {\", \".join(models_found)}</i>'\n",
    "                print(f\"Available models: {', '.join(models_found)}\")\n",
    "            else:\n",
    "                model_status.value = '<i style=\"color: red;\">✗ No models found</i>'\n",
    "                print(\"No models found in directory\")\n",
    "    \n",
    "    check_models_btn.on_click(check_models)\n",
    "    \n",
    "    def on_classify(b):\n",
    "        \"\"\"Main classification handler\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            # Validate inputs\n",
    "            pcap_file = pcap_input.value.strip()\n",
    "            model_dir = model_dir_input.value.strip()\n",
    "            output_dir = output_dir_input.value.strip()\n",
    "            model_name = model_selector.value\n",
    "            \n",
    "            if not pcap_file:\n",
    "                status_display.value = '<div class=\"status-box status-error\">Error: Please specify a PCAP file</div>'\n",
    "                print(\"Error: No PCAP file specified\")\n",
    "                return\n",
    "            \n",
    "            if not os.path.exists(pcap_file):\n",
    "                status_display.value = '<div class=\"status-box status-error\">Error: PCAP file not found</div>'\n",
    "                print(f\"Error: PCAP file not found: {pcap_file}\")\n",
    "                return\n",
    "            \n",
    "            if not os.path.exists(model_dir):\n",
    "                status_display.value = '<div class=\"status-box status-error\">Error: Model directory not found</div>'\n",
    "                print(f\"Error: Model directory not found: {model_dir}\")\n",
    "                return\n",
    "            \n",
    "            # Create output directory if it doesn't exist\n",
    "            if output_dir:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                # Update the inference engine's output directory\n",
    "                inference_output_dir = output_dir\n",
    "            else:\n",
    "                inference_output_dir = os.path.join(model_dir, 'inference_results')\n",
    "                os.makedirs(inference_output_dir, exist_ok=True)\n",
    "            \n",
    "            # Update UI state\n",
    "            classify_btn.disabled = True\n",
    "            stop_btn.disabled = False\n",
    "            progress_bar.value = 0\n",
    "            progress_bar.bar_style = 'info'\n",
    "            status_display.value = '<div class=\"status-box status-processing\">🔄 Classification in progress...</div>'\n",
    "            \n",
    "            try:\n",
    "                # Update Config if needed\n",
    "                if cleanup_temp.value:\n",
    "                    Config.CLEANUP_TEMP = True\n",
    "                \n",
    "                print(\"=\"*70)\n",
    "                print(\"STARTING INFERENCE CLASSIFICATION\")\n",
    "                print(\"=\"*70)\n",
    "                print(f\"PCAP File: {os.path.basename(pcap_file)}\")\n",
    "                print(f\"Model: {model_name}\")\n",
    "                print(f\"Model Directory: {model_dir}\")\n",
    "                print(\"-\"*70)\n",
    "                \n",
    "                # Simulate progress updates\n",
    "                progress_bar.value = 20\n",
    "                \n",
    "                # Run classification with custom output directory\n",
    "                results = classify_new_pcap_enhanced(\n",
    "                    pcap_file=pcap_file,\n",
    "                    model_name=model_name,\n",
    "                    export=export_results.value,\n",
    "                    model_dir=model_dir,\n",
    "                    output_dir=inference_output_dir,\n",
    "                    with_ground_truth=with_ground_truth.value\n",
    "                )\n",
    "                \n",
    "                progress_bar.value = 100\n",
    "                progress_bar.bar_style = 'success'\n",
    "                \n",
    "                if results:\n",
    "                    status_display.value = '<div class=\"status-box status-success\">✅ Classification complete!</div>'\n",
    "                    \n",
    "                    # Display summary\n",
    "                    print(\"\\n\" + \"=\"*70)\n",
    "                    print(\"CLASSIFICATION COMPLETE - SUMMARY\")\n",
    "                    print(\"=\"*70)\n",
    "                    print(f\"Total Flows Analyzed: {results['total_flows']:,}\")\n",
    "                    print(f\"Model Used: {results['model_used']}\")\n",
    "                    print(f\"Export Directory: {results['export_directory']}\")\n",
    "                    \n",
    "                    # Show attack distribution\n",
    "                    if results['attack_distribution']:\n",
    "                        print(\"\\nTop Attack Types Detected:\")\n",
    "                        sorted_attacks = sorted(results['attack_distribution'].items(), \n",
    "                                              key=lambda x: x[1], reverse=True)\n",
    "                        for attack_type, count in sorted_attacks[:5]:\n",
    "                            percentage = (count / results['total_flows']) * 100\n",
    "                            print(f\"  {attack_type}: {count:,} ({percentage:.1f}%)\")\n",
    "                else:\n",
    "                    status_display.value = '<div class=\"status-box status-error\">❌ Classification failed</div>'\n",
    "                    progress_bar.bar_style = 'danger'\n",
    "                    \n",
    "            except Exception as e:\n",
    "                status_display.value = f'<div class=\"status-box status-error\">❌ Error: {str(e)}</div>'\n",
    "                progress_bar.bar_style = 'danger'\n",
    "                print(f\"\\nError during classification: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            finally:\n",
    "                # Reset UI state\n",
    "                classify_btn.disabled = False\n",
    "                stop_btn.disabled = True\n",
    "    \n",
    "    classify_btn.on_click(on_classify)\n",
    "    \n",
    "    def on_clear(b):\n",
    "        \"\"\"Clear output area\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "        progress_bar.value = 0\n",
    "        progress_bar.bar_style = ''\n",
    "        status_display.value = '<div class=\"status-box status-ready\">✅ Ready to classify</div>'\n",
    "    \n",
    "    clear_btn.on_click(on_clear)\n",
    "    \n",
    "    # ========== LAYOUT ASSEMBLY ==========\n",
    "    \n",
    "    # Model section\n",
    "    model_box = widgets.VBox([\n",
    "        widgets.HTML(\"<div class='inference-section'>\"),\n",
    "        model_dir_input,\n",
    "        widgets.HBox([check_models_btn, model_status]),\n",
    "        model_selector,\n",
    "        widgets.HTML(\"</div>\")\n",
    "    ])\n",
    "    \n",
    "    # PCAP section\n",
    "    pcap_box = widgets.VBox([\n",
    "        widgets.HTML(\"<div class='inference-section'>\"),\n",
    "        pcap_input,\n",
    "        file_info,\n",
    "        output_dir_input,\n",
    "        widgets.HTML(\"</div>\")\n",
    "    ])\n",
    "    \n",
    "    # Options section\n",
    "    options_box = widgets.VBox([\n",
    "        widgets.HTML(\"<div class='inference-section'>\"),\n",
    "        with_ground_truth,\n",
    "        export_results,\n",
    "        generate_viz,\n",
    "        cleanup_temp,\n",
    "        advanced_options,\n",
    "        widgets.HTML(\"</div>\")\n",
    "    ])\n",
    "    \n",
    "    # Controls section\n",
    "    controls_box = widgets.HBox([\n",
    "        classify_btn,\n",
    "        stop_btn,\n",
    "        clear_btn\n",
    "    ])\n",
    "    \n",
    "    # Progress section\n",
    "    progress_box = widgets.VBox([\n",
    "        progress_bar,\n",
    "        status_display\n",
    "    ])\n",
    "    \n",
    "    # Main layout\n",
    "    main_layout = widgets.VBox([\n",
    "        model_box,\n",
    "        pcap_box,\n",
    "        options_box,\n",
    "        progress_box,\n",
    "        controls_box,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    display(main_layout)\n",
    "    \n",
    "    # Display instructions\n",
    "    display(HTML(\"\"\"\n",
    "    <div style='background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); \n",
    "                padding: 20px; margin-top: 20px; border-radius: 10px; border-left: 4px solid #2196F3; color: black;'>\n",
    "        <h4 style='color: black;'>Quick Start Guide</h4>\n",
    "        <ol style='color: black;'>\n",
    "            <li><strong>Prepare:</strong> Ensure you have trained models saved from the main pipeline</li>\n",
    "            <li><strong>Configure:</strong> Enter the model directory path and click \"Check Models\"</li>\n",
    "            <li><strong>Select:</strong> Choose your PCAP file to analyze</li>\n",
    "            <li><strong>Options:</strong> Select the model and configure export options</li>\n",
    "            <li><strong>Analyze:</strong> Click \"Classify PCAP\" to start the analysis</li>\n",
    "            <li><strong>Review:</strong> Check the results in the output area and exported files</li>\n",
    "        </ol>\n",
    "        <p style='color: black;'><strong>Tip:</strong> First-time classification will take longer as features are extracted. \n",
    "        Subsequent runs on the same file will be faster.</p>\n",
    "        <p style='color: black;'><strong>Note:</strong> Large PCAP files (>1GB) may take several minutes to process.</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Memory status\n",
    "    mem = psutil.virtual_memory()\n",
    "    disk = psutil.disk_usage('/')\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div style='background: #f8f9fa; padding: 15px; margin-top: 15px; border-radius: 8px; color: black;'>\n",
    "        <h4 style='color: black;'>System Status</h4>\n",
    "        <p style='color: black;'>Memory Available: {mem.available / (1024**3):.2f} GB ({100-mem.percent:.1f}% free)</p>\n",
    "        <p style='color: black;'>Disk Space: {disk.free / (1024**3):.2f} GB available</p>\n",
    "        <p style='color: black;'>Ready for inference analysis</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "\n",
    "# ========== EXECUTE UI CREATION ==========\n",
    "# Display the comprehensive UI when cell is run\n",
    "create_comprehensive_inference_ui()\n",
    "\n",
    "# Also provide manual function call option\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE MODE READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"The interactive UI is displayed above. You can also call functions manually:\")\n",
    "print(\"\\nManual usage example:\")\n",
    "print(\"  results = classify_new_pcap_enhanced(\")\n",
    "print(\"      pcap_file='/path/to/your.pcap',\")\n",
    "print(\"      model_name='random_forest',\")\n",
    "print(\"      model_dir='./analysis_output'\")\n",
    "print(\"  )\")\n",
    "print(\"\\nUse the UI above for easier interaction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e28aa",
   "metadata": {},
   "source": [
    "# ### Step 13: (Optional: only use if ML crashed..) Resume ML Training from Emergency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Resume ML Training from Emergency Features - FIXED WITH MODEL SELECTION\n",
    "\"\"\"\n",
    "PURPOSE: Resume machine learning training from saved emergency features with proper model selection\n",
    "FIXED: Added separate model selection controls independent of feature selection\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads processed features from emergency_features directory\n",
    "2. Allows selection of SPECIFIC ML MODELS to train\n",
    "3. Allows selection of feature method independently\n",
    "4. Resumes ML training with user-selected models only\n",
    "5. Runs comprehensive evaluation with multiple metrics\n",
    "6. Generates visualizations and exports results\n",
    "\n",
    "MAJOR FIX: Now properly separates model selection from feature selection\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "# FIRST: Add the missing generate_report method to MemoryOptimizedResultExporter\n",
    "def add_generate_report_method():\n",
    "    \"\"\"Add the missing generate_report method to MemoryOptimizedResultExporter class\"\"\"\n",
    "    \n",
    "    def generate_report(self, features_dir, ml_results, selected_features):\n",
    "        \"\"\"Generate comprehensive training report\"\"\"\n",
    "        print(\"Generating comprehensive training report...\")\n",
    "        \n",
    "        try:\n",
    "            from datetime import datetime\n",
    "            \n",
    "            # Create report content\n",
    "            report_lines = []\n",
    "            report_lines.append(\"=\"*80)\n",
    "            report_lines.append(\"NETWORK ATTACK DETECTION - TRAINING REPORT\")\n",
    "            report_lines.append(\"=\"*80)\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            report_lines.append(f\"Features Directory: {features_dir}\")\n",
    "            report_lines.append(f\"Output Directory: {self.output_dir}\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            # Feature information\n",
    "            report_lines.append(\"FEATURE INFORMATION\")\n",
    "            report_lines.append(\"-\" * 50)\n",
    "            report_lines.append(f\"Total Features Used: {len(selected_features)}\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"Selected Features:\")\n",
    "            for i, feature in enumerate(selected_features[:20], 1):\n",
    "                report_lines.append(f\"  {i:2d}. {feature}\")\n",
    "            if len(selected_features) > 20:\n",
    "                report_lines.append(f\"  ... and {len(selected_features) - 20} more features\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            # Model performance summary\n",
    "            if ml_results:\n",
    "                report_lines.append(\"MODEL PERFORMANCE SUMMARY\")\n",
    "                report_lines.append(\"-\" * 50)\n",
    "                \n",
    "                # Summary table header\n",
    "                report_lines.append(f\"{'Model':<20} {'Accuracy':<10} {'F1-Score':<10} {'Attack Det.':<12} {'FPR':<8} {'Status':<12}\")\n",
    "                report_lines.append(\"-\" * 80)\n",
    "                \n",
    "                for model_name, results in ml_results.items():\n",
    "                    accuracy = results.get('accuracy', 0)\n",
    "                    f1_macro = results.get('f1_macro', 0)\n",
    "                    attack_detection = results.get('attack_detection_rate', 0)\n",
    "                    fpr = results.get('false_positive_rate', 0)\n",
    "                    unique_preds = results.get('unique_predictions', 0)\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if unique_preds == 1:\n",
    "                        status = \"BROKEN\"\n",
    "                    elif attack_detection < 0.3:\n",
    "                        status = \"POOR\"\n",
    "                    elif attack_detection < 0.7:\n",
    "                        status = \"FAIR\"\n",
    "                    elif attack_detection < 0.9:\n",
    "                        status = \"GOOD\"\n",
    "                    else:\n",
    "                        status = \"EXCELLENT\"\n",
    "                    \n",
    "                    report_lines.append(f\"{model_name:<20} {accuracy:<10.4f} {f1_macro:<10.4f} {attack_detection:<12.4f} {fpr:<8.4f} {status:<12}\")\n",
    "                \n",
    "                report_lines.append(\"\")\n",
    "                \n",
    "                # Detailed per-model analysis\n",
    "                for model_name, results in ml_results.items():\n",
    "                    report_lines.append(f\"{model_name.upper()} - DETAILED METRICS:\")\n",
    "                    report_lines.append(f\"  Accuracy:              {results.get('accuracy', 0):.4f}\")\n",
    "                    report_lines.append(f\"  Precision (Macro):     {results.get('precision_macro', 0):.4f}\")\n",
    "                    report_lines.append(f\"  Recall (Macro):        {results.get('recall_macro', 0):.4f}\")\n",
    "                    report_lines.append(f\"  F1-Score (Macro):      {results.get('f1_macro', 0):.4f}\")\n",
    "                    report_lines.append(f\"  Attack Detection Rate: {results.get('attack_detection_rate', 0):.4f}\")\n",
    "                    report_lines.append(f\"  False Positive Rate:   {results.get('false_positive_rate', 0):.4f}\")\n",
    "                    report_lines.append(f\"  Test Samples:          {results.get('test_samples', 0):,}\")\n",
    "                    report_lines.append(\"\")\n",
    "            \n",
    "            # Training configuration\n",
    "            report_lines.append(\"TRAINING CONFIGURATION\")\n",
    "            report_lines.append(\"-\" * 50)\n",
    "            report_lines.append(\"Dataset: CICIDS2017\")\n",
    "            report_lines.append(\"Algorithms: User Selected from LightGBM, SGD Classifier, Random Forest, XGBoost\")\n",
    "            report_lines.append(\"Feature Selection: Mutual Information + Variance Analysis\")\n",
    "            report_lines.append(\"Cross-Validation: Stratified Train/Test Split\")\n",
    "            report_lines.append(\"Class Balancing: Enabled for Imbalanced Attack Classes\")\n",
    "            report_lines.append(\"Memory Optimization: Batch Processing with Checkpoints\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            # Usage instructions\n",
    "            report_lines.append(\"MODEL DEPLOYMENT INSTRUCTIONS\")\n",
    "            report_lines.append(\"-\" * 50)\n",
    "            report_lines.append(\"To use the trained models for new network traffic analysis:\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"```python\")\n",
    "            report_lines.append(\"import pickle\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"# Load model\")\n",
    "            report_lines.append(\"with open('lightgbm_model.pkl', 'rb') as f:\")\n",
    "            report_lines.append(\"    model = pickle.load(f)\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"# Load scaler\")\n",
    "            report_lines.append(\"with open('feature_scaler.pkl', 'rb') as f:\")\n",
    "            report_lines.append(\"    scaler = pickle.load(f)\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"# Load feature names\")\n",
    "            report_lines.append(\"with open('selected_features.pkl', 'rb') as f:\")\n",
    "            report_lines.append(\"    feature_names = pickle.load(f)\")\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"# For new data:\")\n",
    "            report_lines.append(\"# 1. Extract same features used in training\")\n",
    "            report_lines.append(\"# 2. Select only features in 'feature_names'\")\n",
    "            report_lines.append(\"# 3. Scale: X_scaled = scaler.transform(X)\")\n",
    "            report_lines.append(\"# 4. Predict: predictions = model.predict(X_scaled)\")\n",
    "            report_lines.append(\"```\")\n",
    "            report_lines.append(\"\")\n",
    "            \n",
    "            # Footer\n",
    "            report_lines.append(\"=\"*80)\n",
    "            report_lines.append(\"END OF REPORT\")\n",
    "            report_lines.append(\"=\"*80)\n",
    "            \n",
    "            # Save report\n",
    "            report_path = os.path.join(self.output_dir, 'training_report.txt')\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write('\\n'.join(report_lines))\n",
    "            \n",
    "            print(f\"✓ Training report saved: {report_path}\")\n",
    "            \n",
    "            # Also create a summary version\n",
    "            summary_lines = []\n",
    "            summary_lines.append(\"TRAINING SUMMARY\")\n",
    "            summary_lines.append(\"=\" * 40)\n",
    "            summary_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            summary_lines.append(f\"Features Used: {len(selected_features)}\")\n",
    "            \n",
    "            if ml_results:\n",
    "                for model_name, results in ml_results.items():\n",
    "                    accuracy = results.get('accuracy', 0)\n",
    "                    attack_detection = results.get('attack_detection_rate', 0)\n",
    "                    summary_lines.append(f\"{model_name}: Accuracy={accuracy:.3f}, Attack Detection={attack_detection:.3f}\")\n",
    "            \n",
    "            summary_path = os.path.join(self.output_dir, 'training_summary.txt')\n",
    "            with open(summary_path, 'w') as f:\n",
    "                f.write('\\n'.join(summary_lines))\n",
    "            \n",
    "            print(f\"✓ Training summary saved: {summary_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating report: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Add the method to the existing class\n",
    "    MemoryOptimizedResultExporter.generate_report = generate_report\n",
    "    print(\"✓ Added generate_report method to MemoryOptimizedResultExporter class\")\n",
    "\n",
    "# Apply the fix before running the main function\n",
    "add_generate_report_method()\n",
    "\n",
    "def resume_ml_training_from_emergency_fixed(features_dir, output_dir, selected_models, use_feature_selection=False, top_n_features=50, selected_features_path=None):\n",
    "    \"\"\"\n",
    "    FIXED: Resume ML training with proper model selection\n",
    "    \n",
    "    Args:\n",
    "        features_dir: Path to directory containing emergency feature files\n",
    "        output_dir: Path to output directory for results\n",
    "        selected_models: List of models to train ['sgd', 'random_forest', 'lightgbm', 'xgboost_incremental']\n",
    "        use_feature_selection: If True, run feature selection to pick top N features\n",
    "        top_n_features: Number of features to select (if use_feature_selection=True)\n",
    "        selected_features_path: Path to saved selected features file (optional)\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"RESUMING ML TRAINING FROM EMERGENCY FEATURES (FIXED)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Validate and create paths\n",
    "    if not os.path.exists(features_dir):\n",
    "        print(f\"Error: Emergency features directory not found: {features_dir}\")\n",
    "        print(\"Please verify the path is correct\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Creating output directory: {output_dir}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Using features directory: {features_dir}\")\n",
    "    print(f\"Using output directory: {output_dir}\")\n",
    "    print(f\"SELECTED MODELS: {selected_models}\")  # NEW: Show which models will be trained\n",
    "    \n",
    "    # Count available batch files\n",
    "    import glob\n",
    "    batch_files = glob.glob(os.path.join(features_dir, 'batch_*.parquet'))\n",
    "    print(f\"Found {len(batch_files)} emergency feature batches\")\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(\"Error: No batch files found in emergency features\")\n",
    "        print(\"Expected files matching pattern: batch_*.parquet\")\n",
    "        return None\n",
    "    \n",
    "    # Handle feature selection\n",
    "    if use_feature_selection:\n",
    "        print(f\"Running feature selection to choose top {top_n_features} features...\")\n",
    "        \n",
    "        # Check if pre-saved selected features exist\n",
    "        if selected_features_path and os.path.exists(selected_features_path):\n",
    "            print(f\"Loading pre-saved selected features from: {selected_features_path}\")\n",
    "            try:\n",
    "                with open(selected_features_path, 'rb') as f:\n",
    "                    selected_features = pickle.load(f)\n",
    "                print(f\"Loaded {len(selected_features)} pre-selected features\")\n",
    "                # Trim to requested number if needed\n",
    "                selected_features = selected_features[:top_n_features]\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading selected features: {e}\")\n",
    "                print(\"Falling back to feature selection...\")\n",
    "                selected_features = None\n",
    "        else:\n",
    "            selected_features = None\n",
    "        \n",
    "        # Run feature analysis if no pre-saved features\n",
    "        if selected_features is None:\n",
    "            analyzer = MemoryOptimizedFeatureAnalyzer()\n",
    "            selected_features = analyzer.analyze_features_incrementally(features_dir)\n",
    "            \n",
    "            # Take only the top N features\n",
    "            selected_features = selected_features[:top_n_features]\n",
    "            print(f\"Selected top {len(selected_features)} features\")\n",
    "            \n",
    "            # Save for future use\n",
    "            selected_features_save_path = os.path.join(output_dir, 'selected_features.pkl')\n",
    "            with open(selected_features_save_path, 'wb') as f:\n",
    "                pickle.dump(selected_features, f)\n",
    "            print(f\"Saved selected features to: {selected_features_save_path}\")\n",
    "    else:\n",
    "        # Use all available features from emergency data\n",
    "        print(\"Using all available features from emergency data...\")\n",
    "        sample_batch = pd.read_parquet(batch_files[0])\n",
    "        exclude_cols = {'flow_id', 'label', 'attack_type', 'label_confidence', 'src_ip', 'dst_ip'}\n",
    "        selected_features = [col for col in sample_batch.columns if col not in exclude_cols]\n",
    "        print(f\"Using all {len(selected_features)} available features\")\n",
    "        \n",
    "        # Display sample features\n",
    "        print(\"Sample features:\", selected_features[:10])\n",
    "        del sample_batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # Quick analysis of emergency features\n",
    "    print(\"\\nAnalyzing emergency features...\")\n",
    "    total_flows = 0\n",
    "    label_dist = Counter()\n",
    "    \n",
    "    for batch_file in batch_files:\n",
    "        batch = pd.read_parquet(batch_file)\n",
    "        total_flows += len(batch)\n",
    "        \n",
    "        if 'label' in batch.columns:\n",
    "            batch_labels = Counter(batch['label'])\n",
    "            label_dist.update(batch_labels)\n",
    "        \n",
    "        del batch\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Total flows in emergency features: {total_flows:,}\")\n",
    "    \n",
    "    # Display label distribution\n",
    "    if label_dist:\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        total_labels = sum(label_dist.values())\n",
    "        benign_count = label_dist.get(0, 0)\n",
    "        attack_count = total_labels - benign_count\n",
    "        \n",
    "        print(f\"  BENIGN (0): {benign_count:,} ({benign_count/total_labels:.1%})\")\n",
    "        print(f\"  ATTACKS (1-14): {attack_count:,} ({attack_count/total_labels:.1%})\")\n",
    "        \n",
    "        if attack_count / total_labels > 0.05:\n",
    "            print(\"Good: Sufficient attack examples for training\")\n",
    "        else:\n",
    "            print(\"Warning: Low attack representation - may still have class imbalance\")\n",
    "    \n",
    "    # FIXED: Initialize ML pipeline with ONLY selected models\n",
    "    print(f\"\\nInitializing ML pipeline with SELECTED models: {selected_models}\")\n",
    "    ml_pipeline = MemoryOptimizedMLPipelineFixed(selected_models=selected_models)\n",
    "    \n",
    "    # Resume training\n",
    "    print(\"Resuming ML training with ONLY selected models...\")\n",
    "    print(\"This will use the fixed create_stratified_split method\")\n",
    "    \n",
    "    try:\n",
    "        models, ml_results = ml_pipeline.train_models_incrementally(\n",
    "            features_dir, selected_features\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ML TRAINING RESUMED SUCCESSFULLY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Display key results\n",
    "        if ml_results:\n",
    "            for model_name, results in ml_results.items():\n",
    "                unique_preds = results.get('unique_predictions', 0)\n",
    "                accuracy = results.get('accuracy', 0)\n",
    "                attack_detection = results.get('attack_detection_rate', 0)\n",
    "                \n",
    "                print(f\"\\n{model_name.upper()}:\")\n",
    "                print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  Attack Detection Rate: {attack_detection:.4f}\")\n",
    "                print(f\"  Unique Predictions: {unique_preds}\")\n",
    "                \n",
    "                if unique_preds == 1:\n",
    "                    print(\"  STATUS: BROKEN - Only predicts one class\")\n",
    "                elif attack_detection < 0.3:\n",
    "                    print(\"  STATUS: POOR - Low attack detection\")\n",
    "                elif attack_detection > 0.7:\n",
    "                    print(\"  STATUS: GOOD - Solid attack detection\")\n",
    "                else:\n",
    "                    print(\"  STATUS: FAIR - Moderate performance\")\n",
    "        \n",
    "        # Generate visualizations if enabled\n",
    "        if Config.GENERATE_VISUALS and ml_results:\n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            print(\"GENERATING VISUALIZATIONS\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            try:\n",
    "                visualizer = MemoryOptimizedVisualizer(\n",
    "                    features_dir, ml_results, output_dir\n",
    "                )\n",
    "                visualizer.create_all_visualizations()\n",
    "                print(\"Visualizations created successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Visualization creation failed: {e}\")\n",
    "        \n",
    "        # Export results - NOW WITH WORKING generate_report METHOD\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"EXPORTING RESULTS\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        try:\n",
    "            exporter = MemoryOptimizedResultExporter(output_dir)\n",
    "            exporter.export_models(models, ml_pipeline.scaler, selected_features)\n",
    "            exporter.generate_report(features_dir, ml_results, selected_features)\n",
    "            print(\"Results exported successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Export failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            'models': models,\n",
    "            'ml_results': ml_results,\n",
    "            'selected_features': selected_features,\n",
    "            'selected_models': selected_models,  # NEW: Include which models were trained\n",
    "            'features_dir': features_dir,\n",
    "            'output_dir': output_dir,\n",
    "            'total_flows': total_flows\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during ML training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# FIXED: Modified ML Pipeline class to only train selected models\n",
    "class MemoryOptimizedMLPipelineFixed(MemoryOptimizedMLPipeline):\n",
    "    def __init__(self, selected_models=None):\n",
    "        \"\"\"\n",
    "        Initialize ML pipeline with specific model selection\n",
    "        \n",
    "        Args:\n",
    "            selected_models: List of models to train (e.g., ['sgd', 'lightgbm'])\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.selected_models = selected_models or ['sgd', 'lightgbm']  # Default to SGD and LightGBM\n",
    "        print(f\"ML Pipeline initialized for models: {self.selected_models}\")\n",
    "    \n",
    "    def initialize_enhanced_models(self):\n",
    "        \"\"\"\n",
    "        FIXED: Initialize ONLY selected models\n",
    "        \"\"\"\n",
    "        print(f\"\\nInitializing ONLY selected models: {self.selected_models}\")\n",
    "        \n",
    "        if 'sgd' in self.selected_models:\n",
    "            print(\"- Initializing Enhanced SGD Classifier...\")\n",
    "            from sklearn.linear_model import SGDClassifier\n",
    "            \n",
    "            self.models['sgd'] = SGDClassifier(\n",
    "                loss='log_loss',  # Fixed deprecated parameter\n",
    "                penalty='l2',\n",
    "                alpha=0.001,\n",
    "                max_iter=1000,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                n_jobs=Config.N_JOBS\n",
    "            )\n",
    "        \n",
    "        if 'random_forest' in self.selected_models:\n",
    "            print(\"- Random Forest will be trained in batch mode...\")\n",
    "            self.models['random_forest'] = None\n",
    "        \n",
    "        if 'lightgbm' in self.selected_models:\n",
    "            print(\"- LightGBM will be trained in batch mode...\")\n",
    "            self.models['lightgbm'] = None\n",
    "        \n",
    "        if 'xgboost_incremental' in self.selected_models:\n",
    "            print(\"- XGBoost will be trained in batch mode...\")\n",
    "            self.models['xgboost_incremental'] = None\n",
    "        \n",
    "        print(f\"Models to be trained: {list(self.models.keys())}\")\n",
    "\n",
    "def quick_feature_analysis(features_dir):\n",
    "    \"\"\"\n",
    "    Quick analysis of what's in the emergency features\n",
    "    \n",
    "    Args:\n",
    "        features_dir: Path to directory containing emergency feature files\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"EMERGENCY FEATURES ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(features_dir):\n",
    "        print(f\"Error: Features directory not found: {features_dir}\")\n",
    "        return\n",
    "    \n",
    "    import glob\n",
    "    batch_files = glob.glob(os.path.join(features_dir, 'batch_*.parquet'))\n",
    "    \n",
    "    if not batch_files:\n",
    "        print(f\"No emergency features found in: {features_dir}\")\n",
    "        print(\"Expected files matching pattern: batch_*.parquet\")\n",
    "        return\n",
    "    \n",
    "    # Analyze first batch\n",
    "    sample_batch = pd.read_parquet(batch_files[0])\n",
    "    \n",
    "    print(f\"Features directory: {features_dir}\")\n",
    "    print(f\"Batch files: {len(batch_files)}\")\n",
    "    print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "    print(f\"Features per flow: {len(sample_batch.columns)}\")\n",
    "    \n",
    "    # Show column types\n",
    "    print(f\"\\nColumn breakdown:\")\n",
    "    print(f\"  Metadata columns: {len([col for col in sample_batch.columns if col in ['flow_id', 'src_ip', 'dst_ip']])}\")\n",
    "    print(f\"  Feature columns: {len([col for col in sample_batch.columns if col not in ['flow_id', 'label', 'attack_type', 'label_confidence']])}\")\n",
    "    \n",
    "    # Check for labels\n",
    "    if 'label' in sample_batch.columns:\n",
    "        label_dist = sample_batch['label'].value_counts()\n",
    "        print(f\"\\nSample label distribution:\")\n",
    "        for label, count in label_dist.head(10).items():\n",
    "            print(f\"  Label {label}: {count}\")\n",
    "    \n",
    "    # Show sample features\n",
    "    feature_cols = [col for col in sample_batch.columns \n",
    "                   if col not in ['flow_id', 'label', 'attack_type', 'label_confidence']]\n",
    "    \n",
    "    print(f\"\\nSample feature columns:\")\n",
    "    for col in feature_cols[:15]:\n",
    "        print(f\"  {col}\")\n",
    "    if len(feature_cols) > 15:\n",
    "        print(f\"  ... and {len(feature_cols) - 15} more\")\n",
    "    \n",
    "    del sample_batch\n",
    "    gc.collect()\n",
    "\n",
    "def validate_path(path_widget, path_type):\n",
    "    \"\"\"\n",
    "    Validate if a path exists and update widget style\n",
    "    \n",
    "    Args:\n",
    "        path_widget: The path input widget\n",
    "        path_type: Type of path for display ('directory' or 'file')\n",
    "    \"\"\"\n",
    "    path = path_widget.value.strip()\n",
    "    if not path:\n",
    "        path_widget.style = {'description_width': 'initial', 'text_color': 'orange'}\n",
    "        return False\n",
    "    \n",
    "    if path_type == 'directory':\n",
    "        exists = os.path.isdir(path)\n",
    "    else:\n",
    "        exists = os.path.isfile(path)\n",
    "    \n",
    "    if exists:\n",
    "        path_widget.style = {'description_width': 'initial', 'text_color': 'green'}\n",
    "        return True\n",
    "    else:\n",
    "        path_widget.style = {'description_width': 'initial', 'text_color': 'red'}\n",
    "        return False\n",
    "\n",
    "def browse_for_path(path_widget, path_type='directory'):\n",
    "    \"\"\"\n",
    "    Simple path browser helper (displays common paths for user reference)\n",
    "    \n",
    "    Args:\n",
    "        path_widget: The path input widget to update\n",
    "        path_type: Type of path ('directory' or 'file')\n",
    "    \"\"\"\n",
    "    common_paths = {\n",
    "        'directory': [\n",
    "            './analysis_output/emergency_features',\n",
    "            './output',\n",
    "            './data/processed',\n",
    "            '../emergency_features',\n",
    "            '~/ml_project/emergency_features'\n",
    "        ],\n",
    "        'file': [\n",
    "            './analysis_output/selected_features.pkl',\n",
    "            './output/selected_features.pkl',\n",
    "            './data/selected_features.pkl'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(f\"Common {path_type} paths for reference:\")\n",
    "    for i, path in enumerate(common_paths.get(path_type, []), 1):\n",
    "        exists = os.path.exists(path)\n",
    "        status = \"✓\" if exists else \"✗\"\n",
    "        print(f\"  {i}. {path} {status}\")\n",
    "\n",
    "# FIXED: Enhanced Interactive UI with proper model selection\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def setup_fixed_resume_ui():\n",
    "    \"\"\"\n",
    "    FIXED: Create interactive UI with proper model selection separate from feature selection\n",
    "    \"\"\"\n",
    "    display(HTML(\"<h3>Resume ML Training from Emergency Features (FIXED)</h3>\"))\n",
    "    display(HTML(\"<p><em>Select specific ML models and configure feature options independently.</em></p>\"))\n",
    "    \n",
    "    # Path configuration section\n",
    "    display(HTML(\"<h4>📁 Path Configuration</h4>\"))\n",
    "    \n",
    "    # Emergency features directory path\n",
    "    features_path = widgets.Text(\n",
    "        value='./analysis_output/emergency_features',\n",
    "        placeholder='Enter path to emergency features directory',\n",
    "        description='Features Directory:',\n",
    "        style={'description_width': '150px'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Browse button for features path\n",
    "    features_browse_btn = widgets.Button(\n",
    "        description='Browse Examples',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    # Output directory path\n",
    "    output_path = widgets.Text(\n",
    "        value='./analysis_output',\n",
    "        placeholder='Enter path to output directory',\n",
    "        description='Output Directory:',\n",
    "        style={'description_width': '150px'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Browse button for output path\n",
    "    output_browse_btn = widgets.Button(\n",
    "        description='Browse Examples',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    # Selected features file path (optional)\n",
    "    selected_features_path = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='(Optional) Path to saved selected_features.pkl file',\n",
    "        description='Selected Features:',\n",
    "        style={'description_width': '150px'},\n",
    "        layout=widgets.Layout(width='600px')\n",
    "    )\n",
    "    \n",
    "    # Browse button for selected features\n",
    "    features_file_browse_btn = widgets.Button(\n",
    "        description='Browse Examples',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    \n",
    "    # Path validation status\n",
    "    path_status = widgets.HTML(value=\"<p><em>Enter paths above and they will be validated automatically</em></p>\")\n",
    "    \n",
    "    # NEW: Model selection configuration\n",
    "    display(HTML(\"<h4>🤖 Machine Learning Model Selection</h4>\"))\n",
    "    \n",
    "    model_selection = widgets.SelectMultiple(\n",
    "        options=[\n",
    "            ('SGD Classifier (Fast, Incremental)', 'sgd'),\n",
    "            ('LightGBM (Best Performance)', 'lightgbm'),\n",
    "            ('Random Forest (Balanced)', 'random_forest'),\n",
    "            ('XGBoost (Advanced)', 'xgboost_incremental')\n",
    "        ],\n",
    "        value=['lightgbm'],  # Default to LightGBM\n",
    "        description='Select Models:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='600px', height='100px')\n",
    "    )\n",
    "    \n",
    "    model_info = widgets.HTML(value=\"\"\"\n",
    "    <p><strong>Model Recommendations:</strong></p>\n",
    "    <ul>\n",
    "        <li><strong>LightGBM</strong>: Best overall performance for this dataset</li>\n",
    "        <li><strong>SGD Classifier</strong>: Fastest training, good for large datasets</li>\n",
    "        <li><strong>Random Forest</strong>: Most interpretable, balanced performance</li>\n",
    "        <li><strong>XGBoost</strong>: Advanced boosting, may take longer to train</li>\n",
    "    </ul>\n",
    "    <p><em>You can select multiple models to compare performance.</em></p>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Feature selection configuration\n",
    "    display(HTML(\"<h4>🔧 Feature Selection Configuration</h4>\"))\n",
    "    \n",
    "    # Feature selection method\n",
    "    feature_method = widgets.RadioButtons(\n",
    "        options=[\n",
    "            ('Use all available features (faster)', 'all'),\n",
    "            ('Select top N features (slower)', 'select'),\n",
    "            ('Use pre-saved selected features', 'saved')\n",
    "        ],\n",
    "        value='all',\n",
    "        description='Feature Method:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Number of features (only enabled if select method chosen)\n",
    "    num_features = widgets.IntSlider(\n",
    "        value=50,\n",
    "        min=20,\n",
    "        max=100,\n",
    "        step=5,\n",
    "        description='Top N Features:',\n",
    "        style={'description_width': 'initial'},\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Analysis options\n",
    "    display(HTML(\"<h4>📊 Analysis Options</h4>\"))\n",
    "    \n",
    "    run_analysis = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Run quick feature analysis first',\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    # Output area for results\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Control buttons\n",
    "    display(HTML(\"<h4>🚀 Actions</h4>\"))\n",
    "    \n",
    "    validate_btn = widgets.Button(\n",
    "        description='Validate Paths',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='150px', height='40px')\n",
    "    )\n",
    "    \n",
    "    resume_btn = widgets.Button(\n",
    "        description='Resume ML Training',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px', height='40px'),\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    def validate_paths(b=None):\n",
    "        \"\"\"Validate all entered paths and update status\"\"\"\n",
    "        with output_area:\n",
    "            if b:  # Only clear if called from button\n",
    "                clear_output()\n",
    "            \n",
    "            # Validate features directory\n",
    "            features_valid = validate_path(features_path, 'directory')\n",
    "            \n",
    "            # Validate output directory (or check if parent exists for creation)\n",
    "            output_dir = output_path.value.strip()\n",
    "            output_valid = True\n",
    "            if output_dir:\n",
    "                if not os.path.exists(output_dir):\n",
    "                    # Check if parent directory exists\n",
    "                    parent_dir = os.path.dirname(output_dir)\n",
    "                    if parent_dir and not os.path.exists(parent_dir):\n",
    "                        output_valid = False\n",
    "                        output_path.style = {'description_width': '150px', 'text_color': 'red'}\n",
    "                    else:\n",
    "                        output_path.style = {'description_width': '150px', 'text_color': 'orange'}\n",
    "                else:\n",
    "                    output_path.style = {'description_width': '150px', 'text_color': 'green'}\n",
    "            else:\n",
    "                output_valid = False\n",
    "                output_path.style = {'description_width': '150px', 'text_color': 'red'}\n",
    "            \n",
    "            # Validate selected features file (if provided)\n",
    "            selected_features_valid = True\n",
    "            if selected_features_path.value.strip():\n",
    "                selected_features_valid = validate_path(selected_features_path, 'file')\n",
    "            else:\n",
    "                selected_features_path.style = {'description_width': '150px', 'text_color': 'black'}\n",
    "            \n",
    "            # Validate model selection\n",
    "            models_selected = len(model_selection.value) > 0\n",
    "            \n",
    "            # Update status\n",
    "            status_html = \"<h5>Validation Status:</h5><ul>\"\n",
    "            \n",
    "            if features_valid:\n",
    "                status_html += \"<li style='color: green'>✓ Features directory found</li>\"\n",
    "            else:\n",
    "                status_html += \"<li style='color: red'>✗ Features directory not found</li>\"\n",
    "            \n",
    "            if output_valid:\n",
    "                if os.path.exists(output_dir):\n",
    "                    status_html += \"<li style='color: green'>✓ Output directory exists</li>\"\n",
    "                else:\n",
    "                    status_html += \"<li style='color: orange'>⚠ Output directory will be created</li>\"\n",
    "            else:\n",
    "                status_html += \"<li style='color: red'>✗ Output directory path invalid</li>\"\n",
    "            \n",
    "            if models_selected:\n",
    "                status_html += f\"<li style='color: green'>✓ {len(model_selection.value)} model(s) selected</li>\"\n",
    "            else:\n",
    "                status_html += \"<li style='color: red'>✗ No models selected</li>\"\n",
    "            \n",
    "            if selected_features_path.value.strip():\n",
    "                if selected_features_valid:\n",
    "                    status_html += \"<li style='color: green'>✓ Selected features file found</li>\"\n",
    "                else:\n",
    "                    status_html += \"<li style='color: red'>✗ Selected features file not found</li>\"\n",
    "            else:\n",
    "                status_html += \"<li style='color: gray'>- Selected features file not specified (optional)</li>\"\n",
    "            \n",
    "            status_html += \"</ul>\"\n",
    "            \n",
    "            # Enable/disable resume button based on validation\n",
    "            all_valid = features_valid and output_valid and models_selected\n",
    "            resume_btn.disabled = not all_valid\n",
    "            \n",
    "            if all_valid:\n",
    "                status_html += \"<p style='color: green; font-weight: bold'>✓ Ready to resume training!</p>\"\n",
    "            else:\n",
    "                status_html += \"<p style='color: red; font-weight: bold'>✗ Please fix issues before resuming</p>\"\n",
    "            \n",
    "            path_status.value = status_html\n",
    "    \n",
    "    def show_browse_examples(widget, path_type):\n",
    "        \"\"\"Show browse examples for a path type\"\"\"\n",
    "        def handler(b):\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                browse_for_path(widget, path_type)\n",
    "        return handler\n",
    "    \n",
    "    def toggle_feature_options(change):\n",
    "        \"\"\"Enable/disable options based on feature method\"\"\"\n",
    "        method = change['new']\n",
    "        num_features.disabled = (method != 'select')\n",
    "        \n",
    "        # Show/hide selected features path based on method\n",
    "        if method == 'saved':\n",
    "            selected_features_path.layout.display = 'flex'\n",
    "            features_file_browse_btn.layout.display = 'flex'\n",
    "        else:\n",
    "            selected_features_path.layout.display = 'none'\n",
    "            features_file_browse_btn.layout.display = 'none'\n",
    "    \n",
    "    def on_resume(b):\n",
    "        \"\"\"FIXED: Handle resume button click with proper model selection\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            # Get paths\n",
    "            features_dir = features_path.value.strip()\n",
    "            output_dir = output_path.value.strip()\n",
    "            selected_features_file = selected_features_path.value.strip() or None\n",
    "            \n",
    "            # Get selected models\n",
    "            selected_models = list(model_selection.value)\n",
    "            \n",
    "            # Run quick analysis if requested\n",
    "            if run_analysis.value:\n",
    "                print(\"Running quick analysis of emergency features...\\n\")\n",
    "                quick_feature_analysis(features_dir)\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "            \n",
    "            # Resume training with selected options\n",
    "            print(\"Resuming ML training with configuration:\")\n",
    "            print(f\"  Features Directory: {features_dir}\")\n",
    "            print(f\"  Output Directory: {output_dir}\")\n",
    "            print(f\"  Selected Models: {selected_models}\")\n",
    "            print(f\"  Feature Method: {feature_method.value}\")\n",
    "            \n",
    "            if feature_method.value == 'select':\n",
    "                print(f\"  Top Features: {num_features.value}\")\n",
    "                results = resume_ml_training_from_emergency_fixed(\n",
    "                    features_dir=features_dir,\n",
    "                    output_dir=output_dir,\n",
    "                    selected_models=selected_models,  # FIXED: Pass selected models\n",
    "                    use_feature_selection=True,\n",
    "                    top_n_features=num_features.value,\n",
    "                    selected_features_path=selected_features_file\n",
    "                )\n",
    "            elif feature_method.value == 'saved':\n",
    "                if not selected_features_file:\n",
    "                    print(\"Error: Selected features file required for 'saved' method\")\n",
    "                    return\n",
    "                print(f\"  Selected Features File: {selected_features_file}\")\n",
    "                results = resume_ml_training_from_emergency_fixed(\n",
    "                    features_dir=features_dir,\n",
    "                    output_dir=output_dir,\n",
    "                    selected_models=selected_models,  # FIXED: Pass selected models\n",
    "                    use_feature_selection=True,\n",
    "                    top_n_features=num_features.value,  # Will be overridden by saved file\n",
    "                    selected_features_path=selected_features_file\n",
    "                )\n",
    "            else:\n",
    "                print(\"  Using all available features\")\n",
    "                results = resume_ml_training_from_emergency_fixed(\n",
    "                    features_dir=features_dir,\n",
    "                    output_dir=output_dir,\n",
    "                    selected_models=selected_models,  # FIXED: Pass selected models\n",
    "                    use_feature_selection=False\n",
    "                )\n",
    "    \n",
    "    # Connect event handlers\n",
    "    features_browse_btn.on_click(show_browse_examples(features_path, 'directory'))\n",
    "    output_browse_btn.on_click(show_browse_examples(output_path, 'directory'))\n",
    "    features_file_browse_btn.on_click(show_browse_examples(selected_features_path, 'file'))\n",
    "    \n",
    "    feature_method.observe(toggle_feature_options, names='value')\n",
    "    validate_btn.on_click(validate_paths)\n",
    "    resume_btn.on_click(on_resume)\n",
    "    \n",
    "    # Auto-validate paths on change\n",
    "    def auto_validate(change):\n",
    "        validate_paths()\n",
    "    \n",
    "    features_path.observe(auto_validate, names='value')\n",
    "    output_path.observe(auto_validate, names='value')\n",
    "    selected_features_path.observe(auto_validate, names='value')\n",
    "    model_selection.observe(auto_validate, names='value')  # NEW: Also validate on model change\n",
    "    \n",
    "    # Initial setup\n",
    "    toggle_feature_options({'new': feature_method.value})\n",
    "    \n",
    "    # Layout all components\n",
    "    display(widgets.VBox([\n",
    "        # Path configuration\n",
    "        widgets.HBox([features_path, features_browse_btn]),\n",
    "        widgets.HBox([output_path, output_browse_btn]),\n",
    "        widgets.HBox([selected_features_path, features_file_browse_btn]),\n",
    "        path_status,\n",
    "        \n",
    "        # Model selection - NEW SECTION\n",
    "        model_selection,\n",
    "        model_info,\n",
    "        \n",
    "        # Feature selection configuration\n",
    "        feature_method,\n",
    "        num_features,\n",
    "        \n",
    "        # Analysis options\n",
    "        run_analysis,\n",
    "        \n",
    "        # Control buttons\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        widgets.HBox([validate_btn, resume_btn]),\n",
    "        \n",
    "        # Output area\n",
    "        output_area\n",
    "    ]))\n",
    "    \n",
    "    # Run initial validation\n",
    "    validate_paths()\n",
    "\n",
    "# Run the FIXED UI\n",
    "setup_fixed_resume_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdec4d4",
   "metadata": {},
   "source": [
    "# ### Step 14: convert csv file features into parquet manually\n",
    "# ### use this only if emergancy features or checkpoint has failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_csv_to_parquet(features_dir):\n",
    "    \"\"\"\n",
    "    Convert compressed CSV files to Parquet format for resume functionality\n",
    "    \"\"\"\n",
    "    print(\"Converting CSV files to Parquet format...\")\n",
    "    \n",
    "    # Find all CSV.gz files\n",
    "    csv_files = glob.glob(os.path.join(features_dir, 'features_chunk_*.csv.gz'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV.gz files found\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to convert\")\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files):\n",
    "        try:\n",
    "            # Read compressed CSV\n",
    "            df = pd.read_csv(csv_file, compression='gzip')\n",
    "            \n",
    "            # Create corresponding Parquet filename\n",
    "            parquet_file = os.path.join(features_dir, f'batch_{i:04d}.parquet')\n",
    "            \n",
    "            # Save as Parquet\n",
    "            df.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "            \n",
    "            print(f\"Converted: {os.path.basename(csv_file)} -> batch_{i:04d}.parquet\")\n",
    "            \n",
    "            del df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {csv_file}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"Successfully converted {len(csv_files)} files to Parquet format\")\n",
    "    return True\n",
    "\n",
    "# Run the conversion\n",
    "features_dir = \"./analysis_output_XGBoost_Fri_Thur_Tues_Weds/features\"\n",
    "success = convert_csv_to_parquet(features_dir)\n",
    "\n",
    "if success:\n",
    "    print(\"Now you can run the resume function!\")\n",
    "else:\n",
    "    print(\"Conversion failed - check the error messages above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb638c7",
   "metadata": {},
   "source": [
    "# ### Step 15: (Optional) run for Evaluation Only - Generate Actual Report Without Retraining \n",
    "# ### use to regenerate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Evaluation-Only Report Generator - Generate Comprehensive Analysis Without Retraining\n",
    "\n",
    "\"\"\"\n",
    "PURPOSE: Generate comprehensive evaluation reports from existing ML results without retraining\n",
    "This cell loads previously saved ML results and generates detailed analysis reports, \n",
    "confusion matrices, and visualizations without running the training pipeline again.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads saved ML results from pickle files or JSON exports\n",
    "2. Regenerates all evaluation metrics and confusion matrices\n",
    "3. Creates comprehensive text reports with detailed analysis\n",
    "4. Generates all visualizations (confusion matrices, per-class performance, etc.)\n",
    "5. Exports results in multiple formats (TXT, CSV, JSON, LaTeX)\n",
    "6. Provides detailed per-attack-type analysis\n",
    "7. Creates thesis-ready summary tables and figures\n",
    "\n",
    "USE CASES:\n",
    "- Regenerate reports after training completion\n",
    "- Create additional analysis without retraining\n",
    "- Generate thesis-ready outputs in different formats\n",
    "- Perform deeper analysis on existing results\n",
    "- Create publication-quality figures\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Completed training run with saved results\n",
    "- ML results saved in ./analysis_output/\n",
    "- Access to training metadata and model performance data\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "class FlexibleEvaluationReportGenerator:\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation reports from user-specified ML result files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir='./evaluation_reports'):\n",
    "        \"\"\"\n",
    "        Initialize report generator\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory where reports will be saved\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.ml_results = {}\n",
    "        self.training_metadata = {}\n",
    "        self.selected_features = []\n",
    "        self.confusion_matrices = {}\n",
    "        self.classification_reports = {}\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Attack type mapping for CICIDS2017\n",
    "        self.attack_mapping = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS_GoldenEye',\n",
    "            4: 'DoS_Hulk',\n",
    "            5: 'DoS_Slowhttptest',\n",
    "            6: 'DoS_slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web_Attack_BruteForce',\n",
    "            13: 'Web_Attack_SqlInjection',\n",
    "            14: 'Web_Attack_XSS'\n",
    "        }\n",
    "        \n",
    "        # Attack severity mapping\n",
    "        self.attack_severity = {\n",
    "            0: 'INFO', 1: 'HIGH', 2: 'CRITICAL', 3: 'HIGH', 4: 'HIGH',\n",
    "            5: 'HIGH', 6: 'HIGH', 7: 'MEDIUM', 8: 'CRITICAL', 9: 'CRITICAL',\n",
    "            10: 'MEDIUM', 11: 'MEDIUM', 12: 'HIGH', 13: 'CRITICAL', 14: 'HIGH'\n",
    "        }\n",
    "        \n",
    "        # Track loaded files for user reference\n",
    "        self.loaded_files = {\n",
    "            'metrics': None,\n",
    "            'confusion_matrices': None,\n",
    "            'features': None,\n",
    "            'classification_reports': None,\n",
    "            'metadata': None\n",
    "        }\n",
    "    \n",
    "    def load_json_file(self, filepath: str, file_type: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load a JSON file containing ML results\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to JSON file\n",
    "            file_type: Type of file (metrics, classification_report, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if file_type == 'metrics':\n",
    "                self.ml_results = data\n",
    "                self.loaded_files['metrics'] = filepath\n",
    "                print(f\"✓ Loaded metrics from: {filepath}\")\n",
    "                print(f\"  Found {len(self.ml_results)} models\")\n",
    "                return True\n",
    "            \n",
    "            elif file_type == 'classification_reports':\n",
    "                self.classification_reports = data\n",
    "                self.loaded_files['classification_reports'] = filepath\n",
    "                print(f\"✓ Loaded classification reports from: {filepath}\")\n",
    "                return True\n",
    "            \n",
    "            elif file_type == 'metadata':\n",
    "                self.training_metadata = data\n",
    "                self.loaded_files['metadata'] = filepath\n",
    "                print(f\"✓ Loaded training metadata from: {filepath}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {filepath}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_pickle_file(self, filepath: str, file_type: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load a pickle file containing ML results\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to pickle file\n",
    "            file_type: Type of file (results, features, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            if file_type == 'results':\n",
    "                # Handle different pickle formats\n",
    "                if isinstance(data, dict):\n",
    "                    if 'ml_results' in data:\n",
    "                        self.ml_results = data['ml_results']\n",
    "                    else:\n",
    "                        self.ml_results = data\n",
    "                    self.loaded_files['metrics'] = filepath\n",
    "                    print(f\"✓ Loaded results from: {filepath}\")\n",
    "                    print(f\"  Found {len(self.ml_results)} models\")\n",
    "                    return True\n",
    "            \n",
    "            elif file_type == 'features':\n",
    "                if isinstance(data, (list, np.ndarray)):\n",
    "                    self.selected_features = list(data)\n",
    "                    self.loaded_files['features'] = filepath\n",
    "                    print(f\"✓ Loaded {len(self.selected_features)} features from: {filepath}\")\n",
    "                    return True\n",
    "            \n",
    "            elif file_type == 'confusion_matrices':\n",
    "                self.confusion_matrices = data\n",
    "                self.loaded_files['confusion_matrices'] = filepath\n",
    "                print(f\"✓ Loaded confusion matrices from: {filepath}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {filepath}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_csv_file(self, filepath: str, file_type: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load a CSV file containing results or features\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to CSV file\n",
    "            file_type: Type of file\n",
    "        \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            if file_type == 'features':\n",
    "                # Assume first column or column named 'features' contains feature names\n",
    "                if 'features' in df.columns:\n",
    "                    self.selected_features = df['features'].tolist()\n",
    "                else:\n",
    "                    self.selected_features = df.iloc[:, 0].tolist()\n",
    "                self.loaded_files['features'] = filepath\n",
    "                print(f\"✓ Loaded {len(self.selected_features)} features from: {filepath}\")\n",
    "                return True\n",
    "            \n",
    "            elif file_type == 'metrics':\n",
    "                # Try to convert CSV to metrics dict\n",
    "                if 'model' in df.columns or 'Model' in df.columns:\n",
    "                    model_col = 'model' if 'model' in df.columns else 'Model'\n",
    "                    self.ml_results = {}\n",
    "                    for _, row in df.iterrows():\n",
    "                        model_name = row[model_col]\n",
    "                        self.ml_results[model_name] = row.to_dict()\n",
    "                    self.loaded_files['metrics'] = filepath\n",
    "                    print(f\"✓ Loaded metrics from: {filepath}\")\n",
    "                    print(f\"  Found {len(self.ml_results)} models\")\n",
    "                    return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {filepath}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_loaded_data(self) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Validate that minimum required data is loaded\n",
    "        \n",
    "        Returns:\n",
    "            Dict with validation status for each component\n",
    "        \"\"\"\n",
    "        validation = {\n",
    "            'has_metrics': bool(self.ml_results),\n",
    "            'has_models': len(self.ml_results) > 0 if self.ml_results else False,\n",
    "            'has_features': bool(self.selected_features),\n",
    "            'ready_for_report': False\n",
    "        }\n",
    "        \n",
    "        # Minimum requirement: at least one model with metrics\n",
    "        validation['ready_for_report'] = validation['has_metrics'] and validation['has_models']\n",
    "        \n",
    "        return validation\n",
    "    \n",
    "    def generate_comprehensive_report(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report\n",
    "        \n",
    "        Returns:\n",
    "            Path to generated report or None if failed\n",
    "        \"\"\"\n",
    "        if not self.ml_results:\n",
    "            print(\"✗ No ML results loaded. Please load metrics file first.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GENERATING COMPREHENSIVE EVALUATION REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        report_file = self.output_dir / f'comprehensive_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        \n",
    "        with open(report_file, 'w') as f:\n",
    "            # Header\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "            f.write(\"COMPREHENSIVE MACHINE LEARNING EVALUATION REPORT\\n\")\n",
    "            f.write(\"Network Intrusion Detection Analysis\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            # Data Sources\n",
    "            f.write(\"DATA SOURCES\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for file_type, filepath in self.loaded_files.items():\n",
    "                if filepath:\n",
    "                    f.write(f\"{file_type.title()}: {os.path.basename(filepath)}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Executive Summary\n",
    "            f.write(\"EXECUTIVE SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            if self.ml_results:\n",
    "                # Find best model based on F1-macro or accuracy\n",
    "                best_model = None\n",
    "                best_metric = 0\n",
    "                metric_name = 'f1_macro'\n",
    "                \n",
    "                for model_name, results in self.ml_results.items():\n",
    "                    # Try different metric names\n",
    "                    for metric in ['f1_macro', 'f1-score', 'accuracy']:\n",
    "                        if metric in results:\n",
    "                            if results[metric] > best_metric:\n",
    "                                best_metric = results[metric]\n",
    "                                best_model = model_name\n",
    "                                metric_name = metric\n",
    "                            break\n",
    "                \n",
    "                if best_model:\n",
    "                    f.write(f\"Best Performing Model: {best_model} ({metric_name}: {best_metric:.4f})\\n\")\n",
    "                \n",
    "                f.write(f\"Models Evaluated: {len(self.ml_results)}\\n\")\n",
    "                f.write(f\"Features Used: {len(self.selected_features) if self.selected_features else 'Not specified'}\\n\")\n",
    "                \n",
    "                # Calculate averages for available metrics\n",
    "                metrics_to_average = ['accuracy', 'f1_macro', 'f1-score', 'precision', 'recall']\n",
    "                for metric in metrics_to_average:\n",
    "                    values = [r.get(metric, 0) for r in self.ml_results.values() if metric in r]\n",
    "                    if values:\n",
    "                        f.write(f\"Average {metric.replace('_', ' ').title()}: {np.mean(values):.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Detailed Model Analysis\n",
    "            f.write(\"DETAILED MODEL PERFORMANCE ANALYSIS\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "            \n",
    "            for model_name, results in self.ml_results.items():\n",
    "                f.write(f\"{model_name.upper()} MODEL ANALYSIS\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                \n",
    "                # Core metrics - handle different naming conventions\n",
    "                f.write(\"Core Performance Metrics:\\n\")\n",
    "                \n",
    "                # Try different key names for each metric\n",
    "                metric_mappings = {\n",
    "                    'Accuracy': ['accuracy', 'Accuracy', 'acc'],\n",
    "                    'F1-Score (Macro)': ['f1_macro', 'f1-score', 'f1_score', 'F1_macro'],\n",
    "                    'F1-Score (Weighted)': ['f1_weighted', 'f1-weighted', 'F1_weighted'],\n",
    "                    'Precision': ['precision', 'precision_macro', 'Precision'],\n",
    "                    'Recall': ['recall', 'recall_macro', 'Recall']\n",
    "                }\n",
    "                \n",
    "                for metric_display, possible_keys in metric_mappings.items():\n",
    "                    for key in possible_keys:\n",
    "                        if key in results:\n",
    "                            value = results[key]\n",
    "                            if isinstance(value, (int, float)):\n",
    "                                f.write(f\"  {metric_display}: {value:.4f}\\n\")\n",
    "                            break\n",
    "                \n",
    "                # Security-focused metrics if available\n",
    "                if any(k in results for k in ['attack_detection_rate', 'false_positive_rate']):\n",
    "                    f.write(\"\\nSecurity-Focused Metrics:\\n\")\n",
    "                    if 'attack_detection_rate' in results:\n",
    "                        f.write(f\"  Attack Detection Rate: {results['attack_detection_rate']:.4f}\\n\")\n",
    "                    if 'false_positive_rate' in results:\n",
    "                        f.write(f\"  False Positive Rate: {results['false_positive_rate']:.4f}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            # Comparative Analysis\n",
    "            f.write(\"COMPARATIVE MODEL ANALYSIS\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            \n",
    "            # Create comparison table\n",
    "            f.write(f\"{'Model':<25} {'Accuracy':<12} {'F1-Score':<12} {'Precision':<12} {'Recall':<12}\\n\")\n",
    "            f.write(\"-\" * 73 + \"\\n\")\n",
    "            \n",
    "            for model_name, results in self.ml_results.items():\n",
    "                # Get metrics with fallbacks\n",
    "                accuracy = results.get('accuracy', results.get('Accuracy', 0))\n",
    "                f1 = results.get('f1_macro', results.get('f1-score', results.get('f1_score', 0)))\n",
    "                precision = results.get('precision', results.get('precision_macro', 0))\n",
    "                recall = results.get('recall', results.get('recall_macro', 0))\n",
    "                \n",
    "                f.write(f\"{model_name:<25} {accuracy:<12.4f} {f1:<12.4f} {precision:<12.4f} {recall:<12.4f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nReport generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(f\"✓ Report saved to: {report_file}\")\n",
    "        return str(report_file)\n",
    "    \n",
    "    def generate_confusion_matrices(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate confusion matrix visualizations\n",
    "        \n",
    "        Returns:\n",
    "            Path to matrices directory or None if failed\n",
    "        \"\"\"\n",
    "        print(\"\\nGenerating confusion matrices...\")\n",
    "        \n",
    "        matrices_dir = self.output_dir / 'confusion_matrices'\n",
    "        matrices_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        matrices_found = False\n",
    "        \n",
    "        # Check for confusion matrices in ml_results\n",
    "        for model_name, results in self.ml_results.items():\n",
    "            if 'confusion_matrix' in results:\n",
    "                matrices_found = True\n",
    "                cm = np.array(results['confusion_matrix'])\n",
    "                \n",
    "                # Create labels\n",
    "                n_classes = cm.shape[0]\n",
    "                labels = [self.attack_mapping.get(i, f'Class_{i}') for i in range(n_classes)]\n",
    "                \n",
    "                # Plot confusion matrix\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                           xticklabels=labels, yticklabels=labels)\n",
    "                plt.title(f'Confusion Matrix - {model_name.title()}', fontsize=16)\n",
    "                plt.xlabel('Predicted Label', fontsize=12)\n",
    "                plt.ylabel('True Label', fontsize=12)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.yticks(rotation=0)\n",
    "                \n",
    "                # Save\n",
    "                matrix_file = matrices_dir / f'confusion_matrix_{model_name}.png'\n",
    "                plt.savefig(matrix_file, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Also save as CSV\n",
    "                cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "                csv_file = matrices_dir / f'confusion_matrix_{model_name}.csv'\n",
    "                cm_df.to_csv(csv_file)\n",
    "                \n",
    "                print(f\"  ✓ {model_name}: {matrix_file}\")\n",
    "        \n",
    "        # Check separate confusion matrices if loaded\n",
    "        if self.confusion_matrices:\n",
    "            matrices_found = True\n",
    "            for model_name, cm in self.confusion_matrices.items():\n",
    "                if isinstance(cm, (list, np.ndarray)):\n",
    "                    cm = np.array(cm)\n",
    "                    \n",
    "                    # Create labels\n",
    "                    n_classes = cm.shape[0]\n",
    "                    labels = [self.attack_mapping.get(i, f'Class_{i}') for i in range(n_classes)]\n",
    "                    \n",
    "                    # Plot and save (same as above)\n",
    "                    plt.figure(figsize=(12, 10))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                               xticklabels=labels, yticklabels=labels)\n",
    "                    plt.title(f'Confusion Matrix - {model_name.title()}', fontsize=16)\n",
    "                    plt.xlabel('Predicted Label', fontsize=12)\n",
    "                    plt.ylabel('True Label', fontsize=12)\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    plt.yticks(rotation=0)\n",
    "                    \n",
    "                    matrix_file = matrices_dir / f'confusion_matrix_{model_name}.png'\n",
    "                    plt.savefig(matrix_file, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"  ✓ {model_name}: {matrix_file}\")\n",
    "        \n",
    "        if not matrices_found:\n",
    "            print(\"  ✗ No confusion matrices found in loaded data\")\n",
    "            return None\n",
    "        \n",
    "        return str(matrices_dir)\n",
    "    \n",
    "    def generate_summary_statistics(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Generate summary statistics CSV\n",
    "        \n",
    "        Returns:\n",
    "            Path to statistics file or None if failed\n",
    "        \"\"\"\n",
    "        if not self.ml_results:\n",
    "            print(\"✗ No ML results loaded\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nGenerating summary statistics...\")\n",
    "        \n",
    "        # Collect all metrics\n",
    "        summary_data = []\n",
    "        \n",
    "        for model_name, results in self.ml_results.items():\n",
    "            row = {'Model': model_name}\n",
    "            \n",
    "            # Add all numeric metrics\n",
    "            for key, value in results.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    row[key] = value\n",
    "            \n",
    "            summary_data.append(row)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Save to CSV\n",
    "        stats_file = self.output_dir / f'summary_statistics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        df.to_csv(stats_file, index=False)\n",
    "        \n",
    "        # Also create a descriptive statistics file\n",
    "        desc_stats = df.describe()\n",
    "        desc_file = self.output_dir / f'descriptive_statistics_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        desc_stats.to_csv(desc_file)\n",
    "        \n",
    "        print(f\"  ✓ Summary statistics: {stats_file}\")\n",
    "        print(f\"  ✓ Descriptive statistics: {desc_file}\")\n",
    "        \n",
    "        return str(stats_file)\n",
    "    \n",
    "    def generate_all_reports(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate all available reports based on loaded data\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of generated file paths\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"GENERATING ALL REPORTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        generated_files = {}\n",
    "        \n",
    "        # Validate data first\n",
    "        validation = self.validate_loaded_data()\n",
    "        \n",
    "        if not validation['ready_for_report']:\n",
    "            print(\"✗ Insufficient data loaded. Please load at least a metrics file.\")\n",
    "            return generated_files\n",
    "        \n",
    "        # Generate reports\n",
    "        report_file = self.generate_comprehensive_report()\n",
    "        if report_file:\n",
    "            generated_files['report'] = report_file\n",
    "        \n",
    "        matrices_dir = self.generate_confusion_matrices()\n",
    "        if matrices_dir:\n",
    "            generated_files['confusion_matrices'] = matrices_dir\n",
    "        \n",
    "        stats_file = self.generate_summary_statistics()\n",
    "        if stats_file:\n",
    "            generated_files['statistics'] = stats_file\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"REPORT GENERATION COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Output directory: {self.output_dir}\")\n",
    "        print(f\"Files generated: {len(generated_files)}\")\n",
    "        \n",
    "        return generated_files\n",
    "\n",
    "\n",
    "def create_flexible_evaluation_ui():\n",
    "    \"\"\"\n",
    "    Create an interactive UI with file upload capabilities for evaluation report generation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = FlexibleEvaluationReportGenerator()\n",
    "    \n",
    "    # Create UI components\n",
    "    display(HTML(\"\"\"\n",
    "    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "                 padding: 20px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n",
    "        <h2>📊 Flexible Evaluation Report Generator</h2>\n",
    "        <p>Upload your ML results files to generate comprehensive analysis reports</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    # File upload widgets\n",
    "    display(HTML(\"<h3>📁 Step 1: Upload Required Files</h3>\"))\n",
    "    display(HTML(\"<p>Upload files containing your ML results. Supported formats: JSON, Pickle (.pkl), CSV</p>\"))\n",
    "    \n",
    "    # Metrics file (required)\n",
    "    metrics_upload = widgets.FileUpload(\n",
    "        accept='.json,.pkl,.pickle,.csv',\n",
    "        multiple=False,\n",
    "        description='Metrics File (Required):',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    metrics_type = widgets.Dropdown(\n",
    "        options=['Auto-detect', 'JSON metrics', 'Pickle results', 'CSV metrics'],\n",
    "        value='Auto-detect',\n",
    "        description='File Type:',\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    # Features file (optional)\n",
    "    features_upload = widgets.FileUpload(\n",
    "        accept='.json,.pkl,.pickle,.csv,.txt',\n",
    "        multiple=False,\n",
    "        description='Features File (Optional):',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    features_type = widgets.Dropdown(\n",
    "        options=['Auto-detect', 'Pickle features', 'CSV features', 'Text list'],\n",
    "        value='Auto-detect',\n",
    "        description='File Type:',\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    # Confusion matrices file (optional)\n",
    "    confusion_upload = widgets.FileUpload(\n",
    "        accept='.json,.pkl,.pickle',\n",
    "        multiple=False,\n",
    "        description='Confusion Matrices (Optional):',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Output directory\n",
    "    display(HTML(\"<h3>📂 Step 2: Configure Output</h3>\"))\n",
    "    output_dir_input = widgets.Text(\n",
    "        value='./evaluation_reports',\n",
    "        description='Output Directory:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Report options\n",
    "    display(HTML(\"<h3>⚙️ Step 3: Select Report Types</h3>\"))\n",
    "    generate_text_report = widgets.Checkbox(value=True, description='Comprehensive Text Report')\n",
    "    generate_matrices = widgets.Checkbox(value=True, description='Confusion Matrix Visualizations')\n",
    "    generate_statistics = widgets.Checkbox(value=True, description='Summary Statistics CSV')\n",
    "    \n",
    "    # Status output\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Load files button\n",
    "    load_btn = widgets.Button(\n",
    "        description='Load Files',\n",
    "        button_style='warning',\n",
    "        icon='upload',\n",
    "        layout=widgets.Layout(width='150px', height='40px')\n",
    "    )\n",
    "    \n",
    "    # Generate button\n",
    "    generate_btn = widgets.Button(\n",
    "        description='Generate Reports',\n",
    "        button_style='success',\n",
    "        icon='check',\n",
    "        layout=widgets.Layout(width='150px', height='40px'),\n",
    "        disabled=True  # Initially disabled until files are loaded\n",
    "    )\n",
    "    \n",
    "    # Status label\n",
    "    status_label = widgets.HTML(value=\"<p style='color: orange;'>⚠️ Please upload at least a metrics file</p>\")\n",
    "    \n",
    "    def load_files(b):\n",
    "        \"\"\"Handle file loading\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"=\" * 50)\n",
    "            print(\"LOADING FILES\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Set output directory\n",
    "            generator.output_dir = Path(output_dir_input.value)\n",
    "            generator.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Load metrics file (required)\n",
    "            if metrics_upload.value:\n",
    "                file_content = metrics_upload.value[0]\n",
    "                filename = file_content['name']\n",
    "                content = file_content['content']\n",
    "                \n",
    "                # Save temporary file\n",
    "                temp_file = Path(f'/tmp/{filename}')\n",
    "                with open(temp_file, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                # Determine file type and load\n",
    "                if metrics_type.value == 'Auto-detect':\n",
    "                    if filename.endswith('.json'):\n",
    "                        generator.load_json_file(str(temp_file), 'metrics')\n",
    "                    elif filename.endswith(('.pkl', '.pickle')):\n",
    "                        generator.load_pickle_file(str(temp_file), 'results')\n",
    "                    elif filename.endswith('.csv'):\n",
    "                        generator.load_csv_file(str(temp_file), 'metrics')\n",
    "                elif 'JSON' in metrics_type.value:\n",
    "                    generator.load_json_file(str(temp_file), 'metrics')\n",
    "                elif 'Pickle' in metrics_type.value:\n",
    "                    generator.load_pickle_file(str(temp_file), 'results')\n",
    "                elif 'CSV' in metrics_type.value:\n",
    "                    generator.load_csv_file(str(temp_file), 'metrics')\n",
    "            \n",
    "            # Load features file (optional)\n",
    "            if features_upload.value:\n",
    "                file_content = features_upload.value[0]\n",
    "                filename = file_content['name']\n",
    "                content = file_content['content']\n",
    "                \n",
    "                # Save temporary file\n",
    "                temp_file = Path(f'/tmp/{filename}')\n",
    "                with open(temp_file, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                # Load based on type\n",
    "                if features_type.value == 'Auto-detect':\n",
    "                    if filename.endswith(('.pkl', '.pickle')):\n",
    "                        generator.load_pickle_file(str(temp_file), 'features')\n",
    "                    elif filename.endswith('.csv'):\n",
    "                        generator.load_csv_file(str(temp_file), 'features')\n",
    "                    elif filename.endswith('.txt'):\n",
    "                        with open(temp_file, 'r') as f:\n",
    "                            generator.selected_features = [line.strip() for line in f]\n",
    "                        print(f\"✓ Loaded {len(generator.selected_features)} features from text file\")\n",
    "                \n",
    "            # Load confusion matrices (optional)\n",
    "            if confusion_upload.value:\n",
    "                file_content = confusion_upload.value[0]\n",
    "                filename = file_content['name']\n",
    "                content = file_content['content']\n",
    "                \n",
    "                temp_file = Path(f'/tmp/{filename}')\n",
    "                with open(temp_file, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                if filename.endswith('.json'):\n",
    "                    with open(temp_file, 'r') as f:\n",
    "                        generator.confusion_matrices = json.load(f)\n",
    "                    print(f\"✓ Loaded confusion matrices from JSON\")\n",
    "                elif filename.endswith(('.pkl', '.pickle')):\n",
    "                    generator.load_pickle_file(str(temp_file), 'confusion_matrices')\n",
    "            \n",
    "            # Validate loaded data\n",
    "            validation = generator.validate_loaded_data()\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(\"VALIDATION RESULTS\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"✓ Has metrics: {validation['has_metrics']}\")\n",
    "            print(f\"✓ Has models: {validation['has_models']}\")\n",
    "            print(f\"✓ Has features: {validation['has_features']}\")\n",
    "            print(f\"✓ Ready for report: {validation['ready_for_report']}\")\n",
    "            \n",
    "            # Update status and enable generate button if ready\n",
    "            if validation['ready_for_report']:\n",
    "                status_label.value = \"<p style='color: green;'>✅ Files loaded successfully! Ready to generate reports.</p>\"\n",
    "                generate_btn.disabled = False\n",
    "            else:\n",
    "                status_label.value = \"<p style='color: red;'>❌ Insufficient data. Please ensure metrics file contains model results.</p>\"\n",
    "                generate_btn.disabled = True\n",
    "    \n",
    "    def generate_reports(b):\n",
    "        \"\"\"Handle report generation\"\"\"\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(\"=\" * 50)\n",
    "            print(\"GENERATING REPORTS\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            generated_files = {}\n",
    "            \n",
    "            try:\n",
    "                if generate_text_report.value:\n",
    "                    report_file = generator.generate_comprehensive_report()\n",
    "                    if report_file:\n",
    "                        generated_files['report'] = report_file\n",
    "                \n",
    "                if generate_matrices.value:\n",
    "                    matrices_dir = generator.generate_confusion_matrices()\n",
    "                    if matrices_dir:\n",
    "                        generated_files['confusion_matrices'] = matrices_dir\n",
    "                \n",
    "                if generate_statistics.value:\n",
    "                    stats_file = generator.generate_summary_statistics()\n",
    "                    if stats_file:\n",
    "                        generated_files['statistics'] = stats_file\n",
    "                \n",
    "                print(\"\\n\" + \"=\" * 50)\n",
    "                print(\"✅ REPORT GENERATION COMPLETE\")\n",
    "                print(\"=\" * 50)\n",
    "                print(f\"Output directory: {generator.output_dir}\")\n",
    "                print(f\"Files generated: {len(generated_files)}\")\n",
    "                \n",
    "                for file_type, filepath in generated_files.items():\n",
    "                    print(f\"  - {file_type}: {filepath}\")\n",
    "                \n",
    "                status_label.value = f\"<p style='color: green;'>✅ Successfully generated {len(generated_files)} report(s)</p>\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error during generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                status_label.value = \"<p style='color: red;'>❌ Error during report generation. Check output for details.</p>\"\n",
    "    \n",
    "    # Bind button callbacks\n",
    "    load_btn.on_click(load_files)\n",
    "    generate_btn.on_click(generate_reports)\n",
    "    \n",
    "    # Layout the UI\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([metrics_upload, metrics_type]),\n",
    "        widgets.HBox([features_upload, features_type]),\n",
    "        confusion_upload,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        output_dir_input,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        generate_text_report,\n",
    "        generate_matrices,\n",
    "        generate_statistics,\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        status_label,\n",
    "        widgets.HBox([load_btn, generate_btn]),\n",
    "        widgets.HTML(\"<hr>\"),\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "# Quick helper functions for programmatic use\n",
    "def generate_report_from_files(\n",
    "    metrics_file: str,\n",
    "    features_file: Optional[str] = None,\n",
    "    confusion_file: Optional[str] = None,\n",
    "    output_dir: str = './evaluation_reports'\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generate reports from specified files programmatically\n",
    "    \n",
    "    Args:\n",
    "        metrics_file: Path to metrics JSON/pickle/CSV file (required)\n",
    "        features_file: Path to features file (optional)\n",
    "        confusion_file: Path to confusion matrices file (optional)\n",
    "        output_dir: Directory for output files\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of generated file paths\n",
    "    \"\"\"\n",
    "    generator = FlexibleEvaluationReportGenerator(output_dir)\n",
    "    \n",
    "    # Load metrics (required)\n",
    "    if metrics_file.endswith('.json'):\n",
    "        generator.load_json_file(metrics_file, 'metrics')\n",
    "    elif metrics_file.endswith(('.pkl', '.pickle')):\n",
    "        generator.load_pickle_file(metrics_file, 'results')\n",
    "    elif metrics_file.endswith('.csv'):\n",
    "        generator.load_csv_file(metrics_file, 'metrics')\n",
    "    \n",
    "    # Load features (optional)\n",
    "    if features_file:\n",
    "        if features_file.endswith(('.pkl', '.pickle')):\n",
    "            generator.load_pickle_file(features_file, 'features')\n",
    "        elif features_file.endswith('.csv'):\n",
    "            generator.load_csv_file(features_file, 'features')\n",
    "    \n",
    "    # Load confusion matrices (optional)\n",
    "    if confusion_file:\n",
    "        if confusion_file.endswith('.json'):\n",
    "            with open(confusion_file, 'r') as f:\n",
    "                generator.confusion_matrices = json.load(f)\n",
    "        elif confusion_file.endswith(('.pkl', '.pickle')):\n",
    "            generator.load_pickle_file(confusion_file, 'confusion_matrices')\n",
    "    \n",
    "    # Generate all reports\n",
    "    return generator.generate_all_reports()\n",
    "\n",
    "# Display the UI when cell is run\n",
    "create_flexible_evaluation_ui()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FLEXIBLE EVALUATION REPORT GENERATOR READY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Use the interface above to upload files and generate reports\")\n",
    "print(\"\")\n",
    "print(\"For programmatic use:\")\n",
    "print(\"  generated = generate_report_from_files(\")\n",
    "print(\"      metrics_file='path/to/metrics.json',\")\n",
    "print(\"      features_file='path/to/features.pkl',  # optional\")\n",
    "print(\"      output_dir='./my_reports'\")\n",
    "print(\"  )\")\n",
    "print(\"\")\n",
    "print(\"Supported file formats:\")\n",
    "print(\"  ✓ Metrics: JSON, Pickle (.pkl), CSV\")\n",
    "print(\"  ✓ Features: Pickle (.pkl), CSV, TXT\")\n",
    "print(\"  ✓ Confusion Matrices: JSON, Pickle (.pkl)\")\n",
    "print(\"\")\n",
    "print(\"All outputs will be saved to your specified directory!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
