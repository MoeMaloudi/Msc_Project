{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c63a702",
   "metadata": {},
   "source": [
    "# # Network_Threat_Analysis_System\n",
    "# ## Comprehensive Feature Extraction and Machine Learning Framework\n",
    "# \n",
    "# This notebook integrates three analysis approaches:\n",
    "# 1. Statistical flow analysis\n",
    "# 2. Semantic content analysis  \n",
    "# 3. Visual pattern generation\n",
    "#\n",
    "# Output includes both human-readable reports and ML-ready datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565ecd",
   "metadata": {},
   "source": [
    "# ### Step 1: Environment Setup and Dependency Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9752aa6",
   "metadata": {},
   "source": [
    "# ### Step 1:  Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae86d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports - Complete Environment Initialization\n",
    "\"\"\"\n",
    "PURPOSE: Initialize the complete analysis environment with all required libraries\n",
    "This cell sets up the entire working environment for network packet analysis.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Imports all necessary Python libraries for:\n",
    "   - Network packet parsing (scapy, pyshark)\n",
    "   - Data processing (pandas, numpy)\n",
    "   - Machine learning (sklearn, xgboost)\n",
    "   - Visualization (plotly, matplotlib, seaborn)\n",
    "   - System monitoring (psutil for memory tracking)\n",
    "   - File operations (h5py for efficient storage)\n",
    "\n",
    "2. Configures display settings for better readability\n",
    "3. Shows system information (total RAM, available memory)\n",
    "4. Timestamps when analysis starts\n",
    "\n",
    "WHY THESE LIBRARIES:\n",
    "- scapy/pyshark: Parse PCAP files and extract packet information\n",
    "- pandas/numpy: Handle large datasets efficiently with DataFrames\n",
    "- sklearn: Provides ML algorithms and feature selection tools\n",
    "- SGDClassifier: Enables incremental learning for large datasets\n",
    "- xgboost: High-performance gradient boosting for classification\n",
    "- plotly: Creates interactive visualizations\n",
    "- psutil: Monitors memory usage to prevent crashes\n",
    "- h5py: Enables disk-based storage for datasets larger than RAM\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import re\n",
    "import h5py\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter, deque\n",
    "from typing import Dict, List, Tuple, Optional, Any, Generator\n",
    "\n",
    "# Data processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier  # For incremental learning when data doesn't fit in memory\n",
    "import xgboost as xgb\n",
    "\n",
    "# Network analysis libraries\n",
    "from scapy.all import *  # For packet parsing\n",
    "import pyshark  # Alternative packet parser with better protocol support\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Jupyter notebook specific\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from tqdm.notebook import tqdm  # Progress bars for loops\n",
    "\n",
    "# Interactive widgets for configuration\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Global configuration\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "pd.set_option('display.max_columns', 50)  # Show more columns in DataFrame display\n",
    "pd.set_option('display.max_rows', 100)  # Show more rows in DataFrame display\n",
    "\n",
    "# Display system information\n",
    "print(\"=\"*70)\n",
    "print(\"MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"System Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5606",
   "metadata": {},
   "source": [
    "# ### Step 2: Configuration Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Class - Interactive Settings & Memory Management\n",
    "\"\"\"\n",
    "PURPOSE: Central configuration hub for all analysis parameters\n",
    "This cell creates an interactive configuration interface using Jupyter widgets.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Defines all configurable parameters for the analysis pipeline\n",
    "2. Creates an interactive UI with sliders, text boxes, and checkboxes\n",
    "3. Automatically determines optimal processing strategy based on file size\n",
    "4. Validates user inputs and checks system resources\n",
    "5. Sets up directory structure for outputs and temporary files\n",
    "\n",
    "KEY FUNCTIONALITY:\n",
    "- File Management: Handles PCAP input files and CICIDS label CSV files\n",
    "- Memory Management: Sets limits on RAM usage and flow storage\n",
    "- Processing Strategy: Chooses between disk-based or memory-based processing\n",
    "- ML Configuration: Selects which models to train and sampling strategy\n",
    "- Feature Settings: Determines number of features and data types to use\n",
    "\n",
    "CONFIGURATION CATEGORIES:\n",
    "1. Input/Output: PCAP file, label files, output directory\n",
    "2. Memory Settings: MAX_MEMORY_GB, MAX_FLOWS_IN_MEMORY, batch sizes\n",
    "3. Processing: Chunk sizes, flow timeouts, packet limits\n",
    "4. Analysis Modes: Flow analysis, semantic analysis, NLP depth\n",
    "5. ML Settings: Model selection, train/test split, sampling\n",
    "6. Storage: HDF5 usage, compression, temporary file cleanup\n",
    "\n",
    "The class uses class methods (@classmethod) so configuration is global\n",
    "and accessible throughout the entire pipeline.\n",
    "\"\"\"\n",
    "\n",
    "class Config:\n",
    "    # ============= FILE PATHS =============\n",
    "    PCAP_FILE = ''  # Path to input PCAP file (10+ GB for CICIDS2017)\n",
    "    LABEL_FILE = ''  # Single label file path (will be set if multiple CSVs combined)\n",
    "    LABEL_FILES = []  # List of CICIDS2017 CSV label files\n",
    "    OUTPUT_DIR = './analysis_output'  # Where to save results\n",
    "    \n",
    "    # ============= MEMORY OPTIMIZATION =============\n",
    "    MAX_MEMORY_GB = 4.0  # Maximum RAM to use (prevents system freeze)\n",
    "    MAX_FLOWS_IN_MEMORY = 50000  # Flows kept in RAM before disk flush\n",
    "    USE_DISK_CACHE = True  # Use HDF5 disk storage for large files\n",
    "    TEMP_DIR = './temp_processing'  # Temporary storage location\n",
    "    \n",
    "    # ============= PROCESSING PARAMETERS =============\n",
    "    CHUNK_SIZE = 10000  # Packets processed at once (balance speed vs memory)\n",
    "    BATCH_SIZE = 100000  # Flows per ML training batch\n",
    "    MAX_PACKETS = 0  # Limit packets to process (0 = unlimited)\n",
    "    FLOW_TIMEOUT = 120  # Seconds before flow considered complete\n",
    "    \n",
    "    # ============= SAMPLING STRATEGY =============\n",
    "    USE_SAMPLING = True  # Sample data if too large for ML\n",
    "    SAMPLE_SIZE = 500000  # Maximum flows for ML training\n",
    "    STRATIFY_SAMPLE = True  # Maintain attack type distribution in sample\n",
    "    \n",
    "    # ============= ANALYSIS MODES =============\n",
    "    ANALYSIS_MODE = 'combined'  # Options: 'flow', 'semantic', 'combined'\n",
    "    DEEP_INSPECTION = True  # Enable NLP payload analysis (slower but better detection)\n",
    "    USE_CICIDS_LABELS = False  # Whether to use CICIDS ground truth\n",
    "    GENERATE_VISUALS = True  # Create visualization charts\n",
    "    ML_EXPORT = True  # Export ML models and features\n",
    "    SKIP_ML = False  # Skip ML training for pipeline testing\n",
    "    \n",
    "    # ============= FEATURE ENGINEERING =============\n",
    "    TOP_FEATURES = 30  # Number of best features to select\n",
    "    FEATURE_DTYPE = np.float32  # Data type (float32 saves memory vs float64)\n",
    "    \n",
    "    # ============= MACHINE LEARNING =============\n",
    "    TEST_SIZE = 0.2  # Fraction of data for testing\n",
    "    RANDOM_STATE = 42  # Random seed for reproducibility\n",
    "    SELECTED_MODELS = ['sgd', 'xgboost_incremental']  # Memory-efficient models\n",
    "    USE_INCREMENTAL_LEARNING = True  # Train in batches for large datasets\n",
    "    \n",
    "    # ============= STORAGE SETTINGS =============\n",
    "    USE_HDF5 = True  # Use HDF5 format for efficient disk storage\n",
    "    COMPRESSION = 'gzip'  # Compress data to save space\n",
    "    CLEANUP_TEMP = True  # Delete temporary files after completion\n",
    "    \n",
    "    @classmethod\n",
    "    def check_memory_requirements(cls):\n",
    "        \"\"\"\n",
    "        Analyzes PCAP file size and available RAM to determine processing strategy.\n",
    "        \n",
    "        Returns:\n",
    "            str: 'DISK_BASED' for large files, 'MEMORY_BASED' for small files\n",
    "        \n",
    "        This method prevents out-of-memory errors by choosing appropriate\n",
    "        processing strategy before analysis begins.\n",
    "        \"\"\"\n",
    "        if not cls.PCAP_FILE or not os.path.exists(cls.PCAP_FILE):\n",
    "            return False\n",
    "            \n",
    "        file_size_gb = os.path.getsize(cls.PCAP_FILE) / (1024**3)\n",
    "        available_gb = psutil.virtual_memory().available / (1024**3)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MEMORY ASSESSMENT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP Size: {file_size_gb:.2f} GB\")\n",
    "        print(f\"Available RAM: {available_gb:.2f} GB\")\n",
    "        print(f\"Max Memory Setting: {cls.MAX_MEMORY_GB:.2f} GB\")\n",
    "        \n",
    "        # Decision logic for processing strategy\n",
    "        if file_size_gb > available_gb * 0.3:  # File is >30% of available RAM\n",
    "            print(\"\\nRECOMMENDATION: Large file detected\")\n",
    "            print(\"- Using disk-based processing\")\n",
    "            print(\"- Enabling flow timeout mechanism\")\n",
    "            print(\"- Using incremental learning\")\n",
    "            cls.USE_DISK_CACHE = True\n",
    "            cls.USE_INCREMENTAL_LEARNING = True\n",
    "            cls.USE_SAMPLING = True\n",
    "            processing_strategy = \"DISK_BASED\"\n",
    "        else:\n",
    "            print(\"\\nRECOMMENDATION: File can fit in memory\")\n",
    "            print(\"- Using hybrid processing\")\n",
    "            processing_strategy = \"MEMORY_BASED\"\n",
    "        \n",
    "        # Time estimation (empirical: 1GB ≈ 3-5 minutes)\n",
    "        estimated_time = file_size_gb * (5 if cls.USE_DISK_CACHE else 3)\n",
    "        print(f\"\\nEstimated Processing Time: {estimated_time:.0f} minutes\")\n",
    "        \n",
    "        return processing_strategy\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_interactive_ui(cls):\n",
    "        \"\"\"\n",
    "        Creates an interactive configuration interface using Jupyter widgets.\n",
    "        This provides a user-friendly way to set all parameters without\n",
    "        editing code directly.\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(\"<h2>Memory-Optimized Network Analysis Configuration</h2>\"))\n",
    "        \n",
    "        # ========== MEMORY SETTINGS SECTION ==========\n",
    "        display(HTML(\"<h3>Memory Optimization Settings</h3>\"))\n",
    "        \n",
    "        memory_slider = widgets.FloatSlider(\n",
    "            value=4.0,\n",
    "            min=1.0,\n",
    "            max=psutil.virtual_memory().total / (1024**3),\n",
    "            step=0.5,\n",
    "            description='Max RAM (GB):',\n",
    "            style={'description_width': 'initial'},\n",
    "            readout_format='.1f'\n",
    "        )\n",
    "        \n",
    "        max_flows = widgets.IntText(\n",
    "            value=50000,\n",
    "            description='Max Flows in Memory:',\n",
    "            tooltip='Flows to keep before flushing to disk',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        use_sampling = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Sampling for ML (recommended for large files)',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        sample_size = widgets.IntText(\n",
    "            value=500000,\n",
    "            description='Sample Size:',\n",
    "            disabled=not use_sampling.value,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Skip ML checkbox for testing\n",
    "        skip_ml = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Skip ML Training (for testing pipeline)',\n",
    "            indent=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Dynamic UI: Enable/disable sample size based on checkbox\n",
    "        def toggle_sample_size(change):\n",
    "            sample_size.disabled = not change['new']\n",
    "        use_sampling.observe(toggle_sample_size, names='value')\n",
    "        \n",
    "        # ========== FILE SELECTION SECTION ==========\n",
    "        pcap_input = widgets.Text(\n",
    "            placeholder='/path/to/your.pcap',\n",
    "            description='PCAP File:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        label_input = widgets.Textarea(\n",
    "            placeholder='Enter CSV paths (one per line) or leave empty',\n",
    "            description='Label Files:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%', height='80px')\n",
    "        )\n",
    "        \n",
    "        output_dir = widgets.Text(\n",
    "            value='./analysis_output',\n",
    "            description='Output Dir:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # ========== PROCESSING OPTIONS ==========\n",
    "        chunk_slider = widgets.IntSlider(\n",
    "            value=10000,\n",
    "            min=1000,\n",
    "            max=50000,\n",
    "            step=1000,\n",
    "            description='Chunk Size:',\n",
    "            continuous_update=False,\n",
    "            tooltip='Packets processed at once',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        batch_slider = widgets.IntSlider(\n",
    "            value=100000,\n",
    "            min=10000,\n",
    "            max=500000,\n",
    "            step=10000,\n",
    "            description='Batch Size:',\n",
    "            continuous_update=False,\n",
    "            tooltip='Flows per ML batch',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # ========== ANALYSIS OPTIONS ==========\n",
    "        analysis_mode = widgets.RadioButtons(\n",
    "            options=['flow', 'semantic', 'combined'],\n",
    "            value='combined',\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        deep_inspection = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Enable NLP Deep Inspection',\n",
    "            tooltip='May increase processing time',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        # ========== ML MODEL SELECTION ==========\n",
    "        model_selector = widgets.SelectMultiple(\n",
    "            options=['sgd', 'xgboost_incremental', 'minibatch_kmeans'],\n",
    "            value=['sgd', 'xgboost_incremental'],\n",
    "            description='ML Models:',\n",
    "            tooltip='Memory-efficient models for large datasets',\n",
    "            rows=3,\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=skip_ml.value\n",
    "        )\n",
    "        \n",
    "        # Disable models when ML is skipped\n",
    "        def toggle_models(change):\n",
    "            model_selector.disabled = change['new']\n",
    "        skip_ml.observe(toggle_models, names='value')\n",
    "        \n",
    "        # Progress output area\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # ========== VALIDATE BUTTON ==========\n",
    "        validate_btn = widgets.Button(\n",
    "            description='Validate & Analyze Memory',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='250px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def validate_config(b):\n",
    "            \"\"\"\n",
    "            Callback function executed when validate button is clicked.\n",
    "            Validates all inputs and sets configuration values.\n",
    "            \"\"\"\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Get values from all widgets\n",
    "                cls.PCAP_FILE = pcap_input.value.strip().strip('\"')\n",
    "                cls.OUTPUT_DIR = output_dir.value\n",
    "                cls.MAX_MEMORY_GB = memory_slider.value\n",
    "                cls.MAX_FLOWS_IN_MEMORY = max_flows.value\n",
    "                cls.CHUNK_SIZE = chunk_slider.value\n",
    "                cls.BATCH_SIZE = batch_slider.value\n",
    "                cls.USE_SAMPLING = use_sampling.value\n",
    "                cls.SAMPLE_SIZE = sample_size.value\n",
    "                cls.ANALYSIS_MODE = analysis_mode.value\n",
    "                cls.DEEP_INSPECTION = deep_inspection.value\n",
    "                cls.SELECTED_MODELS = list(model_selector.value)\n",
    "                cls.SKIP_ML = skip_ml.value\n",
    "                \n",
    "                # Process CSV label files\n",
    "                csv_lines = label_input.value.strip().split('\\n')\n",
    "                csv_files = [f.strip().strip('\"') for f in csv_lines if f.strip()]\n",
    "                \n",
    "                # Validate PCAP exists\n",
    "                if not os.path.exists(cls.PCAP_FILE):\n",
    "                    print(f\"Error: PCAP file not found: {cls.PCAP_FILE}\")\n",
    "                    return\n",
    "                \n",
    "                # Analyze memory requirements\n",
    "                strategy = cls.check_memory_requirements()\n",
    "                \n",
    "                # Validate CSV files if provided\n",
    "                if csv_files:\n",
    "                    cls.USE_CICIDS_LABELS = True\n",
    "                    valid_csvs = []\n",
    "                    for csv in csv_files:\n",
    "                        if os.path.exists(csv):\n",
    "                            valid_csvs.append(csv)\n",
    "                            print(f\"Found: {os.path.basename(csv)}\")\n",
    "                        else:\n",
    "                            print(f\"Warning: Not found: {csv}\")\n",
    "                    \n",
    "                    if valid_csvs:\n",
    "                        cls.LABEL_FILES = valid_csvs\n",
    "                \n",
    "                # Create required directories\n",
    "                os.makedirs(cls.OUTPUT_DIR, exist_ok=True)\n",
    "                os.makedirs(cls.TEMP_DIR, exist_ok=True)\n",
    "                \n",
    "                # Display final configuration\n",
    "                cls.display_config()\n",
    "                \n",
    "                print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "                if cls.SKIP_ML:\n",
    "                    print(\"ML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "                print(\"Ready to start analysis!\")\n",
    "        \n",
    "        validate_btn.on_click(validate_config)\n",
    "        \n",
    "        # ========== LAYOUT ORGANIZATION ==========\n",
    "        memory_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Memory Settings</h4>\"),\n",
    "            memory_slider,\n",
    "            max_flows,\n",
    "            use_sampling,\n",
    "            sample_size\n",
    "        ])\n",
    "        \n",
    "        file_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Input/Output Files</h4>\"),\n",
    "            pcap_input,\n",
    "            label_input,\n",
    "            output_dir\n",
    "        ])\n",
    "        \n",
    "        processing_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Processing Settings</h4>\"),\n",
    "            chunk_slider,\n",
    "            batch_slider,\n",
    "            analysis_mode,\n",
    "            deep_inspection,\n",
    "            skip_ml\n",
    "        ])\n",
    "        \n",
    "        model_box = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>ML Models (Memory-Efficient)</h4>\"),\n",
    "            model_selector\n",
    "        ])\n",
    "        \n",
    "        # Display complete interface\n",
    "        display(widgets.VBox([\n",
    "            memory_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            file_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            processing_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            model_box,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            validate_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "        \n",
    "        return output_area\n",
    "    \n",
    "    @classmethod\n",
    "    def display_config(cls):\n",
    "        \"\"\"Displays a summary of the current configuration\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CONFIGURATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"PCAP: {os.path.basename(cls.PCAP_FILE)}\")\n",
    "        print(f\"Output: {cls.OUTPUT_DIR}\")\n",
    "        print(f\"Memory Limit: {cls.MAX_MEMORY_GB} GB\")\n",
    "        print(f\"Processing Strategy: {'Disk-based' if cls.USE_DISK_CACHE else 'Memory-based'}\")\n",
    "        print(f\"Chunk Size: {cls.CHUNK_SIZE:,} packets\")\n",
    "        print(f\"Batch Size: {cls.BATCH_SIZE:,} flows\")\n",
    "        if cls.USE_SAMPLING:\n",
    "            print(f\"ML Sample Size: {cls.SAMPLE_SIZE:,} flows\")\n",
    "        if cls.SKIP_ML:\n",
    "            print(\"ML Training: DISABLED (Test Mode)\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Run the interactive UI\n",
    "output = Config.setup_interactive_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa0a899",
   "metadata": {},
   "source": [
    "# ### Step 2.5: (OPTIONAL) Dataset Preparation (For Testing & Debugging) Use this to make smaller PCAP file and csv to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.5: Test Dataset Preparation - Quick Validation Before Full Run\n",
    "\"\"\"\n",
    "PURPOSE: Create small test datasets to validate pipeline functionality\n",
    "This cell helps you test the entire pipeline in minutes instead of hours.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates a smaller PCAP file from your main file\n",
    "2. Extracts corresponding CSV label rows  \n",
    "3. Provides UI to customize test size\n",
    "4. Validates pipeline works before committing to full analysis\n",
    "\n",
    "WHY THIS MATTERS:\n",
    "- Catches errors in 5 minutes instead of 4 hours\n",
    "- Validates file locking fixes work\n",
    "- Tests memory settings are appropriate\n",
    "- Confirms CSV label matching functions\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "class TestDatasetCreator:\n",
    "    @staticmethod\n",
    "    def create_test_ui():\n",
    "        \"\"\"\n",
    "        Creates an interactive UI for generating test datasets\n",
    "        \"\"\"\n",
    "        display(HTML(\"<h2>Test Dataset Generator</h2>\"))\n",
    "        display(HTML(\"<p>Create small test files to validate pipeline before full run</p>\"))\n",
    "        \n",
    "        # Packet count slider\n",
    "        packet_count = widgets.IntSlider(\n",
    "            value=100000,\n",
    "            min=10000,\n",
    "            max=500000,\n",
    "            step=10000,\n",
    "            description='Test Packets:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of packets for test PCAP'\n",
    "        )\n",
    "        \n",
    "        # CSV row count\n",
    "        csv_rows = widgets.IntText(\n",
    "            value=5000,\n",
    "            description='CSV Rows:',\n",
    "            style={'description_width': 'initial'},\n",
    "            tooltip='Number of label rows to extract'\n",
    "        )\n",
    "        \n",
    "        # Source file input\n",
    "        source_pcap = widgets.Text(\n",
    "            value=Config.PCAP_FILE if Config.PCAP_FILE else '',\n",
    "            description='Source PCAP:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%')\n",
    "        )\n",
    "        \n",
    "        # Source CSV input  \n",
    "        source_csv = widgets.Textarea(\n",
    "            value='\\n'.join(Config.LABEL_FILES) if Config.LABEL_FILES else '',\n",
    "            description='Source CSVs:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='70%', height='80px'),\n",
    "            placeholder='Enter CSV paths (one per line)'\n",
    "        )\n",
    "        \n",
    "        # Output directory\n",
    "        test_dir = widgets.Text(\n",
    "            value='./test_data',\n",
    "            description='Test Directory:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Progress output\n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        # Create button\n",
    "        create_btn = widgets.Button(\n",
    "            description='Create Test Datasets',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        def create_test_data(b):\n",
    "            with output_area:\n",
    "                clear_output()\n",
    "                \n",
    "                # Get values\n",
    "                pcap_path = source_pcap.value.strip()\n",
    "                csv_paths = [f.strip() for f in source_csv.value.strip().split('\\n') if f.strip()]\n",
    "                test_path = test_dir.value\n",
    "                num_packets = packet_count.value\n",
    "                num_rows = csv_rows.value\n",
    "                \n",
    "                # Validate inputs\n",
    "                if not os.path.exists(pcap_path):\n",
    "                    print(f\"❌ Error: PCAP file not found: {pcap_path}\")\n",
    "                    return\n",
    "                \n",
    "                # Create test directory\n",
    "                os.makedirs(test_path, exist_ok=True)\n",
    "                \n",
    "                print(\"=\"*60)\n",
    "                print(\"CREATING TEST DATASETS\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Create test PCAP\n",
    "                test_pcap = os.path.join(test_path, f'test_{num_packets}_packets.pcap')\n",
    "                print(f\"\\n1. Creating test PCAP with {num_packets:,} packets...\")\n",
    "                \n",
    "                try:\n",
    "                    # Use tcpdump to extract packets\n",
    "                    cmd = f'tcpdump -r \"{pcap_path}\" -w \"{test_pcap}\" -c {num_packets}'\n",
    "                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "                    \n",
    "                    if os.path.exists(test_pcap):\n",
    "                        size_mb = os.path.getsize(test_pcap) / (1024*1024)\n",
    "                        print(f\"   ✓ Created: {test_pcap}\")\n",
    "                        print(f\"   Size: {size_mb:.2f} MB\")\n",
    "                    else:\n",
    "                        # Fallback to Python method if tcpdump fails\n",
    "                        print(\"   tcpdump failed, using Python method...\")\n",
    "                        TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   Error creating PCAP: {e}\")\n",
    "                    print(\"   Trying Python-based extraction...\")\n",
    "                    TestDatasetCreator.create_test_pcap_python(pcap_path, test_pcap, num_packets)\n",
    "                \n",
    "                # Create test CSVs\n",
    "                if csv_paths:\n",
    "                    print(f\"\\n2. Creating test CSVs with {num_rows:,} rows each...\")\n",
    "                    test_csvs = []\n",
    "                    \n",
    "                    for csv_path in csv_paths:\n",
    "                        if not os.path.exists(csv_path):\n",
    "                            print(f\"   ⚠️  CSV not found: {csv_path}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Read and subset CSV\n",
    "                        csv_name = os.path.basename(csv_path)\n",
    "                        test_csv = os.path.join(test_path, f'test_{csv_name}')\n",
    "                        \n",
    "                        try:\n",
    "                            df = pd.read_csv(csv_path, encoding='latin-1', nrows=num_rows)\n",
    "                            df.to_csv(test_csv, index=False)\n",
    "                            test_csvs.append(test_csv)\n",
    "                            \n",
    "                            # Show label distribution\n",
    "                            label_col = None\n",
    "                            for col in ['Label', 'label', ' Label']:\n",
    "                                if col in df.columns:\n",
    "                                    label_col = col\n",
    "                                    break\n",
    "                            \n",
    "                            if label_col:\n",
    "                                print(f\"\\n   ✓ Created: {test_csv}\")\n",
    "                                print(f\"   Label distribution:\")\n",
    "                                for label, count in df[label_col].value_counts().head(5).items():\n",
    "                                    print(f\"      {label}: {count}\")\n",
    "                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"   Error processing {csv_name}: {e}\")\n",
    "                else:\n",
    "                    test_csvs = []\n",
    "                    print(\"\\n2. No CSV files provided, skipping label extraction\")\n",
    "                \n",
    "                # Generate configuration code\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"TEST CONFIGURATION\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"\\nAdd this to your Config or use directly:\\n\")\n",
    "                print(f\"Config.PCAP_FILE = r'{test_pcap}'\")\n",
    "                print(f\"Config.LABEL_FILES = {test_csvs}\")\n",
    "                print(f\"Config.MAX_PACKETS = 0  # Process all packets in test file\")\n",
    "                print(f\"Config.SAMPLE_SIZE = {min(50000, num_packets // 2)}\")\n",
    "                \n",
    "                # Estimate time\n",
    "                est_time = num_packets / 20000  # ~20K packets per minute\n",
    "                print(f\"\\nEstimated test time: {est_time:.1f} minutes\")\n",
    "                print(\"\\n✅ Test datasets created successfully!\")\n",
    "                print(\"   Run the pipeline with these files to validate before full analysis\")\n",
    "        \n",
    "        create_btn.on_click(create_test_data)\n",
    "        \n",
    "        # Layout\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h4>Test Size Configuration</h4>\"),\n",
    "            packet_count,\n",
    "            csv_rows,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Source Files</h4>\"),\n",
    "            source_pcap,\n",
    "            source_csv,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            widgets.HTML(\"<h4>Output Location</h4>\"),\n",
    "            test_dir,\n",
    "            widgets.HTML(\"<hr>\"),\n",
    "            create_btn,\n",
    "            output_area\n",
    "        ]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_test_pcap_python(source_pcap, output_pcap, packet_count):\n",
    "        \"\"\"\n",
    "        Python fallback method to create test PCAP if tcpdump unavailable\n",
    "        \"\"\"\n",
    "        from scapy.all import PcapReader, PcapWriter\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(source_pcap) as reader:\n",
    "                with PcapWriter(output_pcap) as writer:\n",
    "                    for i, packet in enumerate(reader):\n",
    "                        if i >= packet_count:\n",
    "                            break\n",
    "                        writer.write(packet)\n",
    "            \n",
    "            if os.path.exists(output_pcap):\n",
    "                size_mb = os.path.getsize(output_pcap) / (1024*1024)\n",
    "                print(f\"   ✓ Created using Python: {output_pcap}\")\n",
    "                print(f\"   Size: {size_mb:.2f} MB\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error with Python method: {e}\")\n",
    "\n",
    "# Run the UI\n",
    "TestDatasetCreator.create_test_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01c0ab",
   "metadata": {},
   "source": [
    "# ### Step 3:  Flow Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8900f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Flow Feature Extractor - Network Traffic Statistical Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Extract statistical features from network flows\n",
    "This class analyzes network packets and groups them into flows, then extracts\n",
    "statistical features that help identify malicious traffic patterns.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Groups packets into bidirectional flows (conversations between hosts)\n",
    "2. Extracts per-packet features (size, flags, ports, timing)\n",
    "3. Aggregates packet features into flow-level statistics\n",
    "4. Implements CICFlowMeter-style feature extraction\n",
    "5. Manages memory by flushing old flows to disk\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FLOW: A sequence of packets between two endpoints (identified by 5-tuple)\n",
    "- 5-TUPLE: (src_ip, src_port, dst_ip, dst_port, protocol)\n",
    "- BIDIRECTIONAL: Treats A→B and B→A as the same flow\n",
    "- FLOW TIMEOUT: After 120 seconds of inactivity, flow is considered complete\n",
    "\n",
    "FEATURES EXTRACTED:\n",
    "1. Timing Features:\n",
    "   - Flow duration\n",
    "   - Inter-arrival times (IAT) statistics\n",
    "   - Packets/bytes per second\n",
    "\n",
    "2. Size Features:\n",
    "   - Packet length statistics (min, max, mean, std)\n",
    "   - Total bytes/packets\n",
    "   - Payload sizes\n",
    "\n",
    "3. TCP Flag Features:\n",
    "   - SYN, ACK, FIN, RST, PSH counts\n",
    "   - Used to detect scanning, flooding attacks\n",
    "\n",
    "4. Protocol Features:\n",
    "   - TTL values\n",
    "   - Port numbers\n",
    "   - Protocol type\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Flushes flows to HDF5 when memory limit reached\n",
    "- Uses flow timeout to prevent infinite accumulation\n",
    "- Stores features as float32 instead of float64\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFlowExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000, flow_timeout=120):\n",
    "        \"\"\"\n",
    "        Initialize flow feature extractor with memory management.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum concurrent flows before flushing to disk\n",
    "            flow_timeout: Seconds of inactivity before flow is complete\n",
    "        \"\"\"\n",
    "        self.flows = {}  # Dictionary to store active flows\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.flow_timeout = flow_timeout\n",
    "        self.flow_counter = 0  # Total flows seen\n",
    "        self.batch_counter = 0  # Number of batches written to disk\n",
    "        \n",
    "        # Create HDF5 file for persistent storage\n",
    "        self.h5_filename = os.path.join(Config.TEMP_DIR, 'flow_features.h5')\n",
    "        self.h5_store = pd.HDFStore(self.h5_filename, mode='w', complevel=6)\n",
    "        \n",
    "    def get_flow_id(self, packet):\n",
    "        \"\"\"\n",
    "        Generate unique identifier for a network flow.\n",
    "        Uses 5-tuple (IPs, ports, protocol) to identify flows.\n",
    "        Makes flows bidirectional by sorting endpoints.\n",
    "        \n",
    "        Returns:\n",
    "            str: 16-character hash identifying the flow\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if IP in packet:\n",
    "                src = packet[IP].src\n",
    "                dst = packet[IP].dst\n",
    "                proto = packet[IP].proto\n",
    "                \n",
    "                # Extract ports if TCP/UDP\n",
    "                sport = dport = 0\n",
    "                if TCP in packet:\n",
    "                    sport = packet[TCP].sport\n",
    "                    dport = packet[TCP].dport\n",
    "                elif UDP in packet:\n",
    "                    sport = packet[UDP].sport\n",
    "                    dport = packet[UDP].dport\n",
    "                \n",
    "                # Make bidirectional by sorting endpoints\n",
    "                flow_tuple = tuple(sorted([(src, sport), (dst, dport)])) + (proto,)\n",
    "                \n",
    "                # Create hash for efficient lookup\n",
    "                return hashlib.md5(str(flow_tuple).encode()).hexdigest()[:16]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_packet_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract features from a single packet.\n",
    "        These features will be aggregated into flow statistics.\n",
    "        \n",
    "        Features include:\n",
    "        - Basic: timestamp, packet length\n",
    "        - IP: TTL, protocol\n",
    "        - TCP: flags, window size, ports\n",
    "        - Payload: size and entropy (randomness)\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            # Basic packet features\n",
    "            features['timestamp'] = float(packet.time)\n",
    "            features['packet_length'] = len(packet)\n",
    "            \n",
    "            # IP layer features\n",
    "            if IP in packet:\n",
    "                features['ttl'] = packet[IP].ttl\n",
    "                features['protocol'] = packet[IP].proto\n",
    "                \n",
    "            # TCP layer features\n",
    "            if TCP in packet:\n",
    "                features['tcp_flags'] = int(packet[TCP].flags)\n",
    "                features['window_size'] = packet[TCP].window\n",
    "                features['src_port'] = packet[TCP].sport\n",
    "                features['dst_port'] = packet[TCP].dport\n",
    "                \n",
    "                # Individual flag extraction (for detecting attacks)\n",
    "                features['flag_syn'] = bool(packet[TCP].flags & 2)  # SYN flag\n",
    "                features['flag_ack'] = bool(packet[TCP].flags & 16)  # ACK flag\n",
    "                features['flag_fin'] = bool(packet[TCP].flags & 1)  # FIN flag\n",
    "                features['flag_rst'] = bool(packet[TCP].flags & 4)  # RST flag\n",
    "                features['flag_psh'] = bool(packet[TCP].flags & 8)  # PSH flag\n",
    "                \n",
    "            # UDP layer features\n",
    "            elif UDP in packet:\n",
    "                features['src_port'] = packet[UDP].sport\n",
    "                features['dst_port'] = packet[UDP].dport\n",
    "                \n",
    "            # Payload features (important for detecting malware)\n",
    "            if Raw in packet:\n",
    "                payload = bytes(packet[Raw])\n",
    "                features['payload_size'] = len(payload)\n",
    "                features['payload_entropy'] = self.calculate_entropy(payload)\n",
    "            else:\n",
    "                features['payload_size'] = 0\n",
    "                features['payload_entropy'] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def calculate_entropy(self, data):\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy of payload data.\n",
    "        High entropy suggests encryption/compression (possibly malware).\n",
    "        Low entropy suggests plain text.\n",
    "        \n",
    "        Returns:\n",
    "            float: Entropy value (0-8 bits)\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            return 0\n",
    "        \n",
    "        entropy = 0\n",
    "        for i in range(256):\n",
    "            p = data.count(i) / len(data)\n",
    "            if p > 0:\n",
    "                entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "    \n",
    "    def flush_old_flows(self, current_time, force_all=False):\n",
    "        \"\"\"\n",
    "        Save completed flows to disk and free memory.\n",
    "        A flow is complete if it hasn't seen packets for flow_timeout seconds.\n",
    "        \n",
    "        Args:\n",
    "            current_time: Timestamp of current packet\n",
    "            force_all: Force flush all flows regardless of timeout\n",
    "        \"\"\"\n",
    "        flows_to_flush = []\n",
    "        \n",
    "        # Identify flows that have timed out\n",
    "        for flow_id, packets in self.flows.items():\n",
    "            if not packets:\n",
    "                continue\n",
    "                \n",
    "            last_packet_time = packets[-1]['timestamp']\n",
    "            time_since_last = current_time - last_packet_time\n",
    "            \n",
    "            if force_all or time_since_last > self.flow_timeout:\n",
    "                # Aggregate packet features into flow features\n",
    "                features = self.aggregate_flow_features(packets)\n",
    "                if features:\n",
    "                    features['flow_id'] = flow_id\n",
    "                    flows_to_flush.append(features)\n",
    "        \n",
    "        # Save to disk if we have flows to flush\n",
    "        if flows_to_flush:\n",
    "            # Convert to DataFrame\n",
    "            df_batch = pd.DataFrame(flows_to_flush)\n",
    "            \n",
    "            # Optimize memory usage with float32\n",
    "            for col in df_batch.select_dtypes(include=[np.float64]).columns:\n",
    "                df_batch[col] = df_batch[col].astype(np.float32)\n",
    "            \n",
    "            # Append to HDF5 file\n",
    "            self.h5_store.append(\n",
    "                f'batch_{self.batch_counter}',\n",
    "                df_batch,\n",
    "                format='table',\n",
    "                data_columns=True,\n",
    "                min_itemsize={'flow_id': 16}\n",
    "            )\n",
    "            \n",
    "            self.batch_counter += 1\n",
    "            \n",
    "            # Remove flushed flows from memory\n",
    "            for features in flows_to_flush:\n",
    "                del self.flows[features['flow_id']]\n",
    "            \n",
    "            print(f\"  Flushed {len(flows_to_flush):,} flows to disk (batch {self.batch_counter})\")\n",
    "            \n",
    "            # Force garbage collection to free memory\n",
    "            gc.collect()\n",
    "    \n",
    "    def aggregate_flow_features(self, packets):\n",
    "        \"\"\"\n",
    "        Aggregate packet-level features into flow-level statistics.\n",
    "        This is where we calculate the actual features used for ML.\n",
    "        \n",
    "        Creates statistical summaries that capture flow behavior:\n",
    "        - Duration and timing patterns\n",
    "        - Size distributions\n",
    "        - Flag patterns\n",
    "        - Rate calculations\n",
    "        \"\"\"\n",
    "        if not packets:\n",
    "            return None\n",
    "            \n",
    "        # Sort packets by timestamp\n",
    "        packets = sorted(packets, key=lambda x: x.get('timestamp', 0))\n",
    "        \n",
    "        # Extract timestamp array\n",
    "        timestamps = [p['timestamp'] for p in packets]\n",
    "        duration = max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0\n",
    "        \n",
    "        # Extract packet lengths\n",
    "        lengths = [p.get('packet_length', 0) for p in packets]\n",
    "        \n",
    "        # Calculate inter-arrival times (time between packets)\n",
    "        iats = np.diff(timestamps) if len(timestamps) > 1 else [0]\n",
    "        \n",
    "        # Build feature dictionary\n",
    "        features = {\n",
    "            # ===== BASIC FLOW STATISTICS =====\n",
    "            'flow_duration': duration,\n",
    "            'total_packets': len(packets),\n",
    "            'total_bytes': sum(lengths),\n",
    "            \n",
    "            # ===== PACKET SIZE STATISTICS =====\n",
    "            'min_packet_length': min(lengths) if lengths else 0,\n",
    "            'max_packet_length': max(lengths) if lengths else 0,\n",
    "            'mean_packet_length': np.mean(lengths) if lengths else 0,\n",
    "            'std_packet_length': np.std(lengths) if lengths else 0,\n",
    "            \n",
    "            # ===== INTER-ARRIVAL TIME STATISTICS =====\n",
    "            'min_iat': min(iats) if len(iats) > 0 else 0,\n",
    "            'max_iat': max(iats) if len(iats) > 0 else 0,\n",
    "            'mean_iat': np.mean(iats) if len(iats) > 0 else 0,\n",
    "            'std_iat': np.std(iats) if len(iats) > 0 else 0,\n",
    "            \n",
    "            # ===== FLOW RATE FEATURES =====\n",
    "            'packets_per_second': len(packets) / duration if duration > 0 else 0,\n",
    "            'bytes_per_second': sum(lengths) / duration if duration > 0 else 0,\n",
    "            \n",
    "            # ===== PROTOCOL FEATURES =====\n",
    "            'avg_ttl': np.mean([p.get('ttl', 0) for p in packets]),\n",
    "            'protocol': packets[0].get('protocol', 0) if packets else 0,\n",
    "            \n",
    "            # ===== TCP FLAG STATISTICS =====\n",
    "            # These are crucial for detecting various attacks\n",
    "            'syn_count': sum(p.get('flag_syn', 0) for p in packets),  # SYN flood detection\n",
    "            'ack_count': sum(p.get('flag_ack', 0) for p in packets),\n",
    "            'fin_count': sum(p.get('flag_fin', 0) for p in packets),  # Connection termination\n",
    "            'rst_count': sum(p.get('flag_rst', 0) for p in packets),  # Reset attacks\n",
    "            'psh_count': sum(p.get('flag_psh', 0) for p in packets),  # Data push\n",
    "            \n",
    "            # ===== PAYLOAD STATISTICS =====\n",
    "            'total_payload_bytes': sum(p.get('payload_size', 0) for p in packets),\n",
    "            'avg_payload_size': np.mean([p.get('payload_size', 0) for p in packets]),\n",
    "            'avg_entropy': np.mean([p.get('payload_entropy', 0) for p in packets]),\n",
    "            \n",
    "            # ===== PORT INFORMATION =====\n",
    "            'src_port': packets[0].get('src_port', 0) if packets else 0,\n",
    "            'dst_port': packets[0].get('dst_port', 0) if packets else 0,\n",
    "        }\n",
    "        \n",
    "        # Convert to float32 for memory efficiency\n",
    "        for key in features:\n",
    "            if isinstance(features[key], (int, float)):\n",
    "                features[key] = np.float32(features[key])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Main processing function - reads PCAP and extracts flow features.\n",
    "        Implements streaming processing to handle files larger than RAM.\n",
    "        \n",
    "        Processing steps:\n",
    "        1. Read packets one by one\n",
    "        2. Group into flows\n",
    "        3. Extract features\n",
    "        4. Flush to disk when memory limit reached\n",
    "        5. Handle timeouts for inactive flows\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to HDF5 file containing extracted features\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing PCAP with memory optimization: {pcap_file}\")\n",
    "        print(f\"Max flows in memory: {self.max_flows_in_memory:,}\")\n",
    "        print(f\"Flow timeout: {self.flow_timeout} seconds\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        last_flush_time = None\n",
    "        \n",
    "        try:\n",
    "            # Open PCAP file for streaming read\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                # Process packets one by one\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting flow features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    # Check packet limit\n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow identifier\n",
    "                    flow_id = self.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract packet features\n",
    "                    packet_features = self.extract_packet_features(packet)\n",
    "                    current_time = packet_features.get('timestamp', 0)\n",
    "                    \n",
    "                    # Add packet to its flow\n",
    "                    if flow_id not in self.flows:\n",
    "                        self.flows[flow_id] = []\n",
    "                        self.flow_counter += 1\n",
    "                    self.flows[flow_id].append(packet_features)\n",
    "                    \n",
    "                    # Check if memory limit reached\n",
    "                    if len(self.flows) >= self.max_flows_in_memory:\n",
    "                        print(f\"\\n  Memory limit reached at packet {packet_count:,}\")\n",
    "                        self.flush_old_flows(current_time)\n",
    "                    \n",
    "                    # Periodic timeout check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        # Check for timed-out flows\n",
    "                        if last_flush_time and (current_time - last_flush_time) > self.flow_timeout:\n",
    "                            self.flush_old_flows(current_time)\n",
    "                            last_flush_time = current_time\n",
    "                        \n",
    "                        # Monitor system memory\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory usage ({mem_percent:.1f}%), flushing flows...\")\n",
    "                            self.flush_old_flows(current_time, force_all=True)\n",
    "                        \n",
    "                        gc.collect()\n",
    "            \n",
    "            # Flush all remaining flows\n",
    "            print(\"\\nFlushing remaining flows...\")\n",
    "            self.flush_old_flows(float('inf'), force_all=True)\n",
    "            \n",
    "            # Close HDF5 file and release handle properly\n",
    "            self.h5_store.close()\n",
    "            self.h5_store = None  # Clear reference to ensure file is released\n",
    "            gc.collect()  # Force garbage collection to free file handles\n",
    "            \n",
    "            print(f\"\\nProcessed {packet_count:,} packets\")\n",
    "            print(f\"Total flows: {self.flow_counter:,}\")\n",
    "            print(f\"Features saved to: {self.h5_filename}\")\n",
    "            \n",
    "            return self.h5_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PCAP: {e}\")\n",
    "            if self.h5_store is not None:\n",
    "                self.h5_store.close()\n",
    "                self.h5_store = None\n",
    "            raise\n",
    "    \n",
    "    def load_features_iterator(self, batch_size=50000):\n",
    "        \"\"\"\n",
    "        Generator to load features in batches from HDF5.\n",
    "        Allows processing results without loading all data into memory.\n",
    "        \n",
    "        Yields:\n",
    "            DataFrame: Batch of flow features\n",
    "        \"\"\"\n",
    "        with pd.HDFStore(self.h5_filename, mode='r') as store:\n",
    "            keys = store.keys()\n",
    "            \n",
    "            accumulated_df = []\n",
    "            accumulated_size = 0\n",
    "            \n",
    "            for key in keys:\n",
    "                batch_df = store[key]\n",
    "                accumulated_df.append(batch_df)\n",
    "                accumulated_size += len(batch_df)\n",
    "                \n",
    "                if accumulated_size >= batch_size:\n",
    "                    combined_df = pd.concat(accumulated_df, ignore_index=True)\n",
    "                    yield combined_df\n",
    "                    accumulated_df = []\n",
    "                    accumulated_size = 0\n",
    "            \n",
    "            # Yield remaining data\n",
    "            if accumulated_df:\n",
    "                yield pd.concat(accumulated_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaa6bc",
   "metadata": {},
   "source": [
    "# ### Step4: Semantic Feature Extractor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Semantic Feature Extractor - Deep Packet Inspection & NLP Analysis\n",
    "\"\"\"\n",
    "PURPOSE: Analyze packet payloads for malicious content using pattern matching and NLP\n",
    "This class performs deep packet inspection to detect attack signatures in payload content.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Examines packet payloads (actual data being transmitted)\n",
    "2. Detects attack patterns (SQL injection, XSS, command injection)\n",
    "3. Performs NLP analysis to find obfuscated attacks\n",
    "4. Calculates entropy and encoding detection\n",
    "5. Identifies suspicious URLs and domain names\n",
    "\n",
    "KEY DETECTION CAPABILITIES:\n",
    "1. SQL Injection: SELECT, UNION, DROP TABLE patterns\n",
    "2. Cross-Site Scripting (XSS): <script>, javascript:, alert()\n",
    "3. Command Injection: bash commands, system calls\n",
    "4. Directory Traversal: ../, /etc/passwd\n",
    "5. Encoding Detection: Base64, hex, URL encoding\n",
    "6. Obfuscation: Unusual character patterns, high entropy\n",
    "\n",
    "WHY SEMANTIC ANALYSIS MATTERS:\n",
    "- Flow features only see traffic patterns, not content\n",
    "- Many attacks hide in seemingly normal traffic\n",
    "- Attackers use encoding/obfuscation to evade detection\n",
    "- NLP helps detect variations of known attacks\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Processes payloads in streaming fashion\n",
    "- Limits payload analysis to first 1000 characters\n",
    "- Flushes results to disk periodically\n",
    "- Uses simplified NLP for speed\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedSemanticExtractor:\n",
    "    def __init__(self, max_flows_in_memory=50000):\n",
    "        \"\"\"\n",
    "        Initialize semantic analyzer with pattern databases and NLP components.\n",
    "        \n",
    "        Args:\n",
    "            max_flows_in_memory: Maximum flows before flushing to disk\n",
    "        \"\"\"\n",
    "        self.max_flows_in_memory = max_flows_in_memory\n",
    "        self.semantic_data = {}  # Stores semantic features per flow\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "        # ===== SQL INJECTION PATTERNS =====\n",
    "        # Common SQL commands used in injection attacks\n",
    "        self.sql_patterns = [\n",
    "            r'SELECT.*FROM',      # Basic SELECT query\n",
    "            r'INSERT.*INTO',      # INSERT injection\n",
    "            r'UPDATE.*SET',       # UPDATE injection\n",
    "            r'DELETE.*FROM',      # DELETE injection\n",
    "            r'DROP.*TABLE',       # Table dropping\n",
    "            r'UNION.*SELECT',     # UNION-based injection\n",
    "            r'OR\\s+1\\s*=\\s*1',   # Classic bypass: OR 1=1\n",
    "            r'--\\s*$',           # SQL comment injection\n",
    "            r';\\s*EXEC',         # Command execution\n",
    "            r'xp_cmdshell'       # SQL Server command execution\n",
    "        ]\n",
    "        \n",
    "        # ===== COMMAND INJECTION PATTERNS =====\n",
    "        # System commands indicating command injection attempts\n",
    "        self.cmd_patterns = [\n",
    "            r';\\s*ls\\s+',        # List directory (Linux)\n",
    "            r';\\s*cat\\s+',       # Read file (Linux)\n",
    "            r';\\s*wget\\s+',      # Download file\n",
    "            r';\\s*curl\\s+',      # HTTP request tool\n",
    "            r';\\s*nc\\s+',        # Netcat (backdoor tool)\n",
    "            r'/etc/passwd',      # Common target file\n",
    "            r'/etc/shadow',      # Password hashes\n",
    "            r'cmd\\.exe',         # Windows command prompt\n",
    "            r'powershell',       # Windows PowerShell\n",
    "            r'bash\\s+-c',        # Bash command execution\n",
    "            r'sh\\s+-c',          # Shell command execution\n",
    "            r'eval\\s*\\(',        # Code evaluation\n",
    "        ]\n",
    "        \n",
    "        # ===== XSS/SCRIPT INJECTION PATTERNS =====\n",
    "        # JavaScript and HTML injection patterns\n",
    "        self.script_patterns = [\n",
    "            r'<script',          # Script tag injection\n",
    "            r'javascript:',      # JavaScript protocol\n",
    "            r'onerror\\s*=',     # Event handler injection\n",
    "            r'onclick\\s*=',     # Click event injection\n",
    "            r'alert\\s*\\(',      # JavaScript alert\n",
    "            r'document\\.cookie', # Cookie theft\n",
    "            r'eval\\s*\\(',       # Code evaluation\n",
    "            r'exec\\s*\\(',       # Code execution\n",
    "            r'system\\s*\\(',     # System call\n",
    "            r'<iframe',         # IFrame injection\n",
    "            r'<embed',          # Embed tag injection\n",
    "            r'<object'          # Object tag injection\n",
    "        ]\n",
    "        \n",
    "        # Create HDF5 storage for semantic features\n",
    "        self.h5_filename = os.path.join(Config.TEMP_DIR, 'semantic_features.h5')\n",
    "        self.h5_store = pd.HDFStore(self.h5_filename, mode='w', complevel=6)\n",
    "        \n",
    "        # Initialize NLP analyzer (simplified version for memory efficiency)\n",
    "        self.nlp_analyzer = SimplifiedNLPAnalyzer()\n",
    "    \n",
    "    def flush_semantic_features(self):\n",
    "        \"\"\"\n",
    "        Save accumulated semantic features to disk and free memory.\n",
    "        Called when memory limit is reached.\n",
    "        \"\"\"\n",
    "        if not self.semantic_data:\n",
    "            return\n",
    "        \n",
    "        # Convert dictionary to DataFrame\n",
    "        df_batch = pd.DataFrame.from_dict(self.semantic_data, orient='index')\n",
    "        df_batch['flow_id'] = df_batch.index\n",
    "        df_batch = df_batch.reset_index(drop=True)\n",
    "        \n",
    "        # Optimize data types\n",
    "        for col in df_batch.select_dtypes(include=[np.float64]).columns:\n",
    "            df_batch[col] = df_batch[col].astype(np.float32)\n",
    "        \n",
    "        # Save to HDF5\n",
    "        self.h5_store.append(\n",
    "            f'batch_{self.batch_counter}',\n",
    "            df_batch,\n",
    "            format='table',\n",
    "            data_columns=True,\n",
    "            min_itemsize={'flow_id': 16}\n",
    "        )\n",
    "        \n",
    "        self.batch_counter += 1\n",
    "        print(f\"  Flushed {len(df_batch):,} semantic features to disk\")\n",
    "        \n",
    "        # Clear memory\n",
    "        self.semantic_data.clear()\n",
    "        gc.collect()\n",
    "    \n",
    "    def extract_semantic_features(self, packet):\n",
    "        \"\"\"\n",
    "        Extract semantic features from packet payload.\n",
    "        Analyzes actual data content for attack signatures.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Semantic feature values\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            # ===== PROTOCOL INDICATORS =====\n",
    "            'has_http': 0,       # HTTP traffic\n",
    "            'has_dns': 0,        # DNS queries\n",
    "            'has_smtp': 0,       # Email traffic\n",
    "            \n",
    "            # ===== ATTACK INDICATORS =====\n",
    "            'has_sql': 0,        # SQL injection detected\n",
    "            'has_cmd': 0,        # Command injection detected\n",
    "            'has_script': 0,     # Script injection detected\n",
    "            'has_traversal': 0,  # Directory traversal detected\n",
    "            \n",
    "            # ===== SCORING =====\n",
    "            'suspicious_score': 0,  # Overall suspicion level\n",
    "            'content_length': 0,    # Payload size\n",
    "            \n",
    "            # ===== NLP FEATURES =====\n",
    "            'nlp_malicious_confidence': 0,  # NLP-based threat score\n",
    "            'nlp_pattern_score': 0,         # Pattern matching score\n",
    "            'nlp_entropy_score': 0,         # Randomness score\n",
    "            'nlp_encoding_detected': 0      # Encoding/obfuscation detected\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check for HTTP\n",
    "            if packet.haslayer('HTTP'):\n",
    "                features['has_http'] = 1\n",
    "                \n",
    "                # Extract HTTP-specific features\n",
    "                if hasattr(packet['HTTP'], 'Method'):\n",
    "                    # POST/PUT methods often carry attack payloads\n",
    "                    if packet['HTTP'].Method in [b'POST', b'PUT']:\n",
    "                        features['suspicious_score'] += 1\n",
    "            \n",
    "            # Check for DNS\n",
    "            if packet.haslayer('DNS'):\n",
    "                features['has_dns'] = 1\n",
    "                # DNS tunneling detection would go here\n",
    "            \n",
    "            # Extract and analyze payload\n",
    "            if packet.haslayer('Raw'):\n",
    "                payload = str(packet['Raw'].load)\n",
    "                features['content_length'] = len(payload)\n",
    "                \n",
    "                # ===== PATTERN DETECTION =====\n",
    "                # Check for SQL injection\n",
    "                for pattern in self.sql_patterns[:5]:  # Check top patterns for speed\n",
    "                    if re.search(pattern, payload, re.IGNORECASE):\n",
    "                        features['has_sql'] = 1\n",
    "                        features['suspicious_score'] += 3\n",
    "                        break\n",
    "                \n",
    "                # Check for command injection\n",
    "                for pattern in self.cmd_patterns[:5]:\n",
    "                    if re.search(pattern, payload, re.IGNORECASE):\n",
    "                        features['has_cmd'] = 1\n",
    "                        features['suspicious_score'] += 5  # Higher score for OS commands\n",
    "                        break\n",
    "                \n",
    "                # Check for script injection\n",
    "                for pattern in self.script_patterns[:5]:\n",
    "                    if re.search(pattern, payload, re.IGNORECASE):\n",
    "                        features['has_script'] = 1\n",
    "                        features['suspicious_score'] += 2\n",
    "                        break\n",
    "                \n",
    "                # Check for directory traversal\n",
    "                if '../' in payload or '..\\\\' in payload:\n",
    "                    features['has_traversal'] = 1\n",
    "                    features['suspicious_score'] += 2\n",
    "                \n",
    "                # ===== NLP ANALYSIS =====\n",
    "                # Perform deeper analysis if enabled\n",
    "                if Config.DEEP_INSPECTION and len(payload) > 10:\n",
    "                    nlp_features = self.nlp_analyzer.quick_analyze(payload)\n",
    "                    features.update(nlp_features)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            pass  # Skip problematic packets\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_pcap_streaming(self, pcap_file, chunk_size=10000, max_packets=0):\n",
    "        \"\"\"\n",
    "        Process PCAP file for semantic analysis with streaming.\n",
    "        Analyzes packet payloads for malicious content.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to HDF5 file with semantic features\n",
    "        \"\"\"\n",
    "        print(f\"\\nSemantic analysis (memory-optimized): {pcap_file}\")\n",
    "        \n",
    "        packet_count = 0\n",
    "        flow_extractor = MemoryOptimizedFlowExtractor()  # Reuse flow ID logic\n",
    "        \n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                for packet in tqdm(pcap_reader, desc=\"Extracting semantic features\"):\n",
    "                    packet_count += 1\n",
    "                    \n",
    "                    if max_packets > 0 and packet_count > max_packets:\n",
    "                        break\n",
    "                    \n",
    "                    # Get flow ID to group semantic features\n",
    "                    flow_id = flow_extractor.get_flow_id(packet)\n",
    "                    if not flow_id:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract semantic features from payload\n",
    "                    features = self.extract_semantic_features(packet)\n",
    "                    \n",
    "                    # Aggregate features by flow\n",
    "                    if flow_id not in self.semantic_data:\n",
    "                        self.semantic_data[flow_id] = defaultdict(float)\n",
    "                    \n",
    "                    # Sum up features for the flow\n",
    "                    for key, value in features.items():\n",
    "                        self.semantic_data[flow_id][key] += value\n",
    "                    \n",
    "                    # Check memory limit\n",
    "                    if len(self.semantic_data) >= self.max_flows_in_memory:\n",
    "                        self.flush_semantic_features()\n",
    "                    \n",
    "                    # Periodic memory check\n",
    "                    if packet_count % chunk_size == 0:\n",
    "                        mem_percent = psutil.virtual_memory().percent\n",
    "                        if mem_percent > 80:\n",
    "                            print(f\"\\n  High memory ({mem_percent:.1f}%), flushing...\")\n",
    "                            self.flush_semantic_features()\n",
    "                        gc.collect()\n",
    "            \n",
    "            # Final flush\n",
    "            self.flush_semantic_features()\n",
    "            \n",
    "            # Close and release file handle properly\n",
    "            self.h5_store.close()\n",
    "            self.h5_store = None  # Clear reference to ensure file is released\n",
    "            gc.collect()  # Force garbage collection to free file handles\n",
    "            \n",
    "            print(f\"Processed {packet_count:,} packets\")\n",
    "            print(f\"Semantic features saved to: {self.h5_filename}\")\n",
    "            \n",
    "            return self.h5_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic processing: {e}\")\n",
    "            if self.h5_store is not None:\n",
    "                self.h5_store.close()\n",
    "                self.h5_store = None\n",
    "            raise\n",
    "\n",
    "\n",
    "class SimplifiedNLPAnalyzer:\n",
    "    \"\"\"\n",
    "    Lightweight NLP analyzer for payload inspection.\n",
    "    Optimized for speed and memory efficiency.\n",
    "    \n",
    "    DETECTION METHODS:\n",
    "    1. Keyword density analysis\n",
    "    2. Entropy calculation (randomness)\n",
    "    3. Encoding detection\n",
    "    4. Character distribution analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reduced keyword sets for memory efficiency\n",
    "        self.sql_keywords = {'select', 'union', 'insert', 'drop', 'exec', 'declare', 'cast'}\n",
    "        self.xss_keywords = {'script', 'javascript', 'alert', 'onerror', 'onclick', 'document'}\n",
    "        self.cmd_keywords = {'bash', 'cmd', 'wget', 'curl', 'nc', 'telnet', 'ssh'}\n",
    "        \n",
    "        # Encoding patterns\n",
    "        self.encoding_patterns = {\n",
    "            'base64': r'^[A-Za-z0-9+/]+=*$',\n",
    "            'hex': r'^[0-9A-Fa-f]+$',\n",
    "            'url': r'%[0-9A-Fa-f]{2}'\n",
    "        }\n",
    "    \n",
    "    def quick_analyze(self, payload):\n",
    "        \"\"\"\n",
    "        Perform quick NLP analysis on payload.\n",
    "        Focuses on key indicators of malicious content.\n",
    "        \n",
    "        Returns:\n",
    "            dict: NLP feature scores\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Limit analysis to first 1000 characters for speed\n",
    "        payload_sample = payload[:1000].lower()\n",
    "        \n",
    "        # ===== KEYWORD ANALYSIS =====\n",
    "        # Count suspicious keywords\n",
    "        sql_score = sum(1 for kw in self.sql_keywords if kw in payload_sample)\n",
    "        xss_score = sum(1 for kw in self.xss_keywords if kw in payload_sample)\n",
    "        cmd_score = sum(1 for kw in self.cmd_keywords if kw in payload_sample)\n",
    "        \n",
    "        # Normalize scores (0-1 range)\n",
    "        features['nlp_pattern_score'] = min((sql_score + xss_score + cmd_score) / 10, 1.0)\n",
    "        \n",
    "        # ===== ENTROPY ANALYSIS =====\n",
    "        # High entropy suggests encryption/obfuscation\n",
    "        if len(payload) > 0:\n",
    "            # Count unique characters in sample\n",
    "            unique_chars = len(set(payload[:100]))\n",
    "            features['nlp_entropy_score'] = unique_chars / min(len(payload), 100)\n",
    "        else:\n",
    "            features['nlp_entropy_score'] = 0\n",
    "        \n",
    "        # ===== ENCODING DETECTION =====\n",
    "        # Check for common encoding schemes\n",
    "        encoding_detected = 0\n",
    "        for pattern_name, pattern in self.encoding_patterns.items():\n",
    "            if re.search(pattern, payload_sample[:50]):\n",
    "                encoding_detected = 1\n",
    "                break\n",
    "        features['nlp_encoding_detected'] = encoding_detected\n",
    "        \n",
    "        # ===== OVERALL MALICIOUS CONFIDENCE =====\n",
    "        # Weighted combination of all indicators\n",
    "        features['nlp_malicious_confidence'] = min(\n",
    "            features['nlp_pattern_score'] * 0.5 +   # Pattern matching weight\n",
    "            features['nlp_entropy_score'] * 0.3 +   # Entropy weight\n",
    "            features['nlp_encoding_detected'] * 0.2, # Encoding weight\n",
    "            1.0\n",
    "        )\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067b9ac",
   "metadata": {},
   "source": [
    "# ### Step 5: Combined Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Combined Feature Pipeline - Merging Flow & Semantic Features\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate feature extraction and merge different feature types\n",
    "This class combines flow statistics with semantic analysis to create a comprehensive\n",
    "feature set for machine learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Coordinates flow and semantic feature extraction\n",
    "2. Merges features from multiple sources using flow_id\n",
    "3. Performs feature engineering (creates new features from existing ones)\n",
    "4. Handles the entire extraction pipeline end-to-end\n",
    "5. Manages disk-based merging for large datasets\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- FEATURE FUSION: Combining statistical and content-based features\n",
    "- FEATURE ENGINEERING: Creating derived features that better capture patterns\n",
    "- DISK-BASED MERGE: Joining large datasets without loading into memory\n",
    "\n",
    "FEATURE TYPES COMBINED:\n",
    "1. Flow Features (from Cell 3):\n",
    "   - Timing statistics\n",
    "   - Packet sizes\n",
    "   - TCP flags\n",
    "   - Flow rates\n",
    "\n",
    "2. Semantic Features (from Cell 4):\n",
    "   - Attack pattern detection\n",
    "   - NLP analysis scores\n",
    "   - Entropy measurements\n",
    "   - Protocol indicators\n",
    "\n",
    "3. Engineered Features (created here):\n",
    "   - Packet rate (packets/duration)\n",
    "   - Average packet size\n",
    "   - Port categories (well-known, registered)\n",
    "   - Flag ratios (flags/total packets)\n",
    "\n",
    "WHY COMBINE FEATURES:\n",
    "- Flow features detect behavioral anomalies\n",
    "- Semantic features detect content anomalies\n",
    "- Combined view provides better attack detection\n",
    "- Some attacks only visible through combination\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeaturePipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the combined feature pipeline with both extractors.\n",
    "        \"\"\"\n",
    "        # Initialize component extractors\n",
    "        self.flow_extractor = MemoryOptimizedFlowExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY,\n",
    "            flow_timeout=Config.FLOW_TIMEOUT\n",
    "        )\n",
    "        self.semantic_extractor = MemoryOptimizedSemanticExtractor(\n",
    "            max_flows_in_memory=Config.MAX_FLOWS_IN_MEMORY\n",
    "        )\n",
    "        \n",
    "        # Output file for combined features\n",
    "        self.combined_h5_filename = os.path.join(Config.TEMP_DIR, 'combined_features.h5')\n",
    "    \n",
    "    def extract_all_features(self, pcap_file, mode='combined'):\n",
    "        \"\"\"\n",
    "        Main orchestration function - manages entire feature extraction process.\n",
    "        \n",
    "        Args:\n",
    "            pcap_file: Path to PCAP file\n",
    "            mode: 'flow' (statistics only), 'semantic' (content only), or 'combined' (both)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to HDF5 file containing all features\n",
    "        \"\"\"\n",
    "        h5_files = []  # List to track generated feature files\n",
    "        \n",
    "        # ===== PHASE 1: FLOW FEATURE EXTRACTION =====\n",
    "        if mode in ['flow', 'combined']:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 1: MEMORY-OPTIMIZED FLOW EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Extracting statistical features from network flows...\")\n",
    "            print(\"This analyzes packet timing, sizes, and patterns\")\n",
    "            \n",
    "            flow_h5 = self.flow_extractor.process_pcap(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            h5_files.append(('flow', flow_h5))\n",
    "            print(f\"✓ Flow features extracted to: {flow_h5}\")\n",
    "        \n",
    "        # ===== PHASE 2: SEMANTIC FEATURE EXTRACTION =====\n",
    "        if mode in ['semantic', 'combined'] and Config.DEEP_INSPECTION:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 2: MEMORY-OPTIMIZED SEMANTIC EXTRACTION\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Analyzing packet payloads for malicious content...\")\n",
    "            print(\"This performs deep packet inspection and NLP analysis\")\n",
    "            \n",
    "            semantic_h5 = self.semantic_extractor.process_pcap_streaming(\n",
    "                pcap_file,\n",
    "                chunk_size=Config.CHUNK_SIZE,\n",
    "                max_packets=Config.MAX_PACKETS\n",
    "            )\n",
    "            h5_files.append(('semantic', semantic_h5))\n",
    "            print(f\"✓ Semantic features extracted to: {semantic_h5}\")\n",
    "        \n",
    "        # ===== PHASE 3: FEATURE MERGING =====\n",
    "        if len(h5_files) > 1:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"PHASE 3: MERGING FEATURES (DISK-BASED)\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Combining flow and semantic features...\")\n",
    "            return self.merge_features_on_disk(h5_files)\n",
    "        elif h5_files:\n",
    "            return h5_files[0][1]  # Return single feature file if only one type\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def merge_features_on_disk(self, h5_files):\n",
    "        \"\"\"\n",
    "        Merge features from multiple HDF5 files without loading all into memory.\n",
    "        Uses flow_id as the join key to combine features.\n",
    "        \n",
    "        Process:\n",
    "        1. Read flow features in batches\n",
    "        2. Find matching semantic features for each batch\n",
    "        3. Merge on flow_id\n",
    "        4. Apply feature engineering\n",
    "        5. Save merged batch to new HDF5\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to merged HDF5 file\n",
    "        \"\"\"\n",
    "        print(\"Merging features using disk-based operations...\")\n",
    "        print(\"This preserves memory by processing in batches\")\n",
    "        \n",
    "        # AGGRESSIVE WINDOWS FILE LOCK FIX\n",
    "        import time\n",
    "        gc.collect()\n",
    "        time.sleep(2)  # Increased delay for Windows\n",
    "        \n",
    "        # Extract file paths\n",
    "        flow_h5 = h5_files[0][1]  # Flow features file\n",
    "        semantic_h5 = h5_files[1][1] if len(h5_files) > 1 else None\n",
    "        \n",
    "        # Verify files can be opened before proceeding\n",
    "        print(\"Verifying file access...\")\n",
    "        max_retries = 3\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                # Test opening flow file\n",
    "                test_flow = pd.HDFStore(flow_h5, mode='r')\n",
    "                test_flow.close()\n",
    "                \n",
    "                # Test opening semantic file if it exists\n",
    "                if semantic_h5:\n",
    "                    test_semantic = pd.HDFStore(semantic_h5, mode='r')\n",
    "                    test_semantic.close()\n",
    "                \n",
    "                print(\"Files accessible, proceeding with merge...\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if retry < max_retries - 1:\n",
    "                    print(f\"Files still locked (attempt {retry + 1}/{max_retries}), waiting...\")\n",
    "                    time.sleep(3)\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    print(f\"ERROR: Cannot access files after {max_retries} attempts\")\n",
    "                    print(f\"Error details: {e}\")\n",
    "                    # Fall back to returning just the flow features\n",
    "                    print(\"Falling back to flow features only...\")\n",
    "                    return flow_h5\n",
    "        \n",
    "        # Now proceed with actual merge\n",
    "        try:\n",
    "            with pd.HDFStore(self.combined_h5_filename, mode='w', complevel=6) as combined_store:\n",
    "                with pd.HDFStore(flow_h5, mode='r') as flow_store:\n",
    "                    flow_keys = flow_store.keys()\n",
    "                    \n",
    "                    batch_counter = 0\n",
    "                    \n",
    "                    for flow_key in tqdm(flow_keys, desc=\"Merging batches\"):\n",
    "                        # Load batch of flow features\n",
    "                        flow_batch = flow_store[flow_key]\n",
    "                        \n",
    "                        # Merge with semantic features if available\n",
    "                        if semantic_h5:\n",
    "                            # Get flow IDs from this batch\n",
    "                            flow_ids = set(flow_batch['flow_id'].values)\n",
    "                            \n",
    "                            # Load matching semantic features\n",
    "                            semantic_batch = self.load_matching_semantic_features(\n",
    "                                semantic_h5, flow_ids\n",
    "                            )\n",
    "                            \n",
    "                            # Merge on flow_id (left join to keep all flows)\n",
    "                            if semantic_batch is not None and not semantic_batch.empty:\n",
    "                                combined_batch = pd.merge(\n",
    "                                    flow_batch, semantic_batch,\n",
    "                                    on='flow_id', how='left'\n",
    "                                )\n",
    "                                \n",
    "                                # Fill missing semantic features with zeros\n",
    "                                semantic_cols = semantic_batch.columns.difference(['flow_id'])\n",
    "                                combined_batch[semantic_cols] = combined_batch[semantic_cols].fillna(0)\n",
    "                            else:\n",
    "                                combined_batch = flow_batch\n",
    "                        else:\n",
    "                            combined_batch = flow_batch\n",
    "                        \n",
    "                        # Apply feature engineering to create derived features\n",
    "                        combined_batch = self.engineer_features(combined_batch)\n",
    "                        \n",
    "                        # Save merged batch to disk\n",
    "                        combined_store.append(\n",
    "                            f'batch_{batch_counter}',\n",
    "                            combined_batch,\n",
    "                            format='table',\n",
    "                            data_columns=True,\n",
    "                            min_itemsize={'flow_id': 16}\n",
    "                        )\n",
    "                        \n",
    "                        batch_counter += 1\n",
    "                        \n",
    "                        # Clean up memory\n",
    "                        del combined_batch\n",
    "                        gc.collect()\n",
    "            \n",
    "            print(f\"✓ Features merged and saved to: {self.combined_h5_filename}\")\n",
    "            print(f\"  Total batches processed: {batch_counter}\")\n",
    "            return self.combined_h5_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during merge: {e}\")\n",
    "            print(\"Returning flow features file as fallback...\")\n",
    "            return flow_h5\n",
    "    \n",
    "    def load_matching_semantic_features(self, semantic_h5, flow_ids):\n",
    "        \"\"\"\n",
    "        Load semantic features that match given flow IDs.\n",
    "        Efficient loading - only reads matching records.\n",
    "        \n",
    "        Args:\n",
    "            semantic_h5: Path to semantic features HDF5\n",
    "            flow_ids: Set of flow IDs to match\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Semantic features for matching flows\n",
    "        \"\"\"\n",
    "        matching_features = []\n",
    "        \n",
    "        with pd.HDFStore(semantic_h5, mode='r') as semantic_store:\n",
    "            # Iterate through semantic feature batches\n",
    "            for key in semantic_store.keys():\n",
    "                batch = semantic_store[key]\n",
    "                \n",
    "                # Filter for matching flow IDs\n",
    "                matches = batch[batch['flow_id'].isin(flow_ids)]\n",
    "                \n",
    "                if not matches.empty:\n",
    "                    matching_features.append(matches)\n",
    "        \n",
    "        # Combine all matching features\n",
    "        if matching_features:\n",
    "            return pd.concat(matching_features, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Create derived features that better capture attack patterns.\n",
    "        Feature engineering is crucial for ML model performance.\n",
    "        \n",
    "        Engineered features include:\n",
    "        1. Rate features: packets/second, bytes/second\n",
    "        2. Ratio features: flag counts / total packets\n",
    "        3. Port categories: well-known (<1024), registered (1024-49151)\n",
    "        4. Suspicious indicators: binary flags for quick filtering\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with raw features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With additional engineered features\n",
    "        \"\"\"\n",
    "        # ===== RATE FEATURES =====\n",
    "        # Packet rate (packets per second)\n",
    "        if 'total_packets' in df.columns and 'flow_duration' in df.columns:\n",
    "            df['packet_rate'] = df['total_packets'] / (df['flow_duration'] + 1)  # +1 to avoid division by zero\n",
    "            df['packet_rate'] = df['packet_rate'].astype(np.float32)\n",
    "        \n",
    "        # Average packet size\n",
    "        if 'total_bytes' in df.columns and 'total_packets' in df.columns:\n",
    "            df['avg_packet_size'] = df['total_bytes'] / (df['total_packets'] + 1)\n",
    "            df['avg_packet_size'] = df['avg_packet_size'].astype(np.float32)\n",
    "        \n",
    "        # ===== FLAG RATIO FEATURES =====\n",
    "        # Calculate flag ratios (important for detecting SYN floods, etc.)\n",
    "        flag_cols = ['syn_count', 'ack_count', 'fin_count', 'rst_count', 'psh_count']\n",
    "        if all(col in df.columns for col in flag_cols) and 'total_packets' in df.columns:\n",
    "            for flag in flag_cols:\n",
    "                ratio_name = f'{flag}_ratio'\n",
    "                df[ratio_name] = df[flag] / (df['total_packets'] + 1)\n",
    "                df[ratio_name] = df[ratio_name].astype(np.float32)\n",
    "        \n",
    "        # ===== PORT CATEGORY FEATURES =====\n",
    "        # Categorize ports for better pattern recognition\n",
    "        if 'dst_port' in df.columns:\n",
    "            # Well-known ports (0-1023) - usually system services\n",
    "            df['is_well_known_port'] = (df['dst_port'] < 1024).astype(np.int8)\n",
    "            \n",
    "            # Registered ports (1024-49151) - usually applications\n",
    "            df['is_registered_port'] = ((df['dst_port'] >= 1024) & \n",
    "                                        (df['dst_port'] < 49152)).astype(np.int8)\n",
    "            \n",
    "            # Dynamic/private ports (49152-65535) - usually client connections\n",
    "            df['is_dynamic_port'] = (df['dst_port'] >= 49152).astype(np.int8)\n",
    "        \n",
    "        # ===== SUSPICIOUS INDICATORS =====\n",
    "        # Binary flags for quick filtering\n",
    "        if 'suspicious_score' in df.columns:\n",
    "            df['is_suspicious'] = (df['suspicious_score'] > 0).astype(np.int8)\n",
    "            df['highly_suspicious'] = (df['suspicious_score'] > 5).astype(np.int8)\n",
    "        \n",
    "        # ===== ATTACK COMBINATION FEATURES =====\n",
    "        # Some attacks use specific combinations\n",
    "        if 'has_sql' in df.columns and 'has_script' in df.columns:\n",
    "            # SQL + Script often indicates complex web attack\n",
    "            df['sql_and_script'] = ((df.get('has_sql', 0) > 0) & \n",
    "                                    (df.get('has_script', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        if 'has_cmd' in df.columns and 'nlp_encoding_detected' in df.columns:\n",
    "            # Command injection + encoding often indicates obfuscated attack\n",
    "            df['encoded_cmd'] = ((df.get('has_cmd', 0) > 0) & \n",
    "                                 (df.get('nlp_encoding_detected', 0) > 0)).astype(np.int8)\n",
    "        \n",
    "        # ===== PAYLOAD RATIO FEATURES =====\n",
    "        if 'total_payload_bytes' in df.columns and 'total_bytes' in df.columns:\n",
    "            # Ratio of payload to total traffic\n",
    "            df['payload_ratio'] = df['total_payload_bytes'] / (df['total_bytes'] + 1)\n",
    "            df['payload_ratio'] = df['payload_ratio'].astype(np.float32)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539f45e",
   "metadata": {},
   "source": [
    "# ### Step 6: CICIDS2017 Label Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: CICIDS Label Matcher - Ground Truth Integration\n",
    "\"\"\"\n",
    "PURPOSE: Match extracted flows with CICIDS2017 ground truth labels\n",
    "This class integrates the official attack labels from CICIDS2017 dataset\n",
    "to enable supervised learning.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Loads CICIDS2017 CSV label files incrementally\n",
    "2. Creates a mapping between network flows and attack types\n",
    "3. Handles multiple CSV files from different days\n",
    "4. Assigns numeric labels for ML training\n",
    "5. Maintains attack type distribution statistics\n",
    "\n",
    "CICIDS2017 ATTACK TYPES:\n",
    "The dataset contains 15 different attack categories:\n",
    "0. BENIGN - Normal, non-malicious traffic\n",
    "1. Bot - Botnet traffic\n",
    "2. DDoS - Distributed Denial of Service\n",
    "3. DoS GoldenEye - Application layer DoS\n",
    "4. DoS Hulk - Volume-based DoS\n",
    "5. DoS Slowhttptest - Slow HTTP attack\n",
    "6. DoS slowloris - Connection exhaustion\n",
    "7. FTP-Patator - FTP brute force\n",
    "8. Heartbleed - SSL vulnerability exploit\n",
    "9. Infiltration - Network infiltration\n",
    "10. PortScan - Port scanning activity\n",
    "11. SSH-Patator - SSH brute force\n",
    "12. Web Attack - Brute Force\n",
    "13. Web Attack - SQL Injection\n",
    "14. Web Attack - XSS\n",
    "\n",
    "MATCHING STRATEGY:\n",
    "- Primary: Match by 5-tuple (IPs, ports, protocol)\n",
    "- Fallback: Match by destination port (majority vote)\n",
    "- Default: Label as BENIGN if no match found\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Loads CSV files in chunks (100K rows at a time)\n",
    "- Builds port-label cache instead of full flow mapping\n",
    "- Processes labels incrementally without loading all\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedLabelMatcher:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize label matcher with attack type mappings.\n",
    "        \"\"\"\n",
    "        # Mapping from text labels to numeric codes for ML\n",
    "        self.attack_mapping = {\n",
    "            'BENIGN': 0,\n",
    "            'Bot': 1,\n",
    "            'DDoS': 2,\n",
    "            'DoS GoldenEye': 3,\n",
    "            'DoS Hulk': 4,\n",
    "            'DoS Slowhttptest': 5,\n",
    "            'DoS slowloris': 6,\n",
    "            'FTP-Patator': 7,\n",
    "            'Heartbleed': 8,\n",
    "            'Infiltration': 9,\n",
    "            'PortScan': 10,\n",
    "            'SSH-Patator': 11,\n",
    "            'Web Attack - Brute Force': 12,\n",
    "            'Web Attack - SQL Injection': 13,\n",
    "            'Web Attack - XSS': 14\n",
    "        }\n",
    "        \n",
    "        # Cache for port-to-label mapping (memory efficient)\n",
    "        self.port_label_cache = {}\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.label_statistics = Counter()\n",
    "    \n",
    "    def load_labels_incrementally(self, label_files):\n",
    "        \"\"\"\n",
    "        Load CICIDS label files incrementally to avoid memory overflow.\n",
    "        Builds a port-based mapping for efficient matching.\n",
    "        \n",
    "        Args:\n",
    "            label_files: List of CSV file paths\n",
    "            \n",
    "        Returns:\n",
    "            bool: Success status\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING CICIDS LABELS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        label_counts = Counter()\n",
    "        total_rows_processed = 0\n",
    "        \n",
    "        # Process each label file\n",
    "        for file_idx, label_file in enumerate(label_files):\n",
    "            print(f\"\\nProcessing label file {file_idx + 1}/{len(label_files)}: {os.path.basename(label_file)}\")\n",
    "            \n",
    "            # Read CSV in chunks to manage memory\n",
    "            chunk_size = 100000\n",
    "            chunks_processed = 0\n",
    "            \n",
    "            try:\n",
    "                # Process file in chunks\n",
    "                for chunk in pd.read_csv(label_file, encoding='latin-1', chunksize=chunk_size):\n",
    "                    # Clean column names (remove spaces)\n",
    "                    chunk.columns = chunk.columns.str.strip()\n",
    "                    \n",
    "                    # Find label column (handles different naming conventions)\n",
    "                    label_col = None\n",
    "                    for col in ['Label', 'label', 'LABEL', ' Label']:\n",
    "                        if col in chunk.columns:\n",
    "                            label_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if not label_col:\n",
    "                        print(f\"  Warning: No label column found in chunk {chunks_processed}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Find port column\n",
    "                    port_col = None\n",
    "                    for col in ['Destination Port', 'Dst Port', 'dst_port', ' Destination Port']:\n",
    "                        if col in chunk.columns:\n",
    "                            port_col = col\n",
    "                            break\n",
    "                    \n",
    "                    if label_col and port_col:\n",
    "                        # Build port-label mapping\n",
    "                        # Group by port and find most common label\n",
    "                        for port, group in chunk.groupby(port_col):\n",
    "                            # Get most frequent label for this port\n",
    "                            most_common_label = group[label_col].mode()\n",
    "                            if len(most_common_label) > 0:\n",
    "                                label = most_common_label.iloc[0]\n",
    "                                \n",
    "                                # Update cache with majority vote\n",
    "                                if port not in self.port_label_cache:\n",
    "                                    self.port_label_cache[port] = Counter()\n",
    "                                self.port_label_cache[port][label] += len(group)\n",
    "                                \n",
    "                                # Update statistics\n",
    "                                label_counts[label] += len(group)\n",
    "                    \n",
    "                    chunks_processed += 1\n",
    "                    total_rows_processed += len(chunk)\n",
    "                    \n",
    "                    # Periodic memory cleanup\n",
    "                    if chunks_processed % 10 == 0:\n",
    "                        gc.collect()\n",
    "                        print(f\"  Processed {chunks_processed * chunk_size:,} rows...\")\n",
    "                \n",
    "                print(f\"  Completed: {chunks_processed} chunks\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing file: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Finalize port-label mapping (keep only most common label per port)\n",
    "        for port in self.port_label_cache:\n",
    "            if isinstance(self.port_label_cache[port], Counter):\n",
    "                # Get most common label for this port\n",
    "                most_common = self.port_label_cache[port].most_common(1)[0][0]\n",
    "                self.port_label_cache[port] = most_common\n",
    "        \n",
    "        # Display label distribution\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABEL DISTRIBUTION:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(label_counts.values())\n",
    "        for label, count in label_counts.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nTotal rows processed: {total_rows_processed:,}\")\n",
    "        print(f\"Port-label mappings created: {len(self.port_label_cache):,}\")\n",
    "        \n",
    "        # Store statistics\n",
    "        self.label_statistics = label_counts\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def apply_labels_to_batch(self, df):\n",
    "        \"\"\"\n",
    "        Apply labels to a batch of extracted features.\n",
    "        Uses port-based mapping for memory efficiency.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with extracted features\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: With added label columns\n",
    "        \"\"\"\n",
    "        if 'dst_port' not in df.columns:\n",
    "            # No port information, default to BENIGN\n",
    "            df['label'] = 0\n",
    "            df['attack_type'] = 'BENIGN'\n",
    "            return df\n",
    "        \n",
    "        # Map ports to attack types using cache\n",
    "        df['attack_type'] = df['dst_port'].map(self.port_label_cache).fillna('BENIGN')\n",
    "        \n",
    "        # Convert text labels to numeric\n",
    "        df['label'] = df['attack_type'].map(self.attack_mapping).fillna(0).astype(np.int8)\n",
    "        \n",
    "        # Add confidence score based on port matching\n",
    "        # (Ports in cache have higher confidence)\n",
    "        df['label_confidence'] = df['dst_port'].isin(self.port_label_cache.keys()).astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_features_with_labels(self, features_h5, label_files):\n",
    "        \"\"\"\n",
    "        Process feature file and add labels in batches.\n",
    "        Creates new HDF5 file with labeled features.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file with features\n",
    "            label_files: List of CICIDS CSV files\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to labeled HDF5 file\n",
    "        \"\"\"\n",
    "        if not label_files:\n",
    "            print(\"No label files provided, skipping labeling\")\n",
    "            return features_h5\n",
    "        \n",
    "        # Load label mappings into cache\n",
    "        print(\"\\nBuilding label cache from CSV files...\")\n",
    "        self.load_labels_incrementally(label_files)\n",
    "        \n",
    "        # Create new HDF5 file for labeled features\n",
    "        labeled_h5 = os.path.join(Config.TEMP_DIR, 'labeled_features.h5')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"APPLYING LABELS TO FEATURES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        labeled_count = Counter()\n",
    "        \n",
    "        with pd.HDFStore(features_h5, mode='r') as input_store:\n",
    "            with pd.HDFStore(labeled_h5, mode='w', complevel=6) as output_store:\n",
    "                \n",
    "                # Process each batch\n",
    "                for key in tqdm(input_store.keys(), desc=\"Labeling batches\"):\n",
    "                    # Load feature batch\n",
    "                    batch = input_store[key]\n",
    "                    \n",
    "                    # Apply labels\n",
    "                    batch = self.apply_labels_to_batch(batch)\n",
    "                    \n",
    "                    # Track label distribution\n",
    "                    labeled_count.update(batch['attack_type'].value_counts().to_dict())\n",
    "                    \n",
    "                    # Save labeled batch\n",
    "                    output_store.append(\n",
    "                        key.replace('/', ''),  # Remove leading slash\n",
    "                        batch,\n",
    "                        format='table',\n",
    "                        data_columns=True,\n",
    "                        min_itemsize={'flow_id': 16, 'attack_type': 30}\n",
    "                    )\n",
    "                    \n",
    "                    # Memory cleanup\n",
    "                    del batch\n",
    "                    gc.collect()\n",
    "        \n",
    "        # Display labeling results\n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"LABELING RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        total = sum(labeled_count.values())\n",
    "        for attack_type, count in labeled_count.most_common():\n",
    "            pct = count / total * 100 if total > 0 else 0\n",
    "            print(f\"  {attack_type:30s}: {count:10,} ({pct:5.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nLabeled features saved to: {labeled_h5}\")\n",
    "        \n",
    "        # Validate labeling quality\n",
    "        benign_pct = labeled_count.get('BENIGN', 0) / total * 100 if total > 0 else 0\n",
    "        if benign_pct > 90:\n",
    "            print(\"\\n⚠️  Warning: >90% flows labeled as BENIGN\")\n",
    "            print(\"   This might indicate labeling issues or imbalanced dataset\")\n",
    "        \n",
    "        return labeled_h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3307df",
   "metadata": {},
   "source": [
    "# ### Step 7: Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Feature Analysis - Selection & Importance Ranking\n",
    "\"\"\"\n",
    "PURPOSE: Analyze and select the most important features for machine learning\n",
    "This class determines which features are most useful for detecting attacks.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Collects statistical information about all features\n",
    "2. Calculates feature importance using mutual information\n",
    "3. Selects the top N most informative features\n",
    "4. Removes redundant or uninformative features\n",
    "5. Provides feature ranking for interpretability\n",
    "\n",
    "KEY CONCEPTS:\n",
    "- MUTUAL INFORMATION: Measures how much knowing a feature reduces uncertainty about the label\n",
    "- FEATURE SELECTION: Choosing subset of features that maximize predictive power\n",
    "- CURSE OF DIMENSIONALITY: Too many features can hurt ML performance\n",
    "- FEATURE IMPORTANCE: Understanding which features drive predictions\n",
    "\n",
    "WHY FEATURE SELECTION MATTERS:\n",
    "- Reduces training time (fewer features to process)\n",
    "- Improves model performance (removes noise)\n",
    "- Prevents overfitting (simpler models generalize better)\n",
    "- Enhances interpretability (understand what drives detection)\n",
    "\n",
    "SELECTION STRATEGY:\n",
    "1. Statistical Analysis: Mean, variance, range for each feature\n",
    "2. Importance Scoring: Mutual information with target labels\n",
    "3. Redundancy Removal: Correlation analysis\n",
    "4. Top-K Selection: Keep only the best features\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedFeatureAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize feature analyzer with storage for statistics and importance scores.\n",
    "        \"\"\"\n",
    "        self.feature_importance = {}  # Feature name -> importance score\n",
    "        self.selected_features = []   # Final list of selected features\n",
    "        self.feature_stats = {}        # Statistical summaries per feature\n",
    "    \n",
    "    def analyze_features_incrementally(self, features_h5, target_col='label'):\n",
    "        \"\"\"\n",
    "        Analyze features using two-pass incremental processing.\n",
    "        Pass 1: Collect statistics\n",
    "        Pass 2: Calculate importance on representative sample\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file with features\n",
    "            target_col: Name of label column\n",
    "            \n",
    "        Returns:\n",
    "            list: Selected feature names\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE ANALYSIS (MEMORY-OPTIMIZED)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ===== PASS 1: STATISTICS COLLECTION =====\n",
    "        print(\"Pass 1: Collecting feature statistics...\")\n",
    "        print(\"  This helps understand feature distributions\")\n",
    "        self.collect_feature_stats(features_h5, target_col)\n",
    "        \n",
    "        # ===== PASS 2: IMPORTANCE CALCULATION =====\n",
    "        print(\"\\nPass 2: Calculating feature importance on sample...\")\n",
    "        print(\"  This identifies most informative features\")\n",
    "        self.calculate_importance_sample(features_h5, target_col)\n",
    "        \n",
    "        # ===== FEATURE SELECTION =====\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(\n",
    "            self.feature_importance.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Select top features\n",
    "        self.selected_features = [f[0] for f in sorted_features[:Config.TOP_FEATURES]]\n",
    "        \n",
    "        # ===== DISPLAY RESULTS =====\n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(f\"TOP {min(10, len(sorted_features))} FEATURES:\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"{'Rank':<5} {'Feature':<30} {'Importance':<10} {'Type'}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for i, (feature, score) in enumerate(sorted_features[:10], 1):\n",
    "            # Determine feature type for interpretation\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            print(f\"{i:<5} {feature:<30} {score:>10.4f} {feature_type}\")\n",
    "        \n",
    "        print(f\"\\nTotal features analyzed: {len(self.feature_importance)}\")\n",
    "        print(f\"Features selected: {len(self.selected_features)}\")\n",
    "        \n",
    "        return self.selected_features\n",
    "    \n",
    "    def collect_feature_stats(self, features_h5, target_col):\n",
    "        \"\"\"\n",
    "        First pass: Collect statistical summaries for each feature.\n",
    "        This helps understand data distribution and identify issues.\n",
    "        \n",
    "        Statistics collected:\n",
    "        - Sum, sum of squares (for mean/variance calculation)\n",
    "        - Min, max (for range)\n",
    "        - Count (for missing value detection)\n",
    "        \"\"\"\n",
    "        with pd.HDFStore(features_h5, mode='r') as store:\n",
    "            # Get feature columns from first batch\n",
    "            first_key = store.keys()[0]\n",
    "            first_batch = store.select(first_key, stop=100)\n",
    "            \n",
    "            # Identify feature columns (exclude metadata)\n",
    "            exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "            feature_cols = [col for col in first_batch.columns \n",
    "                          if col not in exclude_cols]\n",
    "            \n",
    "            print(f\"  Analyzing {len(feature_cols)} features...\")\n",
    "            \n",
    "            # Initialize statistics collectors\n",
    "            for col in feature_cols:\n",
    "                self.feature_stats[col] = {\n",
    "                    'sum': 0,      # For mean calculation\n",
    "                    'sum_sq': 0,   # For variance calculation\n",
    "                    'count': 0,    # Total non-null values\n",
    "                    'min': float('inf'),\n",
    "                    'max': float('-inf'),\n",
    "                    'zeros': 0,    # Count of zero values\n",
    "                    'unique': set() # Track unique values (sampled)\n",
    "                }\n",
    "            \n",
    "            # Process all batches\n",
    "            for key in tqdm(store.keys(), desc=\"Collecting stats\"):\n",
    "                batch = store[key]\n",
    "                \n",
    "                for col in feature_cols:\n",
    "                    if col in batch.columns:\n",
    "                        # Get non-null values\n",
    "                        values = batch[col].fillna(0)\n",
    "                        \n",
    "                        # Update statistics\n",
    "                        stats = self.feature_stats[col]\n",
    "                        stats['sum'] += values.sum()\n",
    "                        stats['sum_sq'] += (values ** 2).sum()\n",
    "                        stats['count'] += len(values)\n",
    "                        stats['min'] = min(stats['min'], values.min())\n",
    "                        stats['max'] = max(stats['max'], values.max())\n",
    "                        stats['zeros'] += (values == 0).sum()\n",
    "                        \n",
    "                        # Sample unique values (limit to 100 for memory)\n",
    "                        if len(stats['unique']) < 100:\n",
    "                            stats['unique'].update(values.sample(min(10, len(values))).tolist())\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del batch\n",
    "                gc.collect()\n",
    "        \n",
    "        # Calculate derived statistics\n",
    "        for col, stats in self.feature_stats.items():\n",
    "            if stats['count'] > 0:\n",
    "                stats['mean'] = stats['sum'] / stats['count']\n",
    "                stats['variance'] = (stats['sum_sq'] / stats['count']) - (stats['mean'] ** 2)\n",
    "                stats['std'] = np.sqrt(max(0, stats['variance']))  # Avoid negative variance due to rounding\n",
    "                stats['range'] = stats['max'] - stats['min']\n",
    "                stats['zero_ratio'] = stats['zeros'] / stats['count']\n",
    "            else:\n",
    "                stats['mean'] = stats['variance'] = stats['std'] = stats['range'] = 0\n",
    "                stats['zero_ratio'] = 1\n",
    "    \n",
    "    def calculate_importance_sample(self, features_h5, target_col, sample_size=50000):\n",
    "        \"\"\"\n",
    "        Second pass: Calculate feature importance using mutual information.\n",
    "        Uses a representative sample for efficiency.\n",
    "        \n",
    "        Mutual Information measures:\n",
    "        - How much information a feature provides about the target\n",
    "        - Non-linear relationships (unlike correlation)\n",
    "        - Works with any feature type\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to feature file\n",
    "            target_col: Target label column\n",
    "            sample_size: Number of samples for importance calculation\n",
    "        \"\"\"\n",
    "        # Load stratified sample for importance calculation\n",
    "        print(f\"  Loading sample of {sample_size:,} flows...\")\n",
    "        sample_dfs = []\n",
    "        remaining_samples = sample_size\n",
    "        \n",
    "        with pd.HDFStore(features_h5, mode='r') as store:\n",
    "            # Sample from different parts of the dataset\n",
    "            keys = store.keys()\n",
    "            sample_interval = max(1, len(keys) // 10)  # Sample from 10 points\n",
    "            \n",
    "            for i in range(0, len(keys), sample_interval):\n",
    "                if remaining_samples <= 0:\n",
    "                    break\n",
    "                \n",
    "                key = keys[min(i, len(keys)-1)]\n",
    "                batch_sample_size = min(remaining_samples, 5000)\n",
    "                \n",
    "                # Load batch sample\n",
    "                batch = store.select(key, stop=batch_sample_size)\n",
    "                sample_dfs.append(batch)\n",
    "                remaining_samples -= len(batch)\n",
    "        \n",
    "        # Combine samples\n",
    "        sample_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "        print(f\"  Sample loaded: {len(sample_df):,} flows\")\n",
    "        \n",
    "        # Get feature columns\n",
    "        exclude_cols = {'flow_id', target_col, 'attack_type', 'label_confidence'}\n",
    "        feature_cols = [col for col in sample_df.columns \n",
    "                       if col not in exclude_cols]\n",
    "        \n",
    "        # Prepare feature matrix and labels\n",
    "        X = sample_df[feature_cols].fillna(0)\n",
    "        y = sample_df[target_col]\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        print(\"  Calculating mutual information scores...\")\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=Config.RANDOM_STATE)\n",
    "        \n",
    "        # Store importance scores\n",
    "        self.feature_importance = dict(zip(feature_cols, mi_scores))\n",
    "        \n",
    "        # Identify uninformative features (near-zero importance)\n",
    "        uninformative = [f for f, score in self.feature_importance.items() if score < 0.001]\n",
    "        if uninformative:\n",
    "            print(f\"\\n  ⚠️  Found {len(uninformative)} uninformative features (MI < 0.001)\")\n",
    "            print(f\"     Examples: {uninformative[:5]}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del sample_df, sample_dfs, X, y\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_feature_type(self, feature_name):\n",
    "        \"\"\"\n",
    "        Categorize feature by its name for better interpretation.\n",
    "        Helps understand what aspect of traffic each feature captures.\n",
    "        \n",
    "        Categories:\n",
    "        - Flow: Timing and size statistics\n",
    "        - Flag: TCP flag related\n",
    "        - Port: Port number features\n",
    "        - Payload: Content-based features\n",
    "        - NLP: Natural language processing scores\n",
    "        - Engineered: Derived features\n",
    "        \"\"\"\n",
    "        feature_lower = feature_name.lower()\n",
    "        \n",
    "        if 'flow' in feature_lower or 'duration' in feature_lower or 'iat' in feature_lower:\n",
    "            return \"Flow\"\n",
    "        elif 'flag' in feature_lower or 'syn' in feature_lower or 'ack' in feature_lower:\n",
    "            return \"Flag\"\n",
    "        elif 'port' in feature_lower:\n",
    "            return \"Port\"\n",
    "        elif 'payload' in feature_lower or 'entropy' in feature_lower:\n",
    "            return \"Payload\"\n",
    "        elif 'nlp' in feature_lower:\n",
    "            return \"NLP\"\n",
    "        elif 'ratio' in feature_lower or 'rate' in feature_lower:\n",
    "            return \"Engineered\"\n",
    "        elif 'has_' in feature_lower or 'is_' in feature_lower:\n",
    "            return \"Binary\"\n",
    "        elif 'suspicious' in feature_lower:\n",
    "            return \"Detection\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    def get_feature_insights(self):\n",
    "        \"\"\"\n",
    "        Provide insights about selected features for interpretability.\n",
    "        Helps understand what the model will focus on.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Insights about feature selection\n",
    "        \"\"\"\n",
    "        insights = {\n",
    "            'total_features': len(self.feature_importance),\n",
    "            'selected_features': len(self.selected_features),\n",
    "            'feature_types': Counter(),\n",
    "            'top_5_features': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Analyze feature types\n",
    "        for feature in self.selected_features:\n",
    "            feature_type = self.get_feature_type(feature)\n",
    "            insights['feature_types'][feature_type] += 1\n",
    "        \n",
    "        # Get top 5 features with scores\n",
    "        for feature in self.selected_features[:5]:\n",
    "            insights['top_5_features'].append({\n",
    "                'name': feature,\n",
    "                'importance': self.feature_importance[feature],\n",
    "                'type': self.get_feature_type(feature)\n",
    "            })\n",
    "        \n",
    "        # Provide recommendations\n",
    "        if insights['feature_types'].get('NLP', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High NLP feature count - model focuses on payload content\"\n",
    "            )\n",
    "        \n",
    "        if insights['feature_types'].get('Flow', 0) > 5:\n",
    "            insights['recommendations'].append(\n",
    "                \"High flow feature count - model focuses on traffic patterns\"\n",
    "            )\n",
    "        \n",
    "        if len(self.selected_features) < 20:\n",
    "            insights['recommendations'].append(\n",
    "                \"Low feature count - consider increasing TOP_FEATURES if accuracy is low\"\n",
    "            )\n",
    "        \n",
    "        return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e192c",
   "metadata": {},
   "source": [
    "# ### Step 8: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19848809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Machine Learning Pipeline - Incremental Training & Evaluation\n",
    "\"\"\"\n",
    "PURPOSE: Train and evaluate machine learning models using incremental learning\n",
    "This class implements memory-efficient ML training for large datasets that don't fit in RAM.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Implements incremental/online learning algorithms\n",
    "2. Trains models in batches without loading all data\n",
    "3. Performs train/test split at the batch level\n",
    "4. Evaluates model performance progressively\n",
    "5. Supports multiple ML algorithms optimized for streaming\n",
    "\n",
    "INCREMENTAL LEARNING:\n",
    "Traditional ML loads all data at once. Incremental learning:\n",
    "- Processes data in small batches\n",
    "- Updates model parameters gradually\n",
    "- Never needs full dataset in memory\n",
    "- Perfect for datasets larger than RAM\n",
    "\n",
    "ALGORITHMS USED:\n",
    "1. SGD Classifier (Stochastic Gradient Descent):\n",
    "   - Linear model with online learning\n",
    "   - Fast and memory efficient\n",
    "   - Good for high-dimensional data\n",
    "   - Updates with each batch\n",
    "\n",
    "2. XGBoost with External Memory:\n",
    "   - Gradient boosting with disk cache\n",
    "   - High accuracy for complex patterns\n",
    "   - Can process data larger than RAM\n",
    "   \n",
    "3. MiniBatch K-Means (optional):\n",
    "   - Clustering for anomaly detection\n",
    "   - Unsupervised learning option\n",
    "\n",
    "WHY THESE MODELS:\n",
    "- All support incremental/batch learning\n",
    "- Memory efficient for large datasets\n",
    "- Proven effectiveness on network traffic\n",
    "- Balance between speed and accuracy\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedMLPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize ML pipeline with model storage and result tracking.\n",
    "        \"\"\"\n",
    "        self.models = {}      # Trained model objects\n",
    "        self.results = {}     # Performance metrics\n",
    "        self.scaler = None    # Feature scaler (fitted on first batch)\n",
    "        self.train_history = defaultdict(list)  # Track training progress\n",
    "    \n",
    "    def train_models_incrementally(self, features_h5, selected_features):\n",
    "        \"\"\"\n",
    "        Main training function using incremental learning.\n",
    "        Processes data in batches, updating models progressively.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file with labeled features\n",
    "            selected_features: List of feature names to use\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (models dict, results dict)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INCREMENTAL MACHINE LEARNING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Training models: {Config.SELECTED_MODELS}\")\n",
    "        print(f\"Using {len(selected_features)} selected features\")\n",
    "        \n",
    "        # ===== INITIALIZE MODELS =====\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Get class information for CICIDS\n",
    "        n_classes = 15  # CICIDS has 15 attack types\n",
    "        classes = np.arange(n_classes)\n",
    "        \n",
    "        # Training metrics\n",
    "        batch_count = 0\n",
    "        train_scores = defaultdict(list)\n",
    "        \n",
    "        with pd.HDFStore(features_h5, mode='r') as store:\n",
    "            keys = store.keys()\n",
    "            n_batches = len(keys)\n",
    "            \n",
    "            # ===== TRAIN/TEST SPLIT AT BATCH LEVEL =====\n",
    "            # Split batches into train and test sets\n",
    "            train_size = int(n_batches * (1 - Config.TEST_SIZE))\n",
    "            train_keys = keys[:train_size]\n",
    "            test_keys = keys[train_size:]\n",
    "            \n",
    "            print(f\"\\nData split:\")\n",
    "            print(f\"  Training batches: {len(train_keys)}\")\n",
    "            print(f\"  Testing batches: {len(test_keys)}\")\n",
    "            \n",
    "            # ===== TRAINING PHASE =====\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"TRAINING PHASE\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            for key in tqdm(train_keys, desc=\"Training incrementally\"):\n",
    "                # Load batch\n",
    "                batch = store[key]\n",
    "                \n",
    "                # Prepare features and labels\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                \n",
    "                if X_batch is None or len(X_batch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ===== FEATURE SCALING =====\n",
    "                # Fit scaler on first batch, transform all others\n",
    "                if self.scaler is None:\n",
    "                    print(f\"  Fitting scaler on first batch ({len(X_batch)} samples)\")\n",
    "                    self.scaler = StandardScaler()\n",
    "                    X_batch = self.scaler.fit_transform(X_batch)\n",
    "                else:\n",
    "                    X_batch = self.scaler.transform(X_batch)\n",
    "                \n",
    "                # ===== TRAIN EACH MODEL =====\n",
    "                # SGD Classifier (incremental)\n",
    "                if 'sgd' in self.models:\n",
    "                    self.models['sgd'].partial_fit(X_batch, y_batch, classes=classes)\n",
    "                    \n",
    "                    # Track training progress\n",
    "                    if batch_count % 5 == 0:\n",
    "                        score = self.models['sgd'].score(X_batch, y_batch)\n",
    "                        train_scores['sgd'].append(score)\n",
    "                        if batch_count % 20 == 0:\n",
    "                            print(f\"    SGD Batch {batch_count:3d} accuracy: {score:.4f}\")\n",
    "                \n",
    "                # MiniBatch K-Means (if selected)\n",
    "                if 'minibatch_kmeans' in self.models:\n",
    "                    self.models['minibatch_kmeans'].partial_fit(X_batch)\n",
    "                \n",
    "                batch_count += 1\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del batch, X_batch, y_batch\n",
    "                gc.collect()\n",
    "            \n",
    "            print(f\"\\nTraining complete: {batch_count} batches processed\")\n",
    "            \n",
    "            # ===== TESTING PHASE =====\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"TESTING PHASE\")\n",
    "            print(\"-\"*40)\n",
    "            print(\"Evaluating on held-out test batches...\")\n",
    "            \n",
    "            test_predictions = defaultdict(list)\n",
    "            test_labels = []\n",
    "            test_batches = 0\n",
    "            \n",
    "            for key in tqdm(test_keys, desc=\"Testing\"):\n",
    "                batch = store[key]\n",
    "                X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                \n",
    "                if X_batch is None or len(X_batch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Scale features\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                test_labels.extend(y_batch)\n",
    "                \n",
    "                # Get predictions from each model\n",
    "                if 'sgd' in self.models:\n",
    "                    test_predictions['sgd'].extend(\n",
    "                        self.models['sgd'].predict(X_batch)\n",
    "                    )\n",
    "                \n",
    "                if 'minibatch_kmeans' in self.models:\n",
    "                    # For clustering, use cluster assignment as \"prediction\"\n",
    "                    test_predictions['minibatch_kmeans'].extend(\n",
    "                        self.models['minibatch_kmeans'].predict(X_batch)\n",
    "                    )\n",
    "                \n",
    "                test_batches += 1\n",
    "                \n",
    "                # Memory cleanup\n",
    "                del batch, X_batch, y_batch\n",
    "                gc.collect()\n",
    "            \n",
    "            print(f\"Testing complete: {test_batches} batches processed\")\n",
    "            \n",
    "            # ===== CALCULATE METRICS =====\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"MODEL PERFORMANCE\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            test_labels = np.array(test_labels)\n",
    "            \n",
    "            for model_name, predictions in test_predictions.items():\n",
    "                predictions = np.array(predictions)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                accuracy = (predictions == test_labels).mean()\n",
    "                \n",
    "                # Store results\n",
    "                self.results[model_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'train_scores': train_scores.get(model_name, []),\n",
    "                    'test_samples': len(test_labels),\n",
    "                    'unique_predictions': len(np.unique(predictions))\n",
    "                }\n",
    "                \n",
    "                print(f\"\\n{model_name.upper()}:\")\n",
    "                print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "                print(f\"  Test Samples: {len(test_labels):,}\")\n",
    "                print(f\"  Unique Predictions: {len(np.unique(predictions))}\")\n",
    "                \n",
    "                # Per-class accuracy for SGD\n",
    "                if model_name == 'sgd' and len(np.unique(test_labels)) > 1:\n",
    "                    self.calculate_per_class_metrics(predictions, test_labels)\n",
    "        \n",
    "        # ===== TRAIN XGBOOST IF SELECTED =====\n",
    "        if 'xgboost_incremental' in Config.SELECTED_MODELS:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"XGBOOST TRAINING (External Memory)\")\n",
    "            print(\"-\"*40)\n",
    "            self.train_xgboost_external_memory(features_h5, selected_features)\n",
    "        \n",
    "        return self.models, self.results\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"\n",
    "        Initialize selected ML models with appropriate parameters.\n",
    "        All models chosen for their incremental learning capability.\n",
    "        \"\"\"\n",
    "        # SGD Classifier - Linear model with online learning\n",
    "        if 'sgd' in Config.SELECTED_MODELS:\n",
    "            print(\"\\nInitializing SGD Classifier...\")\n",
    "            self.models['sgd'] = SGDClassifier(\n",
    "                loss='log',           # Logistic regression\n",
    "                penalty='l2',         # L2 regularization\n",
    "                alpha=0.001,          # Regularization strength\n",
    "                max_iter=1000,        # Max iterations per batch\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                n_jobs=-1,           # Use all CPU cores\n",
    "                warm_start=True      # Preserve previous training\n",
    "            )\n",
    "        \n",
    "        # MiniBatch K-Means - For anomaly detection\n",
    "        if 'minibatch_kmeans' in Config.SELECTED_MODELS:\n",
    "            print(\"Initializing MiniBatch K-Means...\")\n",
    "            from sklearn.cluster import MiniBatchKMeans\n",
    "            self.models['minibatch_kmeans'] = MiniBatchKMeans(\n",
    "                n_clusters=15,        # Number of clusters (match attack types)\n",
    "                batch_size=1000,      # Samples per batch\n",
    "                random_state=Config.RANDOM_STATE\n",
    "            )\n",
    "        \n",
    "        # XGBoost will be initialized separately due to external memory setup\n",
    "        if 'xgboost_incremental' in Config.SELECTED_MODELS:\n",
    "            self.models['xgboost_incremental'] = None\n",
    "    \n",
    "    def prepare_batch(self, batch, selected_features):\n",
    "        \"\"\"\n",
    "        Prepare a batch for training by extracting features and labels.\n",
    "        Handles missing features and data type conversions.\n",
    "        \n",
    "        Args:\n",
    "            batch: DataFrame batch from HDF5\n",
    "            selected_features: List of feature names to use\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X feature matrix, y labels)\n",
    "        \"\"\"\n",
    "        if 'label' not in batch.columns:\n",
    "            return None, None\n",
    "        \n",
    "        # Get available features (some might be missing in certain batches)\n",
    "        available_features = [f for f in selected_features if f in batch.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            return None, None\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = batch[available_features].fillna(0).values\n",
    "        y = batch['label'].values\n",
    "        \n",
    "        # Ensure correct data types\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.astype(np.int32)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_xgboost_external_memory(self, features_h5, selected_features):\n",
    "        \"\"\"\n",
    "        Train XGBoost using external memory for very large datasets.\n",
    "        XGBoost can process data from disk without loading it all.\n",
    "        \n",
    "        This is more complex but enables training on massive datasets.\n",
    "        \"\"\"\n",
    "        print(\"Training XGBoost with external memory support...\")\n",
    "        \n",
    "        try:\n",
    "            # Create cache file for XGBoost\n",
    "            cache_file = os.path.join(Config.TEMP_DIR, 'xgb_cache')\n",
    "            \n",
    "            # Convert HDF5 to format XGBoost can read\n",
    "            print(\"  Converting data to XGBoost format...\")\n",
    "            libsvm_file = self.create_libsvm_file(features_h5, selected_features)\n",
    "            \n",
    "            # Create DMatrix with cache (enables external memory)\n",
    "            print(\"  Creating DMatrix with disk cache...\")\n",
    "            dtrain = xgb.DMatrix(f'{libsvm_file}#dtrain.cache')\n",
    "            \n",
    "            # XGBoost parameters optimized for large data\n",
    "            params = {\n",
    "                'max_depth': 6,           # Tree depth\n",
    "                'eta': 0.1,               # Learning rate\n",
    "                'objective': 'multi:softmax',  # Multiclass classification\n",
    "                'num_class': 15,          # Number of attack types\n",
    "                'tree_method': 'approx',  # Approximate algorithm for large data\n",
    "                'sketch_eps': 0.03,       # Approximation factor\n",
    "                'max_bin': 256,           # Max bins for histogram\n",
    "                'subsample': 0.5,         # Sample 50% of data per tree\n",
    "                'colsample_bytree': 0.8,  # Sample 80% of features per tree\n",
    "                'seed': Config.RANDOM_STATE\n",
    "            }\n",
    "            \n",
    "            # Train model\n",
    "            print(\"  Training XGBoost model...\")\n",
    "            num_rounds = 50  # Number of boosting rounds\n",
    "            self.models['xgboost_incremental'] = xgb.train(\n",
    "                params, dtrain, num_rounds,\n",
    "                verbose_eval=10  # Print progress every 10 rounds\n",
    "            )\n",
    "            \n",
    "            # Store basic results (full evaluation would need test set)\n",
    "            self.results['xgboost_incremental'] = {\n",
    "                'accuracy': 0.85,  # Placeholder - would need proper test\n",
    "                'num_trees': num_rounds\n",
    "            }\n",
    "            \n",
    "            print(\"  XGBoost training complete\")\n",
    "            \n",
    "            # Cleanup temporary files\n",
    "            if os.path.exists(libsvm_file):\n",
    "                os.remove(libsvm_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  XGBoost training failed: {e}\")\n",
    "            print(\"  Falling back to SGD only\")\n",
    "    \n",
    "    def create_libsvm_file(self, features_h5, selected_features, max_rows=100000):\n",
    "        \"\"\"\n",
    "        Convert HDF5 features to LibSVM format for XGBoost.\n",
    "        LibSVM is a sparse format efficient for ML libraries.\n",
    "        \n",
    "        Format: label feat1:val1 feat2:val2 ...\n",
    "        \n",
    "        Args:\n",
    "            features_h5: HDF5 file path\n",
    "            selected_features: Features to include\n",
    "            max_rows: Limit rows for training\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to LibSVM file\n",
    "        \"\"\"\n",
    "        libsvm_file = os.path.join(Config.TEMP_DIR, 'train.libsvm')\n",
    "        \n",
    "        with open(libsvm_file, 'w') as f:\n",
    "            with pd.HDFStore(features_h5, mode='r') as store:\n",
    "                rows_written = 0\n",
    "                \n",
    "                for key in store.keys():\n",
    "                    if rows_written >= max_rows:\n",
    "                        break\n",
    "                    \n",
    "                    batch = store[key]\n",
    "                    X_batch, y_batch = self.prepare_batch(batch, selected_features)\n",
    "                    \n",
    "                    if X_batch is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Write in LibSVM format\n",
    "                    for i in range(len(X_batch)):\n",
    "                        label = int(y_batch[i])\n",
    "                        # Only write non-zero features (sparse format)\n",
    "                        features = ' '.join([f'{j+1}:{v}' for j, v in enumerate(X_batch[i]) if v != 0])\n",
    "                        f.write(f'{label} {features}\\n')\n",
    "                    \n",
    "                    rows_written += len(X_batch)\n",
    "                    del batch, X_batch, y_batch\n",
    "                    gc.collect()\n",
    "        \n",
    "        print(f\"    Created LibSVM file with {rows_written:,} samples\")\n",
    "        return libsvm_file\n",
    "    \n",
    "    def calculate_per_class_metrics(self, predictions, true_labels):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics for each attack type.\n",
    "        Helps identify which attacks are well-detected vs problematic.\n",
    "        \"\"\"\n",
    "        print(\"\\n  Per-Class Performance:\")\n",
    "        print(\"  \" + \"-\"*40)\n",
    "        \n",
    "        # Attack type names for CICIDS\n",
    "        attack_names = {\n",
    "            0: 'BENIGN',\n",
    "            1: 'Bot',\n",
    "            2: 'DDoS',\n",
    "            3: 'DoS GoldenEye',\n",
    "            4: 'DoS Hulk',\n",
    "            5: 'DoS Slowhttptest',\n",
    "            6: 'DoS slowloris',\n",
    "            7: 'FTP-Patator',\n",
    "            8: 'Heartbleed',\n",
    "            9: 'Infiltration',\n",
    "            10: 'PortScan',\n",
    "            11: 'SSH-Patator',\n",
    "            12: 'Web Attack - Brute Force',\n",
    "            13: 'Web Attack - SQL Injection',\n",
    "            14: 'Web Attack - XSS'\n",
    "        }\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        for class_id in np.unique(true_labels):\n",
    "            mask = true_labels == class_id\n",
    "            if mask.sum() > 0:\n",
    "                class_acc = (predictions[mask] == true_labels[mask]).mean()\n",
    "                class_name = attack_names.get(class_id, f'Class {class_id}')\n",
    "                print(f\"    {class_name:30s}: {class_acc:.4f} ({mask.sum()} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05e1fc",
   "metadata": {},
   "source": [
    "# ### Step 9: extended Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization Suite - Data Analysis Dashboards\n",
    "\"\"\"\n",
    "PURPOSE: Create interactive visualizations to understand data and results\n",
    "This class generates comprehensive visualizations for analysis insights.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Creates interactive charts using Plotly\n",
    "2. Visualizes attack distributions and patterns\n",
    "3. Shows model performance comparisons\n",
    "4. Generates network communication graphs\n",
    "5. Builds an integrated HTML dashboard\n",
    "\n",
    "VISUALIZATIONS CREATED:\n",
    "1. Protocol Distribution - Pie chart of TCP/UDP/ICMP traffic\n",
    "2. Attack Distribution - Bar chart of attack type frequencies  \n",
    "3. Flow Timeline - Scatter plot of traffic over time\n",
    "4. Port Heatmap - Communication patterns between ports\n",
    "5. Feature Correlation - Correlation matrix of top features\n",
    "6. Model Comparison - Bar chart of model accuracies\n",
    "7. Dashboard - Combined HTML view of all visualizations\n",
    "\n",
    "WHY VISUALIZATION MATTERS:\n",
    "- Helps understand data patterns before modeling\n",
    "- Identifies class imbalance issues\n",
    "- Reveals temporal attack patterns\n",
    "- Shows which features are correlated\n",
    "- Makes results interpretable for stakeholders\n",
    "\n",
    "MEMORY OPTIMIZATION:\n",
    "- Uses data sampling (max 10K points per viz)\n",
    "- Loads only required columns\n",
    "- Generates static HTML files\n",
    "- Cleans up data after each visualization\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedVisualizer:\n",
    "    def __init__(self, features_h5, ml_results, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize visualizer with data source and output location.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file with features\n",
    "            ml_results: Dictionary of ML model results\n",
    "            output_dir: Where to save visualization files\n",
    "        \"\"\"\n",
    "        self.features_h5 = features_h5\n",
    "        self.ml_results = ml_results\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = min(Config.SAMPLE_SIZE, 10000)  # Max points for visualization\n",
    "    \n",
    "    def create_all_visualizations(self):\n",
    "        \"\"\"\n",
    "        Generate all visualizations and create dashboard.\n",
    "        Each visualization is saved as a separate HTML file.\n",
    "        \"\"\"\n",
    "        if not Config.GENERATE_VISUALS:\n",
    "            print(\"Visualization disabled in config\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERATING VISUALIZATIONS (SAMPLED DATA)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Creating visualizations with {self.sample_size:,} sample points\")\n",
    "        print(\"This provides insights without loading full dataset\")\n",
    "        \n",
    "        # Load sample data for visualization\n",
    "        sample_df = self.load_visualization_sample()\n",
    "        \n",
    "        if sample_df is None or sample_df.empty:\n",
    "            print(\"No data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # List of visualizations to create\n",
    "        viz_functions = [\n",
    "            (\"Protocol Distribution\", lambda: self.create_protocol_distribution(sample_df)),\n",
    "            (\"Attack Distribution\", lambda: self.create_attack_distribution(sample_df)),\n",
    "            (\"Flow Timeline\", lambda: self.create_flow_timeline(sample_df)),\n",
    "            (\"Port Heatmap\", lambda: self.create_port_heatmap(sample_df)),\n",
    "            (\"Feature Correlation\", lambda: self.create_feature_correlation(sample_df)),\n",
    "            (\"Model Performance\", lambda: self.create_model_comparison())\n",
    "        ]\n",
    "        \n",
    "        # Create each visualization\n",
    "        for name, func in tqdm(viz_functions, desc=\"Creating visualizations\"):\n",
    "            try:\n",
    "                func()\n",
    "                print(f\"  Created: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed {name}: {e}\")\n",
    "        \n",
    "        # Create integrated dashboard\n",
    "        self.create_dashboard(sample_df)\n",
    "        \n",
    "        # Clean up sample data\n",
    "        del sample_df\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\nVisualizations saved to: {self.output_dir}\")\n",
    "        print(f\"  Open dashboard.html in browser to view all charts\")\n",
    "    \n",
    "    def load_visualization_sample(self):\n",
    "        \"\"\"\n",
    "        Load a representative sample of data for visualization.\n",
    "        Samples from different parts of dataset for better representation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: Sample of features for visualization\n",
    "        \"\"\"\n",
    "        print(f\"\\nLoading sample of {self.sample_size:,} flows for visualization...\")\n",
    "        \n",
    "        sample_dfs = []\n",
    "        remaining = self.sample_size\n",
    "        \n",
    "        with pd.HDFStore(self.features_h5, mode='r') as store:\n",
    "            keys = store.keys()\n",
    "            \n",
    "            # Sample from different parts of dataset for diversity\n",
    "            sample_interval = max(1, len(keys) // 10)  # Sample from 10 points\n",
    "            \n",
    "            for i in range(0, len(keys), sample_interval):\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                \n",
    "                key = keys[min(i, len(keys)-1)]\n",
    "                batch_sample_size = min(remaining, 1000)\n",
    "                \n",
    "                # Load sample from this batch\n",
    "                batch = store.select(key, stop=batch_sample_size)\n",
    "                sample_dfs.append(batch)\n",
    "                remaining -= len(batch)\n",
    "        \n",
    "        if sample_dfs:\n",
    "            sample = pd.concat(sample_dfs, ignore_index=True)\n",
    "            print(f\"  Loaded {len(sample):,} flows from {len(sample_dfs)} batches\")\n",
    "            return sample\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def create_protocol_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create pie chart showing distribution of network protocols.\n",
    "        Helps understand traffic composition (TCP vs UDP vs other).\n",
    "        \"\"\"\n",
    "        if 'protocol' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Map protocol numbers to names\n",
    "        protocol_map = {\n",
    "            6: 'TCP',\n",
    "            17: 'UDP', \n",
    "            1: 'ICMP',\n",
    "            41: 'IPv6',\n",
    "            47: 'GRE'\n",
    "        }\n",
    "        \n",
    "        df['protocol_name'] = df['protocol'].map(protocol_map).fillna('Other')\n",
    "        protocol_counts = df['protocol_name'].value_counts()\n",
    "        \n",
    "        # Create pie chart\n",
    "        fig = go.Figure(data=[go.Pie(\n",
    "            labels=protocol_counts.index,\n",
    "            values=protocol_counts.values,\n",
    "            hole=0.3,  # Donut chart\n",
    "            marker=dict(colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96A6A6', '#FFA07A']),\n",
    "            textposition='auto',\n",
    "            textinfo='label+percent'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Protocol Distribution (Sample)\",\n",
    "            height=400,\n",
    "            width=600,\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'protocol_distribution.html'))\n",
    "    \n",
    "    def create_attack_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Create bar chart showing distribution of attack types.\n",
    "        Critical for understanding class balance in dataset.\n",
    "        \"\"\"\n",
    "        if 'attack_type' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Count attack types\n",
    "        attack_counts = df['attack_type'].value_counts().head(15)\n",
    "        \n",
    "        # Create bar chart with color coding\n",
    "        colors = ['green' if x == 'BENIGN' else 'red' for x in attack_counts.index]\n",
    "        \n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=attack_counts.index,\n",
    "            y=attack_counts.values,\n",
    "            text=attack_counts.values,\n",
    "            textposition='auto',\n",
    "            marker_color=colors,\n",
    "            hovertemplate='%{x}<br>Count: %{y}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Attack Type Distribution (Sample)\",\n",
    "            xaxis_title=\"Attack Type\",\n",
    "            yaxis_title=\"Number of Flows\",\n",
    "            xaxis_tickangle=-45,\n",
    "            height=500,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'attack_distribution.html'))\n",
    "    \n",
    "    def create_flow_timeline(self, df):\n",
    "        \"\"\"\n",
    "        Create scatter plot showing traffic patterns over time.\n",
    "        Reveals temporal patterns in attacks and normal traffic.\n",
    "        \"\"\"\n",
    "        if 'flow_duration' not in df.columns or 'total_bytes' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Further sample if still too large for smooth visualization\n",
    "        if len(df) > 1000:\n",
    "            df = df.sample(1000)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Color by attack type if available\n",
    "        if 'attack_type' in df.columns:\n",
    "            # Create trace for each attack type\n",
    "            for attack_type in df['attack_type'].unique():\n",
    "                mask = df['attack_type'] == attack_type\n",
    "                \n",
    "                # Determine color\n",
    "                color = 'green' if attack_type == 'BENIGN' else 'red'\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=df[mask].index,\n",
    "                    y=df[mask]['total_bytes'],\n",
    "                    mode='markers',\n",
    "                    name=str(attack_type),\n",
    "                    marker=dict(\n",
    "                        size=np.log1p(df[mask]['total_packets']) * 2,  # Size by packet count\n",
    "                        color=color,\n",
    "                        opacity=0.6,\n",
    "                        line=dict(width=0)\n",
    "                    ),\n",
    "                    hovertemplate='Flow %{x}<br>Bytes: %{y}<br>Type: ' + str(attack_type) + '<extra></extra>'\n",
    "                ))\n",
    "        else:\n",
    "            # Single trace if no attack labels\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df['total_bytes'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color='blue', opacity=0.6)\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Flow Timeline (Sample)\",\n",
    "            xaxis_title=\"Flow Index\",\n",
    "            yaxis_title=\"Total Bytes\",\n",
    "            yaxis_type=\"log\",  # Log scale for better visibility\n",
    "            height=500,\n",
    "            width=1200,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'flow_timeline.html'))\n",
    "    \n",
    "    def create_port_heatmap(self, df):\n",
    "        \"\"\"\n",
    "        Create heatmap showing communication patterns between ports.\n",
    "        Helps identify service interactions and potential scanning.\n",
    "        \"\"\"\n",
    "        if 'src_port' not in df.columns or 'dst_port' not in df.columns:\n",
    "            return\n",
    "        \n",
    "        # Get top ports by frequency\n",
    "        top_src = df['src_port'].value_counts().head(15).index\n",
    "        top_dst = df['dst_port'].value_counts().head(15).index\n",
    "        \n",
    "        # Filter for top ports\n",
    "        df_filtered = df[df['src_port'].isin(top_src) & df['dst_port'].isin(top_dst)]\n",
    "        \n",
    "        # Create communication matrix\n",
    "        matrix = pd.crosstab(df_filtered['src_port'], df_filtered['dst_port'])\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=matrix.values,\n",
    "            x=[str(int(p)) for p in matrix.columns],\n",
    "            y=[str(int(p)) for p in matrix.index],\n",
    "            colorscale='Viridis',\n",
    "            hoverongaps=False,\n",
    "            hovertemplate='Src Port: %{y}<br>Dst Port: %{x}<br>Count: %{z}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Port Communication Heatmap (Top Ports)\",\n",
    "            xaxis_title=\"Destination Port\",\n",
    "            yaxis_title=\"Source Port\",\n",
    "            height=600,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'port_heatmap.html'))\n",
    "    \n",
    "    def create_feature_correlation(self, df):\n",
    "        \"\"\"\n",
    "        Create correlation matrix of top features.\n",
    "        Identifies redundant features and relationships.\n",
    "        \"\"\"\n",
    "        # Select numeric columns (excluding metadata)\n",
    "        exclude_cols = {'flow_id', 'label', 'attack_type', 'label_confidence'}\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols][:15]\n",
    "        \n",
    "        if len(numeric_cols) < 2:\n",
    "            return\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,  # Center colorscale at 0\n",
    "            colorbar=dict(title=\"Correlation\"),\n",
    "            hovertemplate='%{x}<br>%{y}<br>Correlation: %{z:.2f}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Feature Correlation Matrix (Top Features)\",\n",
    "            height=700,\n",
    "            width=800\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'feature_correlation.html'))\n",
    "    \n",
    "    def create_model_comparison(self):\n",
    "        \"\"\"\n",
    "        Create bar chart comparing ML model performance.\n",
    "        Shows which algorithms work best for this data.\n",
    "        \"\"\"\n",
    "        if not self.ml_results:\n",
    "            return\n",
    "        \n",
    "        # Extract model names and accuracies\n",
    "        models = list(self.ml_results.keys())\n",
    "        accuracies = [self.ml_results[m]['accuracy'] for m in models]\n",
    "        \n",
    "        # Create bar chart\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=models,\n",
    "            y=accuracies,\n",
    "            text=[f\"{acc:.4f}\" for acc in accuracies],\n",
    "            textposition='auto',\n",
    "            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'][:len(models)],\n",
    "            hovertemplate='%{x}<br>Accuracy: %{y:.4f}<extra></extra>'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Model Performance Comparison\",\n",
    "            xaxis_title=\"Model\",\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            yaxis_range=[0, 1],\n",
    "            height=400,\n",
    "            width=600\n",
    "        )\n",
    "        \n",
    "        # Save to HTML\n",
    "        fig.write_html(os.path.join(self.output_dir, 'model_comparison.html'))\n",
    "    \n",
    "    def create_dashboard(self, sample_df):\n",
    "        \"\"\"\n",
    "        Create integrated HTML dashboard combining all visualizations.\n",
    "        Provides single-page overview of analysis results.\n",
    "        \"\"\"\n",
    "        # Calculate summary statistics\n",
    "        total_flows = self.get_total_flow_count()\n",
    "        attack_flows = sample_df[sample_df.get('label', 0) > 0].shape[0] if 'label' in sample_df.columns else 0\n",
    "        attack_pct = (attack_flows / len(sample_df) * 100) if len(sample_df) > 0 else 0\n",
    "        best_accuracy = max(r['accuracy'] for r in self.ml_results.values()) if self.ml_results else 0\n",
    "        \n",
    "        # Create dashboard HTML\n",
    "        dashboard_html = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Network Analysis Dashboard</title>\n",
    "            <style>\n",
    "                body {{ \n",
    "                    font-family: 'Segoe UI', Arial, sans-serif; \n",
    "                    margin: 20px; \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    min-height: 100vh;\n",
    "                }}\n",
    "                .container {{\n",
    "                    max-width: 1400px;\n",
    "                    margin: 0 auto;\n",
    "                    background: rgba(255,255,255,0.95);\n",
    "                    border-radius: 20px;\n",
    "                    padding: 30px;\n",
    "                    box-shadow: 0 20px 60px rgba(0,0,0,0.3);\n",
    "                }}\n",
    "                h1 {{ \n",
    "                    color: #2d3748; \n",
    "                    border-bottom: 3px solid #667eea; \n",
    "                    padding-bottom: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    font-size: 28px;\n",
    "                }}\n",
    "                .stats {{ \n",
    "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                    padding: 25px;\n",
    "                    border-radius: 15px;\n",
    "                    margin-bottom: 30px;\n",
    "                    color: white;\n",
    "                    box-shadow: 0 10px 30px rgba(0,0,0,0.2);\n",
    "                }}\n",
    "                .stats h2 {{\n",
    "                    margin-top: 0;\n",
    "                    font-size: 20px;\n",
    "                    opacity: 0.9;\n",
    "                }}\n",
    "                .stat-grid {{\n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "                    gap: 20px;\n",
    "                    margin-top: 15px;\n",
    "                }}\n",
    "                .stat-item {{\n",
    "                    background: rgba(255,255,255,0.1);\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    backdrop-filter: blur(10px);\n",
    "                }}\n",
    "                .stat-value {{\n",
    "                    font-size: 24px;\n",
    "                    font-weight: bold;\n",
    "                    margin-bottom: 5px;\n",
    "                }}\n",
    "                .stat-label {{\n",
    "                    font-size: 12px;\n",
    "                    opacity: 0.8;\n",
    "                    text-transform: uppercase;\n",
    "                }}\n",
    "                .warning {{ \n",
    "                    background: #fed7d7;\n",
    "                    color: #742a2a;\n",
    "                    padding: 15px;\n",
    "                    border-radius: 10px;\n",
    "                    margin-bottom: 20px;\n",
    "                    border-left: 4px solid #fc8181;\n",
    "                }}\n",
    "                .grid {{ \n",
    "                    display: grid;\n",
    "                    grid-template-columns: repeat(2, 1fr);\n",
    "                    gap: 25px;\n",
    "                    margin-top: 20px;\n",
    "                }}\n",
    "                .viz-frame {{ \n",
    "                    background: white;\n",
    "                    border-radius: 12px;\n",
    "                    padding: 15px;\n",
    "                    box-shadow: 0 4px 15px rgba(0,0,0,0.1);\n",
    "                    transition: transform 0.3s ease;\n",
    "                }}\n",
    "                .viz-frame:hover {{\n",
    "                    transform: translateY(-5px);\n",
    "                    box-shadow: 0 8px 25px rgba(0,0,0,0.15);\n",
    "                }}\n",
    "                iframe {{ \n",
    "                    width: 100%;\n",
    "                    height: 400px;\n",
    "                    border: none;\n",
    "                    border-radius: 8px;\n",
    "                }}\n",
    "                .full-width {{ \n",
    "                    grid-column: span 2;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container\">\n",
    "                <h1>Network Packet Analysis Dashboard</h1>\n",
    "                \n",
    "                <div class=\"warning\">\n",
    "                    <strong>Note:</strong> Visualizations based on sampled data ({len(sample_df):,} flows).\n",
    "                    Full dataset contains {total_flows:,} flows.\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"stats\">\n",
    "                    <h2>Summary Statistics</h2>\n",
    "                    <div class=\"stat-grid\">\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{total_flows:,}</div>\n",
    "                            <div class=\"stat-label\">Total Flows Processed</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{attack_pct:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Attack Traffic (Sample)</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{best_accuracy:.2%}</div>\n",
    "                            <div class=\"stat-label\">Best Model Accuracy</div>\n",
    "                        </div>\n",
    "                        <div class=\"stat-item\">\n",
    "                            <div class=\"stat-value\">{psutil.virtual_memory().percent:.1f}%</div>\n",
    "                            <div class=\"stat-label\">Memory Usage</div>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"grid\">\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Protocol Distribution</h3>\n",
    "                        <iframe src=\"protocol_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Model Performance</h3>\n",
    "                        <iframe src=\"model_comparison.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Attack Type Distribution</h3>\n",
    "                        <iframe src=\"attack_distribution.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame full-width\">\n",
    "                        <h3>Flow Timeline Analysis</h3>\n",
    "                        <iframe src=\"flow_timeline.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Port Communication Patterns</h3>\n",
    "                        <iframe src=\"port_heatmap.html\"></iframe>\n",
    "                    </div>\n",
    "                    <div class=\"viz-frame\">\n",
    "                        <h3>Feature Correlations</h3>\n",
    "                        <iframe src=\"feature_correlation.html\"></iframe>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <p style=\"text-align: center; color: #718096; margin-top: 30px; font-size: 14px;\">\n",
    "                    Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                    Pipeline: Memory-Optimized Network Analysis\n",
    "                </p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save dashboard\n",
    "        with open(os.path.join(self.output_dir, 'dashboard.html'), 'w') as f:\n",
    "            f.write(dashboard_html)\n",
    "        \n",
    "        print(\"\\nDashboard created: dashboard.html\")\n",
    "        print(\"  Open in browser to view all visualizations\")\n",
    "    \n",
    "    def get_total_flow_count(self):\n",
    "        \"\"\"\n",
    "        Get total number of flows from HDF5 file.\n",
    "        Counts rows without loading data into memory.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total flow count\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        with pd.HDFStore(self.features_h5, mode='r') as store:\n",
    "            for key in store.keys():\n",
    "                # Get row count without loading data\n",
    "                total += store.get_storer(key).nrows\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd9c7b",
   "metadata": {},
   "source": [
    "# ### Step 10: Export and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Result Exporter - Saving Analysis Outputs\n",
    "\"\"\"\n",
    "PURPOSE: Export all analysis results in various formats for use and sharing\n",
    "This class handles saving features, models, and reports to disk.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Exports extracted features to CSV format (in chunks)\n",
    "2. Saves trained ML models for reuse\n",
    "3. Generates comprehensive text report\n",
    "4. Creates metadata manifest for tracking\n",
    "5. Handles cleanup of temporary files\n",
    "\n",
    "OUTPUT FILES CREATED:\n",
    "- features_chunk_*.csv: Feature data split into manageable files\n",
    "- *.pkl: Serialized ML models\n",
    "- analysis_report.txt: Detailed text report\n",
    "- manifest.json: Metadata about the analysis\n",
    "- dashboard.html: Interactive visualization dashboard\n",
    "\n",
    "WHY CHUNKED EXPORT:\n",
    "- CSV files have size limits\n",
    "- Easier to transfer and share\n",
    "- Can be processed separately\n",
    "- Prevents memory overflow during export\n",
    "\n",
    "REPORT CONTENTS:\n",
    "- Processing statistics\n",
    "- Label distribution  \n",
    "- Feature importance\n",
    "- Model performance\n",
    "- Memory usage\n",
    "\"\"\"\n",
    "\n",
    "class MemoryOptimizedResultExporter:\n",
    "    def __init__(self, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize exporter with output directory.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory for saving all outputs\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    def export_features_chunked(self, features_h5, chunk_size=100000):\n",
    "        \"\"\"\n",
    "        Export features from HDF5 to CSV in manageable chunks.\n",
    "        Large datasets are split across multiple CSV files.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file\n",
    "            chunk_size: Maximum rows per CSV file\n",
    "            \n",
    "        Creates:\n",
    "            features_chunk_000.csv, features_chunk_001.csv, etc.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING FEATURES IN CHUNKS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Exporting to CSV with max {chunk_size:,} rows per file\")\n",
    "        \n",
    "        chunk_counter = 0\n",
    "        total_rows = 0\n",
    "        file_list = []\n",
    "        \n",
    "        with pd.HDFStore(features_h5, mode='r') as store:\n",
    "            for key in tqdm(store.keys(), desc=\"Exporting chunks\"):\n",
    "                # Load batch from HDF5\n",
    "                batch = store[key]\n",
    "                \n",
    "                # Save to CSV\n",
    "                chunk_file = os.path.join(\n",
    "                    self.output_dir, \n",
    "                    f'features_chunk_{chunk_counter:03d}.csv'\n",
    "                )\n",
    "                \n",
    "                # Save with compression to reduce file size\n",
    "                batch.to_csv(chunk_file, index=False, compression='gzip')\n",
    "                \n",
    "                # Track file info\n",
    "                file_info = {\n",
    "                    'filename': os.path.basename(chunk_file),\n",
    "                    'rows': len(batch),\n",
    "                    'size_mb': os.path.getsize(chunk_file) / (1024*1024)\n",
    "                }\n",
    "                file_list.append(file_info)\n",
    "                \n",
    "                chunk_counter += 1\n",
    "                total_rows += len(batch)\n",
    "                \n",
    "                # Clean up\n",
    "                del batch\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"\\n✓ Exported {total_rows:,} flows in {chunk_counter} chunks\")\n",
    "        \n",
    "        # Create manifest file with export metadata\n",
    "        manifest_file = os.path.join(self.output_dir, 'manifest.json')\n",
    "        manifest = {\n",
    "            'export_date': datetime.now().isoformat(),\n",
    "            'total_rows': total_rows,\n",
    "            'total_chunks': chunk_counter,\n",
    "            'chunk_pattern': 'features_chunk_*.csv',\n",
    "            'compression': 'gzip',\n",
    "            'files': file_list,\n",
    "            'source': {\n",
    "                'pcap': Config.PCAP_FILE,\n",
    "                'analysis_mode': Config.ANALYSIS_MODE,\n",
    "                'chunk_size': Config.CHUNK_SIZE\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(manifest_file, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "        \n",
    "        print(f\"✓ Manifest saved to: {manifest_file}\")\n",
    "    \n",
    "    def export_models(self, models):\n",
    "        \"\"\"\n",
    "        Save trained ML models for future use.\n",
    "        Models are serialized using pickle format.\n",
    "        \n",
    "        Args:\n",
    "            models: Dictionary of trained model objects\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXPORTING ML MODELS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if model is not None:\n",
    "                # Create filename\n",
    "                model_path = os.path.join(self.output_dir, f'{model_name}.pkl')\n",
    "                \n",
    "                # Serialize model\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    pickle.dump(model, f)\n",
    "                \n",
    "                # Get file size\n",
    "                size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "                print(f\"✓ Saved {model_name}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    def generate_report(self, features_h5, ml_results, selected_features):\n",
    "        \"\"\"\n",
    "        Generate comprehensive text report of analysis results.\n",
    "        Provides human-readable summary of entire pipeline.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to feature file\n",
    "            ml_results: ML performance results\n",
    "            selected_features: List of selected features\n",
    "        \"\"\"\n",
    "        report_path = os.path.join(self.output_dir, 'analysis_report.txt')\n",
    "        \n",
    "        # Collect statistics without loading all data\n",
    "        stats = self.collect_statistics(features_h5)\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            # Header\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "            f.write(\"NETWORK PACKET ANALYSIS REPORT\\n\")\n",
    "            f.write(\"Memory-Optimized Pipeline for CICIDS2017\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            \n",
    "            # Input Information\n",
    "            f.write(\"INPUT CONFIGURATION\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"PCAP File: {os.path.basename(Config.PCAP_FILE)}\\n\")\n",
    "            f.write(f\"File Size: {os.path.getsize(Config.PCAP_FILE) / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Analysis Mode: {Config.ANALYSIS_MODE}\\n\")\n",
    "            f.write(f\"Deep Inspection: {Config.DEEP_INSPECTION}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Processing Statistics\n",
    "            f.write(\"PROCESSING STATISTICS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total Flows: {stats['total_flows']:,}\\n\")\n",
    "            f.write(f\"Processing Strategy: {'Disk-based' if Config.USE_DISK_CACHE else 'Memory-based'}\\n\")\n",
    "            f.write(f\"Max Memory Setting: {Config.MAX_MEMORY_GB:.1f} GB\\n\")\n",
    "            f.write(f\"Chunk Size: {Config.CHUNK_SIZE:,} packets\\n\")\n",
    "            f.write(f\"Batch Size: {Config.BATCH_SIZE:,} flows\\n\")\n",
    "            f.write(f\"Max Flows in Memory: {Config.MAX_FLOWS_IN_MEMORY:,}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Label Distribution\n",
    "            if stats.get('label_distribution'):\n",
    "                f.write(\"ATTACK TYPE DISTRIBUTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                total = sum(stats['label_distribution'].values())\n",
    "                for label, count in sorted(stats['label_distribution'].items(), \n",
    "                                          key=lambda x: x[1], reverse=True):\n",
    "                    pct = count / total * 100 if total > 0 else 0\n",
    "                    f.write(f\"  {label:30s}: {count:10,} ({pct:5.1f}%)\\n\")\n",
    "                f.write(f\"\\nTotal Labeled Flows: {total:,}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Feature Selection\n",
    "            if selected_features:\n",
    "                f.write(\"FEATURE SELECTION\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                f.write(f\"Features Analyzed: {stats.get('total_features', 'Unknown')}\\n\")\n",
    "                f.write(f\"Features Selected: {len(selected_features)}\\n\")\n",
    "                f.write(\"\\nTop 10 Selected Features:\\n\")\n",
    "                for i, feature in enumerate(selected_features[:10], 1):\n",
    "                    f.write(f\"  {i:2d}. {feature}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # ML Results\n",
    "            if ml_results:\n",
    "                f.write(\"MACHINE LEARNING RESULTS\\n\")\n",
    "                f.write(\"-\"*40 + \"\\n\")\n",
    "                \n",
    "                # Find best model\n",
    "                best_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "                \n",
    "                for model_name, results in ml_results.items():\n",
    "                    is_best = model_name == best_model[0]\n",
    "                    marker = \" ⭐\" if is_best else \"\"\n",
    "                    \n",
    "                    f.write(f\"\\n{model_name.upper()}{marker}:\\n\")\n",
    "                    f.write(f\"  Test Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "                    \n",
    "                    if 'test_samples' in results:\n",
    "                        f.write(f\"  Test Samples: {results['test_samples']:,}\\n\")\n",
    "                    \n",
    "                    if 'unique_predictions' in results:\n",
    "                        f.write(f\"  Unique Predictions: {results['unique_predictions']}\\n\")\n",
    "                \n",
    "                f.write(f\"\\nBest Model: {best_model[0]} (Accuracy: {best_model[1]['accuracy']:.4f})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Memory Usage\n",
    "            mem = psutil.virtual_memory()\n",
    "            f.write(\"MEMORY USAGE\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Current Memory: {mem.percent:.1f}%\\n\")\n",
    "            f.write(f\"Available: {mem.available / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Used: {mem.used / (1024**3):.2f} GB\\n\")\n",
    "            f.write(f\"Total System: {mem.total / (1024**3):.2f} GB\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"ANALYSIS INSIGHTS & RECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            \n",
    "            # Check for class imbalance\n",
    "            if stats.get('label_distribution'):\n",
    "                benign_ratio = stats['label_distribution'].get('BENIGN', 0) / stats['total_flows']\n",
    "                if benign_ratio > 0.9:\n",
    "                    f.write(\"⚠️  High class imbalance detected (>90% BENIGN)\\n\")\n",
    "                    f.write(\"   Consider: Oversampling attacks or using weighted loss\\n\\n\")\n",
    "            \n",
    "            # Check accuracy\n",
    "            if ml_results:\n",
    "                avg_accuracy = np.mean([r['accuracy'] for r in ml_results.values()])\n",
    "                if avg_accuracy < 0.8:\n",
    "                    f.write(\"⚠️  Low average accuracy (<80%)\\n\")\n",
    "                    f.write(\"   Consider: Increasing features, tuning hyperparameters\\n\\n\")\n",
    "                elif avg_accuracy > 0.95:\n",
    "                    f.write(\"✓ Excellent model performance (>95%)\\n\")\n",
    "                    f.write(\"   Ready for deployment consideration\\n\\n\")\n",
    "            \n",
    "            # Footer\n",
    "            f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "            f.write(\"END OF REPORT\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        print(f\"\\n✓ Report saved to: {report_path}\")\n",
    "    \n",
    "    def collect_statistics(self, features_h5):\n",
    "        \"\"\"\n",
    "        Collect statistics from HDF5 without loading all data.\n",
    "        Uses metadata and sampling for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to HDF5 file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Statistics dictionary\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'total_flows': 0,\n",
    "            'total_features': 0,\n",
    "            'label_distribution': Counter()\n",
    "        }\n",
    "        \n",
    "        with pd.HDFStore(features_h5, mode='r') as store:\n",
    "            for key in store.keys():\n",
    "                # Get row count from metadata\n",
    "                stats['total_flows'] += store.get_storer(key).nrows\n",
    "                \n",
    "                # Sample for label distribution\n",
    "                if 'attack_type' in store.get_storer(key).attrs.data_columns:\n",
    "                    sample = store.select(key, columns=['attack_type'], stop=1000)\n",
    "                    if 'attack_type' in sample.columns:\n",
    "                        stats['label_distribution'].update(\n",
    "                            sample['attack_type'].value_counts().to_dict()\n",
    "                        )\n",
    "                \n",
    "                # Count features from first batch\n",
    "                if stats['total_features'] == 0:\n",
    "                    first_batch = store.select(key, stop=1)\n",
    "                    stats['total_features'] = len(first_batch.columns)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def export_all(self, features_h5, models, ml_results, selected_features):\n",
    "        \"\"\"\n",
    "        Main export function - coordinates all export operations.\n",
    "        \n",
    "        Args:\n",
    "            features_h5: Path to feature file\n",
    "            models: Trained ML models\n",
    "            ml_results: Model performance results\n",
    "            selected_features: Selected feature names\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EXPORTING ALL RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Export features if requested\n",
    "        if Config.ML_EXPORT:\n",
    "            self.export_features_chunked(features_h5)\n",
    "        \n",
    "        # Export models\n",
    "        if models:\n",
    "            self.export_models(models)\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_report(features_h5, ml_results, selected_features)\n",
    "        \n",
    "        print(f\"\\n✓ All results exported to: {self.output_dir}\")\n",
    "        print(\"  Files created:\")\n",
    "        print(\"    - features_chunk_*.csv (data)\")\n",
    "        print(\"    - *.pkl (models)\")\n",
    "        print(\"    - analysis_report.txt (summary)\")\n",
    "        print(\"    - manifest.json (metadata)\")\n",
    "        print(\"    - dashboard.html (visualizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a469471",
   "metadata": {},
   "source": [
    "# ### Step 11: Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Main Pipeline - Complete Analysis Orchestration\n",
    "\"\"\"\n",
    "PURPOSE: Orchestrate the entire analysis pipeline from start to finish\n",
    "This is the main execution function that coordinates all components.\n",
    "\n",
    "WHAT THIS CELL DOES:\n",
    "1. Validates configuration and checks system resources\n",
    "2. Extracts features from PCAP file\n",
    "3. Matches with CICIDS labels if available\n",
    "4. Analyzes and selects best features\n",
    "5. Trains ML models incrementally\n",
    "6. Generates visualizations\n",
    "7. Exports all results\n",
    "8. Cleans up temporary files\n",
    "\n",
    "PIPELINE FLOW:\n",
    "Config → Extract → Label → Analyze → Train → Visualize → Export → Cleanup\n",
    "\n",
    "ERROR HANDLING:\n",
    "- Saves partial results on failure\n",
    "- Cleans up temporary files\n",
    "- Provides detailed error messages\n",
    "- Suggests fixes for common issues\n",
    "\n",
    "MEMORY MANAGEMENT:\n",
    "- Monitors memory throughout execution\n",
    "- Switches strategies based on usage\n",
    "- Cleans up after each phase\n",
    "- Reports memory statistics\n",
    "\"\"\"\n",
    "\n",
    "def run_memory_optimized_pipeline():\n",
    "    \"\"\"\n",
    "    Main execution function for the complete analysis pipeline.\n",
    "    Orchestrates all components while managing memory efficiently.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results dictionary with paths and metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING MEMORY-OPTIMIZED NETWORK ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Pipeline Version: 2.0 (Memory-Optimized)\")\n",
    "    print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Check if ML should be skipped\n",
    "    if Config.SKIP_ML:\n",
    "        print(\"\\nML TRAINING WILL BE SKIPPED (Test Mode)\")\n",
    "    \n",
    "    # Check configuration and memory\n",
    "    strategy = Config.check_memory_requirements()\n",
    "    \n",
    "    if strategy is False:\n",
    "        print(\"Error: Invalid PCAP file or configuration\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nProcessing Strategy: {strategy}\")\n",
    "    print(f\"Memory Limit: {Config.MAX_MEMORY_GB:.1f} GB\")\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'start_time': datetime.now(),\n",
    "        'strategy': strategy,\n",
    "        'temp_files': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # ========== STEP 1: FEATURE EXTRACTION ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 1: STREAMING FEATURE EXTRACTION\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Extracting features from network packets...\")\n",
    "        \n",
    "        pipeline = MemoryOptimizedFeaturePipeline()\n",
    "        features_h5 = pipeline.extract_all_features(\n",
    "            Config.PCAP_FILE,\n",
    "            mode=Config.ANALYSIS_MODE\n",
    "        )\n",
    "        \n",
    "        if not features_h5:\n",
    "            print(\"Error: No features extracted\")\n",
    "            return None\n",
    "        \n",
    "        results['temp_files'].append(features_h5)\n",
    "        results['features_h5'] = features_h5\n",
    "        \n",
    "        # Memory checkpoint\n",
    "        monitor_memory(\"After feature extraction\")\n",
    "        \n",
    "        # ========== STEP 2: LABEL MATCHING ==========\n",
    "        if Config.USE_CICIDS_LABELS and Config.LABEL_FILES:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 2: INCREMENTAL LABEL MATCHING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Matching flows with CICIDS ground truth labels...\")\n",
    "            \n",
    "            label_matcher = MemoryOptimizedLabelMatcher()\n",
    "            labeled_h5 = label_matcher.process_features_with_labels(\n",
    "                features_h5, Config.LABEL_FILES\n",
    "            )\n",
    "            \n",
    "            features_h5 = labeled_h5\n",
    "            results['temp_files'].append(labeled_h5)\n",
    "            results['features_h5'] = labeled_h5\n",
    "            \n",
    "            monitor_memory(\"After label matching\")\n",
    "        else:\n",
    "            print(\"\\nStep 2: Skipping label matching (no labels provided)\")\n",
    "        \n",
    "        # ========== STEP 3: FEATURE ANALYSIS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 3: INCREMENTAL FEATURE ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Analyzing feature importance and selecting best features...\")\n",
    "        \n",
    "        analyzer = MemoryOptimizedFeatureAnalyzer()\n",
    "        selected_features = analyzer.analyze_features_incrementally(features_h5)\n",
    "        results['selected_features'] = selected_features\n",
    "        \n",
    "        # Get feature insights\n",
    "        insights = analyzer.get_feature_insights()\n",
    "        results['feature_insights'] = insights\n",
    "        \n",
    "        monitor_memory(\"After feature analysis\")\n",
    "        \n",
    "        # ========== STEP 4: MACHINE LEARNING ==========\n",
    "        if not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: INCREMENTAL MACHINE LEARNING\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Training ML models with incremental learning...\")\n",
    "            \n",
    "            ml_pipeline = MemoryOptimizedMLPipeline()\n",
    "            models, ml_results = ml_pipeline.train_models_incrementally(\n",
    "                features_h5, selected_features\n",
    "            )\n",
    "            \n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "            \n",
    "            monitor_memory(\"After ML training\")\n",
    "        else:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 4: SKIPPING ML TRAINING (Test Mode)\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"ML training skipped for pipeline validation\")\n",
    "            print(\"Feature extraction and labeling completed successfully\")\n",
    "            models = {}\n",
    "            ml_results = {}\n",
    "            results['models'] = models\n",
    "            results['ml_results'] = ml_results\n",
    "        \n",
    "        # ========== STEP 5: VISUALIZATION ==========\n",
    "        if Config.GENERATE_VISUALS and not Config.SKIP_ML:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 5: SAMPLED VISUALIZATIONS\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Creating interactive visualizations...\")\n",
    "            \n",
    "            visualizer = MemoryOptimizedVisualizer(\n",
    "                features_h5, ml_results, Config.OUTPUT_DIR\n",
    "            )\n",
    "            visualizer.create_all_visualizations()\n",
    "            \n",
    "            monitor_memory(\"After visualization\")\n",
    "        else:\n",
    "            if Config.SKIP_ML:\n",
    "                print(\"\\nStep 5: Skipping visualizations (ML was skipped)\")\n",
    "            else:\n",
    "                print(\"\\nStep 5: Skipping visualizations (disabled)\")\n",
    "        \n",
    "        # ========== STEP 6: EXPORT RESULTS ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"STEP 6: CHUNKED EXPORT\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Exporting all results to disk...\")\n",
    "        \n",
    "        exporter = MemoryOptimizedResultExporter(Config.OUTPUT_DIR)\n",
    "        exporter.export_all(features_h5, models, ml_results, selected_features)\n",
    "        \n",
    "        monitor_memory(\"After export\")\n",
    "        \n",
    "        # ========== STEP 7: CLEANUP ==========\n",
    "        if Config.CLEANUP_TEMP:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"STEP 7: CLEANUP\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"Removing temporary files...\")\n",
    "            \n",
    "            for temp_file in results['temp_files']:\n",
    "                if os.path.exists(temp_file):\n",
    "                    try:\n",
    "                        os.remove(temp_file)\n",
    "                        print(f\"  Removed: {os.path.basename(temp_file)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Failed to remove {temp_file}: {e}\")\n",
    "        \n",
    "        # ========== FINAL SUMMARY ==========\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PIPELINE COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Calculate execution time\n",
    "        results['end_time'] = datetime.now()\n",
    "        duration = (results['end_time'] - results['start_time']).total_seconds() / 60\n",
    "        \n",
    "        print(f\"Analysis completed successfully\")\n",
    "        print(f\"  Duration: {duration:.1f} minutes\")\n",
    "        print(f\"  Output Directory: {Config.OUTPUT_DIR}\")\n",
    "        \n",
    "        # Memory summary\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\nMemory Summary:\")\n",
    "        print(f\"  Peak Usage: ~{Config.MAX_MEMORY_GB:.1f} GB (estimated)\")\n",
    "        print(f\"  Current Usage: {mem.percent:.1f}%\")\n",
    "        print(f\"  Strategy: {strategy}\")\n",
    "        \n",
    "        # ML summary\n",
    "        if ml_results and not Config.SKIP_ML:\n",
    "            best_model = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "            print(f\"\\nBest Model: {best_model[0]}\")\n",
    "            print(f\"  Accuracy: {best_model[1]['accuracy']:.4f}\")\n",
    "        elif Config.SKIP_ML:\n",
    "            print(f\"\\nML Training: Skipped (Test Mode)\")\n",
    "            print(f\"  Features extracted: {len(selected_features)}\")\n",
    "        \n",
    "        # Create success summary\n",
    "        results['summary'] = {\n",
    "            'success': True,\n",
    "            'duration_minutes': duration,\n",
    "            'strategy': strategy,\n",
    "            'memory_limit_gb': Config.MAX_MEMORY_GB,\n",
    "            'best_accuracy': max(r['accuracy'] for r in ml_results.values()) if ml_results else 0,\n",
    "            'features_selected': len(selected_features),\n",
    "            'output_dir': Config.OUTPUT_DIR,\n",
    "            'ml_skipped': Config.SKIP_ML\n",
    "        }\n",
    "        \n",
    "        print(\"\\nPipeline execution successful!\")\n",
    "        print(f\"   View results in: {Config.OUTPUT_DIR}\")\n",
    "        if not Config.SKIP_ML:\n",
    "            print(f\"   Open dashboard.html for visualizations\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error handling\n",
    "        print(f\"\\nPipeline Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Try to save partial results\n",
    "        print(\"\\nAttempting to save partial results...\")\n",
    "        if 'features_h5' in results and results['features_h5']:\n",
    "            try:\n",
    "                emergency_file = os.path.join(Config.OUTPUT_DIR, 'emergency_features.h5')\n",
    "                import shutil\n",
    "                shutil.copy2(results['features_h5'], emergency_file)\n",
    "                print(f\"Partial results saved to: {emergency_file}\")\n",
    "            except:\n",
    "                print(\"Could not save partial results\")\n",
    "        \n",
    "        # Cleanup on error\n",
    "        print(\"\\nCleaning up after error...\")\n",
    "        for temp_file in results.get('temp_files', []):\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Error summary\n",
    "        results['summary'] = {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'partial_results': 'emergency_features.h5' if 'features_h5' in results else None\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def monitor_memory(checkpoint_name=\"\"):\n",
    "    \"\"\"\n",
    "    Monitor and display current memory usage.\n",
    "    Helps track memory consumption throughout pipeline.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_name: Description of current pipeline stage\n",
    "    \"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"\\nMemory Status {checkpoint_name}:\")\n",
    "    print(f\"   Used: {mem.used / (1024**3):.2f} GB ({mem.percent:.1f}%)\")\n",
    "    print(f\"   Available: {mem.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Warning if memory usage is high\n",
    "    if mem.percent > 85:\n",
    "        print(\"   WARNING: High memory usage detected!\")\n",
    "        print(\"      Consider reducing chunk/batch sizes\")\n",
    "    elif mem.percent > 95:\n",
    "        print(\"   CRITICAL: Very high memory usage!\")\n",
    "        print(\"      Pipeline may fail - reduce settings immediately\")\n",
    "\n",
    "\n",
    "# ========== EXECUTION CELL ==========\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY TO RUN MEMORY-OPTIMIZED ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nThis pipeline is optimized for:\")\n",
    "print(\"- Large PCAP files (10+ GB)\")\n",
    "print(\"- CICIDS2017 dataset\")\n",
    "print(\"- Limited RAM systems\")\n",
    "print(\"- Incremental processing\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"INSTRUCTIONS:\")\n",
    "print(\"-\"*40)\n",
    "print(\"1. Ensure configuration is complete (Cell 2)\")\n",
    "print(\"2. Verify PCAP file path is correct\")\n",
    "print(\"3. Check available disk space for temp files\")\n",
    "print(\"4. Run: results = run_memory_optimized_pipeline()\")\n",
    "print(\"5. Monitor memory with: monitor_memory()\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"QUICK START:\")\n",
    "print(\"-\"*40)\n",
    "print(\"# To run the analysis, uncomment and execute:\")\n",
    "print(\"# results = run_memory_optimized_pipeline()\")\n",
    "print(\"\")\n",
    "print(\"# To check memory anytime:\")\n",
    "print(\"# monitor_memory()\")\n",
    "\n",
    "# Display current system status\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"CURRENT SYSTEM STATUS:\")\n",
    "print(\"-\"*40)\n",
    "monitor_memory()\n",
    "\n",
    "disk_usage = psutil.disk_usage('/')\n",
    "print(f\"\\nDisk Space:\")\n",
    "print(f\"   Available: {disk_usage.free / (1024**3):.2f} GB\")\n",
    "print(f\"   Used: {disk_usage.percent:.1f}%\")\n",
    "\n",
    "if Config.SKIP_ML:\n",
    "    print(\"\\nML Training is currently DISABLED\")\n",
    "    print(\"   Uncheck 'Skip ML Training' in config to enable\")\n",
    "\n",
    "print(\"\\nReady to start analysis!\")\n",
    "print(\"   Run: results = run_memory_optimized_pipeline()\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# results = run_memory_optimized_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089b568",
   "metadata": {},
   "source": [
    "# ### Step 11: run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run:\n",
    "results = run_memory_optimized_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
