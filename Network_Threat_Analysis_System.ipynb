{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c63a702",
   "metadata": {},
   "source": [
    "# # Network Traffic Analysis Pipeline for CICIDS2017\n",
    "# ## Comprehensive Feature Extraction and Machine Learning Framework\n",
    "# \n",
    "# This notebook integrates three analysis approaches:\n",
    "# 1. Statistical flow analysis\n",
    "# 2. Semantic content analysis  \n",
    "# 3. Visual pattern generation\n",
    "#\n",
    "# Output includes both human-readable reports and ML-ready datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43565ecd",
   "metadata": {},
   "source": [
    "# ### Step 1: Environment Setup and Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1328ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 1: Install Required Dependencies\n",
    "Run this cell first to ensure all required packages are installed\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages if not already present\"\"\"\n",
    "    packages = [\n",
    "        'pandas', 'numpy', 'scapy', 'tqdm', 'matplotlib', 'seaborn',\n",
    "        'plotly', 'scikit-learn', 'psutil', 'tldextract', 'networkx',\n",
    "        'xgboost', 'lightgbm', 'h5py', 'ipywidgets'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"âœ“ {package} already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ“ {package} installed successfully\")\n",
    "\n",
    "install_packages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916162ac",
   "metadata": {},
   "source": [
    "# ### Step 2: Import Libraries and Load Analysis Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae86d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 2: Import all necessary libraries and custom analysis modules\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5606",
   "metadata": {},
   "source": [
    "# ### Step 3: User Input Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 3: Interactive user input for file paths and configuration\n",
    "This cell creates user-friendly widgets for configuration\n",
    "\"\"\"\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self):\n",
    "        self.config = {}\n",
    "        self.setup_ui()\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Create interactive UI elements for configuration\"\"\"\n",
    "        \n",
    "        # File input widgets\n",
    "        self.pcap_input = widgets.Text(\n",
    "            placeholder='Enter PCAP file path',\n",
    "            description='PCAP File:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='600px')\n",
    "        )\n",
    "        \n",
    "        self.output_dir = widgets.Text(\n",
    "            placeholder='Enter output directory path',\n",
    "            description='Output Dir:',\n",
    "            value='./network_analysis_output',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='600px')\n",
    "        )\n",
    "        \n",
    "        # Analysis options\n",
    "        self.deep_inspection = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Enable Deep Packet Inspection',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        self.generate_visuals = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Generate Visualizations',\n",
    "            indent=False\n",
    "        )\n",
    "        \n",
    "        self.chunk_size = widgets.IntSlider(\n",
    "            value=10000,\n",
    "            min=1000,\n",
    "            max=100000,\n",
    "            step=1000,\n",
    "            description='Chunk Size:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # ML options\n",
    "        self.ml_approach = widgets.RadioButtons(\n",
    "            options=['XGBoost (Fast)', 'LightGBM (Balanced)', 'Deep Learning (CNN+LSTM)', 'Ensemble (All)'],\n",
    "            value='XGBoost (Fast)',\n",
    "            description='ML Model:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Process button\n",
    "        self.process_btn = widgets.Button(\n",
    "            description='Start Analysis',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        self.process_btn.on_click(self.validate_and_save)\n",
    "        \n",
    "        # Status output\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Display UI\n",
    "        display(HTML(\"<h3>Network Traffic Analysis Configuration</h3>\"))\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<b>Input/Output Settings:</b>\"),\n",
    "            self.pcap_input,\n",
    "            self.output_dir,\n",
    "            widgets.HTML(\"<br><b>Analysis Options:</b>\"),\n",
    "            self.deep_inspection,\n",
    "            self.generate_visuals,\n",
    "            self.chunk_size,\n",
    "            widgets.HTML(\"<br><b>Machine Learning Configuration:</b>\"),\n",
    "            self.ml_approach,\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            self.process_btn,\n",
    "            self.status_output\n",
    "        ]))\n",
    "    \n",
    "    def validate_and_save(self, b):\n",
    "        \"\"\"Validate inputs and save configuration\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Validate PCAP file\n",
    "            if not self.pcap_input.value:\n",
    "                print(\"Please enter a PCAP file path\")\n",
    "                return\n",
    "            \n",
    "            if not os.path.exists(self.pcap_input.value):\n",
    "                print(f\"PCAP file not found: {self.pcap_input.value}\")\n",
    "                return\n",
    "            \n",
    "            # Create output directory\n",
    "            os.makedirs(self.output_dir.value, exist_ok=True)\n",
    "            \n",
    "            # Save configuration\n",
    "            self.config = {\n",
    "                'pcap_file': self.pcap_input.value,\n",
    "                'output_dir': self.output_dir.value,\n",
    "                'deep_inspection': self.deep_inspection.value,\n",
    "                'generate_visuals': self.generate_visuals.value,\n",
    "                'chunk_size': self.chunk_size.value,\n",
    "                'ml_approach': self.ml_approach.value\n",
    "            }\n",
    "            \n",
    "            # Display configuration summary\n",
    "            print(\"âœ“ Configuration saved successfully!\\n\")\n",
    "            print(\"Configuration Summary:\")\n",
    "            print(\"-\" * 50)\n",
    "            for key, value in self.config.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Calculate estimated processing time\n",
    "            file_size_mb = os.path.getsize(self.config['pcap_file']) / (1024**2)\n",
    "            estimated_time = file_size_mb * 0.5  # Rough estimate: 2MB/sec\n",
    "            print(f\"\\nFile size: {file_size_mb:.2f} MB\")\n",
    "            print(f\"Estimated processing time: {estimated_time:.1f} seconds\")\n",
    "            \n",
    "            # Create configuration manager\n",
    "config_manager = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a01c0ab",
   "metadata": {},
   "source": [
    "# ### Step 4: Unified Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8900f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 4: Unified feature extraction combining all three approaches\n",
    "This integrates flow analysis, semantic analysis, and visual generation\n",
    "\"\"\"\n",
    "\n",
    "class UnifiedFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Combines all three feature extraction approaches into a single pipeline\n",
    "    Handles memory management and progress tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.features_data = []\n",
    "        self.visual_data = []\n",
    "        self.metadata = []\n",
    "        \n",
    "        # Initialize progress bar\n",
    "        self.progress = widgets.IntProgress(\n",
    "            value=0, min=0, max=100,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            style={'bar_color': 'maroon'},\n",
    "            orientation='horizontal'\n",
    "        )\n",
    "        \n",
    "        # Status text\n",
    "        self.status = widgets.HTML(value='Initializing...')\n",
    "        \n",
    "    def extract_all_features(self):\n",
    "        \"\"\"\n",
    "        Main extraction pipeline that coordinates all three extractors\n",
    "        Returns consolidated feature dataset\n",
    "        \"\"\"\n",
    "        display(widgets.VBox([self.progress, self.status]))\n",
    "        \n",
    "        # Import the custom extractors (assuming they're in the same directory)\n",
    "        # In practice, you'd have these as separate .py files imported here\n",
    "        \n",
    "        self.status.value = \"Loading PCAP file...\"\n",
    "        self.progress.value = 10\n",
    "        \n",
    "        # Simulated extraction (replace with actual extractor calls)\n",
    "        flow_features = self._extract_flow_features()\n",
    "        self.progress.value = 40\n",
    "        \n",
    "        semantic_features = self._extract_semantic_features()\n",
    "        self.progress.value = 70\n",
    "        \n",
    "        if self.config['generate_visuals']:\n",
    "            visual_features = self._generate_visual_features()\n",
    "            self.progress.value = 90\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_df = self._combine_features(flow_features, semantic_features)\n",
    "        self.progress.value = 100\n",
    "        self.status.value = \"âœ“ Feature extraction complete!\"\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def _extract_flow_features(self):\n",
    "        \"\"\"Extract statistical flow features\"\"\"\n",
    "        # This would call your Network_TrafficFlow_Feature_Extractor\n",
    "        # For demonstration, creating sample data\n",
    "        \n",
    "        self.status.value = \"Extracting flow features...\"\n",
    "        \n",
    "        # Simulated flow features\n",
    "        flow_data = {\n",
    "            'duration': np.random.exponential(5, 1000),\n",
    "            'packets_per_second': np.random.gamma(2, 2, 1000),\n",
    "            'bytes_per_second': np.random.gamma(3, 100, 1000),\n",
    "            'avg_packet_size': np.random.normal(500, 200, 1000),\n",
    "            'flow_direction_ratio': np.random.beta(2, 5, 1000),\n",
    "            'syn_flag_count': np.random.poisson(3, 1000),\n",
    "            'rst_flag_count': np.random.poisson(1, 1000),\n",
    "            'packet_size_variance': np.random.exponential(1000, 1000)\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(flow_data)\n",
    "    \n",
    "    def _extract_semantic_features(self):\n",
    "        \"\"\"Extract semantic content features\"\"\"\n",
    "        # This would call your Network_Semantic_Feature_Extractor\n",
    "        \n",
    "        self.status.value = \"Extracting semantic features...\"\n",
    "        \n",
    "        # Simulated semantic features\n",
    "        semantic_data = {\n",
    "            'domain_entropy': np.random.normal(3, 1, 1000),\n",
    "            'dga_probability': np.random.beta(2, 8, 1000),\n",
    "            'suspicious_keywords': np.random.poisson(2, 1000),\n",
    "            'command_keywords': np.random.poisson(0.5, 1000),\n",
    "            'obfuscation_indicators': np.random.poisson(1, 1000),\n",
    "            'query_frequency': np.random.exponential(10, 1000),\n",
    "            'failed_queries': np.random.poisson(0.3, 1000),\n",
    "            'user_agent_entropy': np.random.normal(4, 1, 1000),\n",
    "            'encryption_indicators': np.random.poisson(0.2, 1000)\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(semantic_data)\n",
    "    \n",
    "    def _generate_visual_features(self):\n",
    "        \"\"\"Generate visual representations for CNN\"\"\"\n",
    "        # This would call your visual_nlp_system\n",
    "        \n",
    "        self.status.value = \"Generating visual representations...\"\n",
    "        \n",
    "        # Create sample threat heatmaps (256x256x3 RGB images)\n",
    "        num_samples = min(100, len(self.features_data) if self.features_data else 100)\n",
    "        visual_data = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Generate synthetic heatmap\n",
    "            heatmap = np.random.rand(256, 256, 3)\n",
    "            visual_data.append(heatmap)\n",
    "        \n",
    "        self.visual_data = np.array(visual_data)\n",
    "        return self.visual_data\n",
    "    \n",
    "    def _combine_features(self, flow_df, semantic_df):\n",
    "        \"\"\"Combine all feature types into unified dataset\"\"\"\n",
    "        \n",
    "        self.status.value = \"Combining features...\"\n",
    "        \n",
    "        # Combine DataFrames\n",
    "        combined_df = pd.concat([flow_df, semantic_df], axis=1)\n",
    "        \n",
    "        # Add synthetic labels for demonstration\n",
    "        # In practice, these would come from CICIDS2017 labels\n",
    "        label_distribution = [0.8, 0.05, 0.05, 0.03, 0.03, 0.02, 0.02]\n",
    "        labels = ['Benign', 'DoS', 'PortScan', 'WebAttack', 'Botnet', 'Infiltration', 'BruteForce']\n",
    "        \n",
    "        combined_df['label'] = np.random.choice(\n",
    "            labels, \n",
    "            size=len(combined_df),\n",
    "            p=label_distribution\n",
    "        )\n",
    "        \n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aaa6bc",
   "metadata": {},
   "source": [
    "# ### Step 5: Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 5: Feature importance analysis and selection\n",
    "Identifies the most relevant features for machine learning\n",
    "\"\"\"\n",
    "\n",
    "class FeatureAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes extracted features and selects the most important ones\n",
    "    Provides both statistical analysis and visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_df):\n",
    "        self.features_df = features_df\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def analyze_features(self):\n",
    "        \"\"\"Perform comprehensive feature analysis\"\"\"\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"FEATURE ANALYSIS REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Basic statistics\n",
    "        self._print_basic_stats()\n",
    "        \n",
    "        # Correlation analysis\n",
    "        self._analyze_correlations()\n",
    "        \n",
    "        # Feature importance using mutual information\n",
    "        self._calculate_feature_importance()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self._create_feature_visualizations()\n",
    "        \n",
    "        return self.get_selected_features()\n",
    "    \n",
    "    def _print_basic_stats(self):\n",
    "        \"\"\"Print basic dataset statistics\"\"\"\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total samples: {len(self.features_df):,}\")\n",
    "        print(f\"Total features: {len(self.features_df.columns) - 1}\")  # Excluding label\n",
    "        \n",
    "        # Class distribution\n",
    "        print(\"\\n Class Distribution:\")\n",
    "        class_counts = self.features_df['label'].value_counts()\n",
    "        for label, count in class_counts.items():\n",
    "            percentage = (count / len(self.features_df)) * 100\n",
    "            print(f\"  {label}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = self.features_df.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\n Missing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "    \n",
    "    def _analyze_correlations(self):\n",
    "        \"\"\"Analyze feature correlations\"\"\"\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        numeric_cols = self.features_df.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = self.features_df[numeric_cols].corr()\n",
    "        \n",
    "        # Find highly correlated features\n",
    "        high_corr = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                    high_corr.append({\n",
    "                        'feature1': corr_matrix.columns[i],\n",
    "                        'feature2': corr_matrix.columns[j],\n",
    "                        'correlation': corr_matrix.iloc[i, j]\n",
    "                    })\n",
    "        \n",
    "        if high_corr:\n",
    "            print(\"\\n Highly correlated features (>0.9):\")\n",
    "            for pair in high_corr[:5]:  # Show top 5\n",
    "                print(f\"  {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "    \n",
    "    def _calculate_feature_importance(self):\n",
    "        \"\"\"Calculate feature importance using mutual information\"\"\"\n",
    "        \n",
    "        print(\"\\n Calculating feature importance...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = self.features_df.drop('label', axis=1)\n",
    "        y = LabelEncoder().fit_transform(self.features_df['label'])\n",
    "        \n",
    "        # Calculate mutual information scores\n",
    "        mi_scores = mutual_info_classif(X, y)\n",
    "        \n",
    "        # Store importance scores\n",
    "        self.feature_importance = dict(zip(X.columns, mi_scores))\n",
    "        \n",
    "        # Print top features\n",
    "        print(\"\\n Top 10 Most Important Features:\")\n",
    "        sorted_features = sorted(self.feature_importance.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (feature, score) in enumerate(sorted_features[:10], 1):\n",
    "            print(f\"  {i}. {feature}: {score:.4f}\")\n",
    "    \n",
    "    def _create_feature_visualizations(self):\n",
    "        \"\"\"Create feature analysis visualizations\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Feature Analysis Dashboard', fontsize=16)\n",
    "        \n",
    "        # 1. Feature importance bar chart\n",
    "        ax1 = axes[0, 0]\n",
    "        top_features = sorted(self.feature_importance.items(), \n",
    "                            key=lambda x: x[1], reverse=True)[:10]\n",
    "        features, scores = zip(*top_features)\n",
    "        \n",
    "        ax1.barh(range(len(features)), scores, color='skyblue')\n",
    "        ax1.set_yticks(range(len(features)))\n",
    "        ax1.set_yticklabels(features)\n",
    "        ax1.set_xlabel('Importance Score')\n",
    "        ax1.set_title('Top 10 Feature Importance')\n",
    "        ax1.invert_yaxis()\n",
    "        \n",
    "        # 2. Class distribution pie chart\n",
    "        ax2 = axes[0, 1]\n",
    "        class_counts = self.features_df['label'].value_counts()\n",
    "        ax2.pie(class_counts.values, labels=class_counts.index, \n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Class Distribution')\n",
    "        \n",
    "        # 3. Feature correlation heatmap (top features)\n",
    "        ax3 = axes[1, 0]\n",
    "        top_feature_names = [f for f, _ in top_features]\n",
    "        corr_subset = self.features_df[top_feature_names].corr()\n",
    "        \n",
    "        sns.heatmap(corr_subset, annot=True, fmt='.2f', \n",
    "                   cmap='coolwarm', center=0, ax=ax3,\n",
    "                   cbar_kws={'shrink': 0.8})\n",
    "        ax3.set_title('Top Features Correlation Matrix')\n",
    "        \n",
    "        # 4. Feature distribution comparison\n",
    "        ax4 = axes[1, 1]\n",
    "        feature_to_plot = top_features[0][0]  # Most important feature\n",
    "        \n",
    "        for label in self.features_df['label'].unique():\n",
    "            subset = self.features_df[self.features_df['label'] == label][feature_to_plot]\n",
    "            ax4.hist(subset, alpha=0.5, label=label, bins=20)\n",
    "        \n",
    "        ax4.set_xlabel(feature_to_plot)\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title(f'{feature_to_plot} Distribution by Class')\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_selected_features(self, top_k=20):\n",
    "        \"\"\"Get the top K most important features\"\"\"\n",
    "        sorted_features = sorted(self.feature_importance.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        return [f for f, _ in sorted_features[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6067b9ac",
   "metadata": {},
   "source": [
    "# ### Step 6: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 6: Machine Learning model training and evaluation\n",
    "Implements multiple ML approaches based on user selection\n",
    "\"\"\"\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"\n",
    "    Comprehensive machine learning pipeline supporting multiple algorithms\n",
    "    Handles data preparation, training, evaluation, and export\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features_df, visual_data, config):\n",
    "        self.features_df = features_df\n",
    "        self.visual_data = visual_data\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_data(self, selected_features):\n",
    "        \"\"\"Prepare data for machine learning\"\"\"\n",
    "        \n",
    "        print(\"Preparing data for machine learning...\")\n",
    "        \n",
    "        # Separate features and labels\n",
    "        X = self.features_df[selected_features]\n",
    "        y = LabelEncoder().fit_transform(self.features_df['label'])\n",
    "        \n",
    "        # Handle missing values and infinities\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Training set: {X_train.shape}\")\n",
    "        print(f\"âœ“ Test set: {X_test.shape}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, scaler\n",
    "    \n",
    "    def train_xgboost(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train XGBoost model\"\"\"\n",
    "        \n",
    "        print(\"\\n Training XGBoost model...\")\n",
    "        \n",
    "        # Calculate class weights for imbalanced data\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        classes = np.unique(y_train)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        \n",
    "        # Create sample weights\n",
    "        sample_weights = np.array([class_weights[y] for y in y_train])\n",
    "        \n",
    "        # Train model\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            objective='multi:softprob',\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = (y_pred == y_test).mean()\n",
    "        \n",
    "        print(f\"âœ“ XGBoost Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        self.models['xgboost'] = model\n",
    "        self.results['xgboost'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_lightgbm(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Train LightGBM model\"\"\"\n",
    "        \n",
    "        print(\"\\n Training LightGBM model...\")\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            num_leaves=31,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric='multi_logloss',\n",
    "            callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = (y_pred == y_test).mean()\n",
    "        \n",
    "        print(f\"âœ“ LightGBM Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        self.models['lightgbm'] = model\n",
    "        self.results['lightgbm'] = {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MODEL EVALUATION RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Get label names for reporting\n",
    "        label_names = self.features_df['label'].unique()\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            print(f\"\\nðŸ“Š {model_name.upper()} Performance:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Classification report\n",
    "            report = classification_report(\n",
    "                y_test, \n",
    "                result['predictions'],\n",
    "                target_names=label_names,\n",
    "                output_dict=True\n",
    "            )\n",
    "            \n",
    "            # Print per-class metrics\n",
    "            for label in label_names:\n",
    "                if label in report:\n",
    "                    metrics = report[label]\n",
    "                    print(f\"{label:15s} - Precision: {metrics['precision']:.3f}, \"\n",
    "                          f\"Recall: {metrics['recall']:.3f}, \"\n",
    "                          f\"F1: {metrics['f1-score']:.3f}\")\n",
    "            \n",
    "            # Overall metrics\n",
    "            print(f\"\\nOverall Accuracy: {report['accuracy']:.4f}\")\n",
    "            print(f\"Macro Avg F1: {report['macro avg']['f1-score']:.4f}\")\n",
    "            print(f\"Weighted Avg F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    def create_evaluation_visualizations(self, X_test, y_test):\n",
    "        \"\"\"Create model evaluation visualizations\"\"\"\n",
    "        \n",
    "        # Create subplots for each model\n",
    "        n_models = len(self.models)\n",
    "        fig, axes = plt.subplots(n_models, 2, figsize=(15, 5*n_models))\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        label_names = sorted(self.features_df['label'].unique())\n",
    "        \n",
    "        for idx, (model_name, result) in enumerate(self.results.items()):\n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_test, result['predictions'])\n",
    "            \n",
    "            # Plot confusion matrix\n",
    "            ax1 = axes[idx, 0]\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "            ax1.set_title(f'{model_name.upper()} - Confusion Matrix')\n",
    "            ax1.set_xlabel('Predicted')\n",
    "            ax1.set_ylabel('Actual')\n",
    "            \n",
    "            # Feature importance (if available)\n",
    "            ax2 = axes[idx, 1]\n",
    "            \n",
    "            if hasattr(result['model'], 'feature_importances_'):\n",
    "                importances = result['model'].feature_importances_\n",
    "                feature_names = self.features_df.drop('label', axis=1).columns\n",
    "                \n",
    "                # Get top 10 features\n",
    "                indices = np.argsort(importances)[-10:]\n",
    "                \n",
    "                ax2.barh(range(len(indices)), importances[indices])\n",
    "                ax2.set_yticks(range(len(indices)))\n",
    "                ax2.set_yticklabels([feature_names[i] for i in indices])\n",
    "                ax2.set_xlabel('Importance')\n",
    "                ax2.set_title(f'{model_name.upper()} - Feature Importance')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3307df",
   "metadata": {},
   "source": [
    "# ### Step 7: Export Results for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 7: Export processed data and models for production deployment\n",
    "Creates both human-readable reports and ML-ready datasets\n",
    "\"\"\"\n",
    "\n",
    "class ResultExporter:\n",
    "    \"\"\"\n",
    "    Exports analysis results in multiple formats:\n",
    "    - HDF5 for ML pipelines\n",
    "    - CSV for data analysis\n",
    "    - HTML reports for human review\n",
    "    - Trained models for deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, features_df, models, results):\n",
    "        self.config = config\n",
    "        self.features_df = features_df\n",
    "        self.models = models\n",
    "        self.results = results\n",
    "        self.export_dir = Path(config['output_dir'])\n",
    "        \n",
    "    def export_all(self):\n",
    "        \"\"\"Export all results in appropriate formats\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPORTING RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        (self.export_dir / 'ml_data').mkdir(exist_ok=True)\n",
    "        (self.export_dir / 'human_reports').mkdir(exist_ok=True)\n",
    "        (self.export_dir / 'models').mkdir(exist_ok=True)\n",
    "        \n",
    "        # Export ML-ready data\n",
    "        self._export_ml_data()\n",
    "        \n",
    "        # Export human-readable reports\n",
    "        self._export_human_reports()\n",
    "        \n",
    "        # Export trained models\n",
    "        self._export_models()\n",
    "        \n",
    "        # Create summary file\n",
    "        self._create_summary()\n",
    "        \n",
    "        print(\"\\nâœ“ All results exported successfully!\")\n",
    "        print(f\" Output directory: {self.export_dir}\")\n",
    "    \n",
    "    def _export_ml_data(self):\n",
    "        \"\"\"Export data in ML-ready formats\"\"\"\n",
    "        \n",
    "        print(\"\\n Exporting ML datasets...\")\n",
    "        \n",
    "        # HDF5 format for large-scale ML\n",
    "        h5_path = self.export_dir / 'ml_data' / 'network_features.h5'\n",
    "        \n",
    "        with h5py.File(h5_path, 'w') as f:\n",
    "            # Store feature data\n",
    "            features = self.features_df.drop('label', axis=1).values\n",
    "            f.create_dataset('features', data=features, dtype='float32')\n",
    "            \n",
    "            # Store labels\n",
    "            labels = LabelEncoder().fit_transform(self.features_df['label'])\n",
    "            f.create_dataset('labels', data=labels, dtype='int32')\n",
    "            \n",
    "            # Store metadata\n",
    "            f.attrs['feature_names'] = list(self.features_df.drop('label', axis=1).columns)\n",
    "            f.attrs['label_names'] = list(self.features_df['label'].unique())\n",
    "            f.attrs['n_samples'] = len(features)\n",
    "            f.attrs['n_features'] = features.shape[1]\n",
    "        \n",
    "        print(f\"   HDF5 dataset: {h5_path}\")\n",
    "        \n",
    "        # CSV for easy inspection\n",
    "        csv_path = self.export_dir / 'ml_data' / 'network_features.csv'\n",
    "        self.features_df.to_csv(csv_path, index=False)\n",
    "        print(f\"   CSV dataset: {csv_path}\")\n",
    "        \n",
    "        # NumPy arrays for direct use\n",
    "        np_path = self.export_dir / 'ml_data' / 'features.npy'\n",
    "        np.save(np_path, features)\n",
    "        print(f\"   NumPy array: {np_path}\")\n",
    "    \n",
    "    def _export_human_reports(self):\n",
    "        \"\"\"Generate human-readable analysis reports\"\"\"\n",
    "        \n",
    "        print(\"\\n Generating human reports...\")\n",
    "        \n",
    "        # HTML report\n",
    "        html_content = self._generate_html_report()\n",
    "        html_path = self.export_dir / 'human_reports' / 'analysis_report.html'\n",
    "        \n",
    "        with open(html_path, 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"   HTML report: {html_path}\")\n",
    "        \n",
    "        # Text summary\n",
    "        text_path = self.export_dir / 'human_reports' / 'summary.txt'\n",
    "        self._generate_text_summary(text_path)\n",
    "        print(f\"   Text summary: {text_path}\")\n",
    "    \n",
    "    def _generate_html_report(self):\n",
    "        \"\"\"Generate comprehensive HTML report\"\"\"\n",
    "        \n",
    "        html = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Network Traffic Analysis Report</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "                h1 { color: #333; border-bottom: 2px solid #333; }\n",
    "                h2 { color: #666; }\n",
    "                table { border-collapse: collapse; width: 100%; }\n",
    "                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "                th { background-color: #f2f2f2; }\n",
    "                .metric { background-color: #e8f4f8; padding: 10px; margin: 10px 0; }\n",
    "                .warning { color: #ff6b6b; font-weight: bold; }\n",
    "                .success { color: #51cf66; font-weight: bold; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Network Traffic Analysis Report</h1>\n",
    "            <p>Generated: {timestamp}</p>\n",
    "            \n",
    "            <h2>Dataset Overview</h2>\n",
    "            <div class=\"metric\">\n",
    "                <p>Total Samples: {n_samples}</p>\n",
    "                <p>Features Extracted: {n_features}</p>\n",
    "                <p>Attack Types Detected: {n_classes}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Class Distribution</h2>\n",
    "            <table>\n",
    "                <tr><th>Class</th><th>Count</th><th>Percentage</th></tr>\n",
    "                {class_table}\n",
    "            </table>\n",
    "            \n",
    "            <h2>Model Performance</h2>\n",
    "            <table>\n",
    "                <tr><th>Model</th><th>Accuracy</th><th>F1 Score</th></tr>\n",
    "                {model_table}\n",
    "            </table>\n",
    "            \n",
    "            <h2>Top Security Findings</h2>\n",
    "            <ul>\n",
    "                {findings}\n",
    "            </ul>\n",
    "            \n",
    "            <h2>Recommendations</h2>\n",
    "            <ul>\n",
    "                <li>Monitor high-entropy domains (potential DGA activity)</li>\n",
    "                <li>Investigate flows with command keywords detected</li>\n",
    "                <li>Review connections with obfuscation indicators</li>\n",
    "                <li>Analyze failed DNS queries for reconnaissance attempts</li>\n",
    "            </ul>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fill in template\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        n_samples = len(self.features_df)\n",
    "        n_features = len(self.features_df.columns) - 1\n",
    "        n_classes = len(self.features_df['label'].unique())\n",
    "        \n",
    "        # Class distribution table\n",
    "        class_table = \"\"\n",
    "        for label, count in self.features_df['label'].value_counts().items():\n",
    "            percentage = (count / n_samples) * 100\n",
    "            class_table += f\"<tr><td>{label}</td><td>{count}</td><td>{percentage:.1f}%</td></tr>\"\n",
    "        \n",
    "        # Model performance table\n",
    "        model_table = \"\"\n",
    "        for model_name, result in self.results.items():\n",
    "            accuracy = result['accuracy']\n",
    "            model_table += f\"<tr><td>{model_name}</td><td>{accuracy:.4f}</td><td>-</td></tr>\"\n",
    "        \n",
    "        # Security findings\n",
    "        findings = \"\"\n",
    "        \n",
    "        # Check for high-risk indicators\n",
    "        high_entropy = (self.features_df['domain_entropy'] > 4).sum()\n",
    "        if high_entropy > 0:\n",
    "            findings += f\"<li class='warning'>High entropy domains detected: {high_entropy} flows</li>\"\n",
    "        \n",
    "        command_detected = (self.features_df['command_keywords'] > 0).sum()\n",
    "        if command_detected > 0:\n",
    "            findings += f\"<li class='warning'>Command keywords detected: {command_detected} flows</li>\"\n",
    "        \n",
    "        obfuscation = (self.features_df['obfuscation_indicators'] > 0).sum()\n",
    "        if obfuscation > 0:\n",
    "            findings += f\"<li class='warning'>Obfuscation detected: {obfuscation} flows</li>\"\n",
    "        \n",
    "        return html.format(\n",
    "            timestamp=timestamp,\n",
    "            n_samples=n_samples,\n",
    "            n_features=n_features,\n",
    "            n_classes=n_classes,\n",
    "            class_table=class_table,\n",
    "            model_table=model_table,\n",
    "            findings=findings\n",
    "        )\n",
    "    \n",
    "    def _generate_text_summary(self, output_path):\n",
    "        \"\"\"Generate text summary for quick review\"\"\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"NETWORK TRAFFIC ANALYSIS SUMMARY\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"PCAP File: {self.config['pcap_file']}\\n\")\n",
    "            f.write(f\"Total Flows Analyzed: {len(self.features_df):,}\\n\\n\")\n",
    "            \n",
    "            f.write(\"THREAT SUMMARY\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # Calculate threat metrics\n",
    "            threat_flows = self.features_df[self.features_df['label'] != 'Benign']\n",
    "            threat_percentage = (len(threat_flows) / len(self.features_df)) * 100\n",
    "            \n",
    "            f.write(f\"Threats Detected: {len(threat_flows):,} ({threat_percentage:.1f}%)\\n\")\n",
    "            f.write(f\"Benign Traffic: {len(self.features_df) - len(threat_flows):,}\\n\\n\")\n",
    "            \n",
    "            f.write(\"ATTACK TYPES DETECTED\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            for label in threat_flows['label'].unique():\n",
    "                count = (threat_flows['label'] == label).sum()\n",
    "                f.write(f\"  {label}: {count:,}\\n\")\n",
    "            \n",
    "            f.write(\"\\nHIGH-RISK INDICATORS\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            # High-risk statistics\n",
    "            high_entropy = (self.features_df['domain_entropy'] > 4).sum()\n",
    "            f.write(f\"  High entropy domains: {high_entropy:,}\\n\")\n",
    "            \n",
    "            dga_suspected = (self.features_df['dga_probability'] > 0.7).sum()\n",
    "            f.write(f\"  Suspected DGA domains: {dga_suspected:,}\\n\")\n",
    "            \n",
    "            commands = (self.features_df['command_keywords'] > 0).sum()\n",
    "            f.write(f\"  Command injection attempts: {commands:,}\\n\")\n",
    "            \n",
    "            obfuscated = (self.features_df['obfuscation_indicators'] > 0).sum()\n",
    "            f.write(f\"  Obfuscated payloads: {obfuscated:,}\\n\")\n",
    "            \n",
    "            f.write(\"\\nMODEL PERFORMANCE\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            \n",
    "            for model_name, result in self.results.items():\n",
    "                f.write(f\"  {model_name}: {result['accuracy']:.4f} accuracy\\n\")\n",
    "    \n",
    "    def _export_models(self):\n",
    "        \"\"\"Export trained models for deployment\"\"\"\n",
    "        \n",
    "        print(\"\\n Saving trained models...\")\n",
    "        \n",
    "        import joblib\n",
    "        \n",
    "        for model_name, model_data in self.models.items():\n",
    "            model_path = self.export_dir / 'models' / f'{model_name}_model.pkl'\n",
    "            joblib.dump(model_data, model_path)\n",
    "            print(f\"  âœ“ {model_name}: {model_path}\")\n",
    "    \n",
    "    def _create_summary(self):\n",
    "        \"\"\"Create summary JSON file with all paths and metadata\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': self.config,\n",
    "            'data_files': {\n",
    "                'hdf5': str(self.export_dir / 'ml_data' / 'network_features.h5'),\n",
    "                'csv': str(self.export_dir / 'ml_data' / 'network_features.csv'),\n",
    "                'numpy': str(self.export_dir / 'ml_data' / 'features.npy')\n",
    "            },\n",
    "            'reports': {\n",
    "                'html': str(self.export_dir / 'human_reports' / 'analysis_report.html'),\n",
    "                'text': str(self.export_dir / 'human_reports' / 'summary.txt')\n",
    "            },\n",
    "            'models': {\n",
    "                name: str(self.export_dir / 'models' / f'{name}_model.pkl')\n",
    "                for name in self.models.keys()\n",
    "            },\n",
    "            'statistics': {\n",
    "                'total_samples': len(self.features_df),\n",
    "                'total_features': len(self.features_df.columns) - 1,\n",
    "                'threat_percentage': (\n",
    "                    (self.features_df['label'] != 'Benign').sum() / \n",
    "                    len(self.features_df) * 100\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        summary_path = self.export_dir / 'analysis_summary.json'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Summary file: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e192c",
   "metadata": {},
   "source": [
    "# ### Step 8: Main Execution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19848809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 8: Main execution pipeline that orchestrates the entire analysis\n",
    "Run this cell after configuration to start the complete analysis\n",
    "\"\"\"\n",
    "\n",
    "def run_analysis_pipeline():\n",
    "    \"\"\"\n",
    "    Main function that executes the complete analysis pipeline\n",
    "    Coordinates all components and generates final outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if configuration exists\n",
    "    if not hasattr(config_manager, 'config') or not config_manager.config:\n",
    "        print(\" Please configure settings first (run Cell 3)\")\n",
    "        return\n",
    "    \n",
    "    config = config_manager.config\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING NETWORK TRAFFIC ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Feature Extraction\n",
    "        print(\"\\n Step 1: Feature Extraction\")\n",
    "        print(\"-\" * 40)\n",
    "        extractor = UnifiedFeatureExtractor(config)\n",
    "        features_df = extractor.extract_all_features()\n",
    "        visual_data = extractor.visual_data if config['generate_visuals'] else None\n",
    "        \n",
    "        # Step 2: Feature Analysis\n",
    "        print(\"\\n Step 2: Feature Analysis\")\n",
    "        print(\"-\" * 40)\n",
    "        analyzer = FeatureAnalyzer(features_df)\n",
    "        selected_features = analyzer.analyze_features()\n",
    "        \n",
    "        # Step 3: Machine Learning\n",
    "        print(\"\\n Step 3: Machine Learning\")\n",
    "        print(\"-\" * 40)\n",
    "        ml_pipeline = MLPipeline(features_df, visual_data, config)\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test, scaler = ml_pipeline.prepare_data(selected_features)\n",
    "        \n",
    "        # Train models based on user selection\n",
    "        if config['ml_approach'] == 'XGBoost (Fast)':\n",
    "            ml_pipeline.train_xgboost(X_train, X_test, y_train, y_test)\n",
    "        elif config['ml_approach'] == 'LightGBM (Balanced)':\n",
    "            ml_pipeline.train_lightgbm(X_train, X_test, y_train, y_test)\n",
    "        elif config['ml_approach'] == 'Ensemble (All)':\n",
    "            ml_pipeline.train_xgboost(X_train, X_test, y_train, y_test)\n",
    "            ml_pipeline.train_lightgbm(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Evaluate models\n",
    "        ml_pipeline.evaluate_models(X_test, y_test)\n",
    "        \n",
    "        # Create visualizations\n",
    "        if config['generate_visuals']:\n",
    "            ml_pipeline.create_evaluation_visualizations(X_test, y_test)\n",
    "        \n",
    "        # Step 4: Export Results\n",
    "        print(\"\\n Step 4: Exporting Results\")\n",
    "        print(\"-\" * 40)\n",
    "        exporter = ResultExporter(\n",
    "            config, \n",
    "            features_df,\n",
    "            ml_pipeline.models,\n",
    "            ml_pipeline.results\n",
    "        )\n",
    "        exporter.export_all()\n",
    "        \n",
    "        # Calculate execution time\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" ANALYSIS COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\n Total execution time: {execution_time:.2f} seconds\")\n",
    "        print(f\" Results saved to: {config['output_dir']}\")\n",
    "        \n",
    "        # Display final summary\n",
    "        print(\"\\n Quick Summary:\")\n",
    "        print(f\"  â€¢ Samples analyzed: {len(features_df):,}\")\n",
    "        print(f\"  â€¢ Features extracted: {len(selected_features)}\")\n",
    "        print(f\"  â€¢ Models trained: {len(ml_pipeline.models)}\")\n",
    "        \n",
    "        if ml_pipeline.results:\n",
    "            best_model = max(ml_pipeline.results.items(), \n",
    "                           key=lambda x: x[1]['accuracy'])\n",
    "            print(f\"  â€¢ Best model: {best_model[0]} ({best_model[1]['accuracy']:.4f} accuracy)\")\n",
    "        \n",
    "        # Threat summary\n",
    "        threat_count = (features_df['label'] != 'Benign').sum()\n",
    "        threat_percentage = (threat_count / len(features_df)) * 100\n",
    "        \n",
    "        if threat_percentage > 10:\n",
    "            print(f\"\\n HIGH THREAT LEVEL: {threat_percentage:.1f}% malicious traffic detected!\")\n",
    "        elif threat_percentage > 5:\n",
    "            print(f\"\\n MODERATE THREAT: {threat_percentage:.1f}% suspicious traffic\")\n",
    "        else:\n",
    "            print(f\"\\n LOW THREAT: {threat_percentage:.1f}% anomalous traffic\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error during analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    return features_df, ml_pipeline, exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd9c7b",
   "metadata": {},
   "source": [
    "# ### Step 9: Run the Complete Analysis\n",
    "# Execute this cell to start the analysis with your configured settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8010e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell 9: Execute the analysis pipeline\n",
    "This cell runs the complete analysis based on your configuration\n",
    "\"\"\"\n",
    "\n",
    "# Run the analysis\n",
    "results = run_analysis_pipeline()\n",
    "\n",
    "if results:\n",
    "    features_df, ml_pipeline, exporter = results\n",
    "    print(\"\\n Analysis completed successfully!\")\n",
    "    print(\"You can now:\")\n",
    "    print(\"  1. Review the generated reports in the output directory\")\n",
    "    print(\"  2. Use the exported models for real-time detection\")\n",
    "    print(\"  3. Analyze the feature importance for security insights\")\n",
    "else:\n",
    "    print(\"\\n Please configure settings in Cell 3 before running the analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
